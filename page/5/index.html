<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
  <meta name="theme-color" content="#222">
  <meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>
  <script id="hexo-configurations">
    var NexT = window.NexT ||
    {};
    var CONFIG = {
      "hostname": "cuiqingcai.com",
      "root": "/",
      "scheme": "Pisces",
      "version": "7.8.0",
      "exturl": false,
      "sidebar":
      {
        "position": "right",
        "width": 360,
        "display": "post",
        "padding": 18,
        "offset": 12,
        "onmobile": false,
        "widgets": [
          {
            "type": "image",
            "name": "阿布云",
            "enable": true,
            "url": "https://www.abuyun.com/http-proxy/introduce.html",
            "src": "https://qiniu.cuiqingcai.com/88au8.jpg",
            "width": "100%"
      },
          {
            "type": "image",
            "name": "天验",
            "enable": true,
            "url": "https://tutorial.lengyue.video/?coupon=12ef4b1a-a3db-11ea-bb37-0242ac130002_cqx_850",
            "src": "https://qiniu.cuiqingcai.com/bco2a.png",
            "width": "100%"
      },
          {
            "type": "image",
            "name": "华为云",
            "enable": false,
            "url": "https://activity.huaweicloud.com/2020_618_promotion/index.html?bpName=5f9f98a29e2c40b780c1793086f29fe2&bindType=1&salesID=wangyubei",
            "src": "https://qiniu.cuiqingcai.com/y42ik.jpg",
            "width": "100%"
      },
          {
            "type": "image",
            "name": "张小鸡",
            "enable": false,
            "url": "http://www.zxiaoji.com/",
            "src": "https://qiniu.cuiqingcai.com/fm72f.png",
            "width": "100%"
      },
          {
            "type": "image",
            "name": "Luminati",
            "src": "https://qiniu.cuiqingcai.com/ikkq9.jpg",
            "url": "https://luminati-china.io/?affiliate=ref_5fbbaaa9647883f5c6f77095",
            "width": "100%",
            "enable": true
      },
          {
            "type": "tags",
            "name": "标签云",
            "enable": true
      },
          {
            "type": "categories",
            "name": "分类",
            "enable": true
      },
          {
            "type": "friends",
            "name": "友情链接",
            "enable": true
      },
          {
            "type": "hot",
            "name": "猜你喜欢",
            "enable": true
      }]
      },
      "copycode":
      {
        "enable": true,
        "show_result": true,
        "style": "mac"
      },
      "back2top":
      {
        "enable": true,
        "sidebar": false,
        "scrollpercent": true
      },
      "bookmark":
      {
        "enable": false,
        "color": "#222",
        "save": "auto"
      },
      "fancybox": false,
      "mediumzoom": false,
      "lazyload": false,
      "pangu": true,
      "comments":
      {
        "style": "tabs",
        "active": "gitalk",
        "storage": true,
        "lazyload": false,
        "nav": null,
        "activeClass": "gitalk"
      },
      "algolia":
      {
        "hits":
        {
          "per_page": 10
        },
        "labels":
        {
          "input_placeholder": "Search for Posts",
          "hits_empty": "We didn't find any results for the search: ${query}",
          "hits_stats": "${hits} results found in ${time} ms"
        }
      },
      "localsearch":
      {
        "enable": true,
        "trigger": "auto",
        "top_n_per_article": 10,
        "unescape": false,
        "preload": false
      },
      "motion":
      {
        "enable": false,
        "async": false,
        "transition":
        {
          "post_block": "bounceDownIn",
          "post_header": "slideDownIn",
          "post_body": "slideDownIn",
          "coll_header": "slideLeftIn",
          "sidebar": "slideUpIn"
        }
      },
      "path": "search.xml"
    };

  </script>
  <meta name="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
  <meta property="og:type" content="website">
  <meta property="og:title" content="静觅">
  <meta property="og:url" content="https://cuiqingcai.com/page/5/index.html">
  <meta property="og:site_name" content="静觅">
  <meta property="og:description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
  <meta property="og:locale" content="zh_CN">
  <meta property="article:author" content="崔庆才">
  <meta property="article:tag" content="崔庆才">
  <meta property="article:tag" content="静觅">
  <meta property="article:tag" content="PHP">
  <meta property="article:tag" content="Java">
  <meta property="article:tag" content="Python">
  <meta property="article:tag" content="Spider">
  <meta property="article:tag" content="爬虫">
  <meta property="article:tag" content="Web">
  <meta property="article:tag" content="Kubernetes">
  <meta property="article:tag" content="深度学习">
  <meta property="article:tag" content="机器学习">
  <meta property="article:tag" content="数据分析">
  <meta property="article:tag" content="网络">
  <meta property="article:tag" content="IT">
  <meta property="article:tag" content="技术">
  <meta property="article:tag" content="博客">
  <meta name="twitter:card" content="summary">
  <link rel="canonical" href="https://cuiqingcai.com/page/5/">
  <script id="page-configurations">
    // https://hexo.io/docs/variables.html
    CONFIG.page = {
      sidebar: "",
      isHome: true,
      isPost: false,
      lang: 'zh-CN'
    };

  </script>
  <title>静觅丨崔庆才的个人站点</title>
  <meta name="google-site-verification" content="p_bIcnvirkFzG2dYKuNDivKD8-STet5W7D-01woA2fc" />
  <noscript>
    <style>
      .use-motion .brand,
      .use-motion .menu-item,
      .sidebar-inner,
      .use-motion .post-block,
      .use-motion .pagination,
      .use-motion .comments,
      .use-motion .post-header,
      .use-motion .post-body,
      .use-motion .collection-header
      {
        opacity: initial;
      }

      .use-motion .site-title,
      .use-motion .site-subtitle
      {
        opacity: initial;
        top: initial;
      }

      .use-motion .logo-line-before i
      {
        left: initial;
      }

      .use-motion .logo-line-after i
      {
        right: initial;
      }

    </style>
  </noscript>
  <link rel="alternate" href="/atom.xml" title="静觅" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner">
        <div class="site-brand-container">
          <div class="site-nav-toggle">
            <div class="toggle" aria-label="切换导航栏">
              <span class="toggle-line toggle-line-first"></span>
              <span class="toggle-line toggle-line-middle"></span>
              <span class="toggle-line toggle-line-last"></span>
            </div>
          </div>
          <div class="site-meta">
            <a href="/" class="brand" rel="start">
              <span class="logo-line-before"><i></i></span>
              <h1 class="site-title">静觅 <span class="site-subtitle"> 崔庆才的个人站点 </span>
              </h1>
              <span class="logo-line-after"><i></i></span>
            </a>
          </div>
          <div class="site-nav-right">
            <div class="toggle popup-trigger">
              <i class="fa fa-search fa-fw fa-lg"></i>
            </div>
          </div>
        </div>
        <nav class="site-nav">
          <ul id="menu" class="main-menu menu">
            <li class="menu-item menu-item-home">
              <a href="/" rel="section">首页</a>
            </li>
            <li class="menu-item menu-item-archives">
              <a href="/archives/" rel="section">文章列表</a>
            </li>
            <li class="menu-item menu-item-tags">
              <a href="/tags/" rel="section">文章标签</a>
            </li>
            <li class="menu-item menu-item-categories">
              <a href="/categories/" rel="section">文章分类</a>
            </li>
            <li class="menu-item menu-item-about">
              <a href="/about/" rel="section">关于博主</a>
            </li>
            <li class="menu-item menu-item-message">
              <a href="/message/" rel="section">给我留言</a>
            </li>
            <li class="menu-item menu-item-search">
              <a role="button" class="popup-trigger">搜索 </a>
            </li>
          </ul>
        </nav>
        <div class="search-pop-overlay">
          <div class="popup search-popup">
            <div class="search-header">
              <span class="search-icon">
                <i class="fa fa-search"></i>
              </span>
              <div class="search-input-container">
                <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input">
              </div>
              <span class="popup-btn-close">
                <i class="fa fa-times-circle"></i>
              </span>
            </div>
            <div id="search-result">
              <div id="no-result">
                <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
              </div>
            </div>
          </div>
        </div>
      </div>
    </header>
    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
      <span>0%</span>
    </div>
    <div class="reading-progress-bar"></div>
    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div class="content index posts-expand">
            <div class="carousel">
              <div id="wowslider-container">
                <div class="ws_images">
                  <ul>
                    <li><a target="_blank" href="https://cuiqingcai.com/5052.html"><img title="Python3网络爬虫开发实战教程" src="https://qiniu.cuiqingcai.com/ipy96.jpg" /></a></li>
                    <li><a target="_blank" href="https://t.lagou.com/fRCBRsRCSN6FA"><img title="52讲轻松搞定网络爬虫" src="https://qiniu.cuiqingcai.com/fqq5e.png" /></a></li>
                    <li><a target="_blank" href="https://cuiqingcai.com/4320.html"><img title="Python3网络爬虫开发视频教程" src="https://qiniu.cuiqingcai.com/bjrny.jpg" /></a></li>
                    <li><a target="_blank" href="https://cuiqingcai.com/1052.html"><img title="Python2爬虫学习系列教程" src="https://qiniu.cuiqingcai.com/uyl5v.jpg" /></a></li>
                    <li><a target="_blank" href="https://cuiqingcai.com/5094.html"><img title="爬虫代理哪家强？十大付费代理详细对比评测出炉！" src="https://qiniu.cuiqingcai.com/nifs6.jpg" /></a></li>
                  </ul>
                </div>
                <div class="ws_thumbs">
                  <div>
                    <a target="_blank" href="#"><img src="https://qiniu.cuiqingcai.com/ipy96.jpg" /></a>
                    <a target="_blank" href="#"><img src="https://qiniu.cuiqingcai.com/fqq5e.png" /></a>
                    <a target="_blank" href="#"><img src="https://qiniu.cuiqingcai.com/bjrny.jpg" /></a>
                    <a target="_blank" href="#"><img src="https://qiniu.cuiqingcai.com/uyl5v.jpg" /></a>
                    <a target="_blank" href="#"><img src="https://qiniu.cuiqingcai.com/nifs6.jpg" /></a>
                  </div>
                </div>
                <div class="ws_shadow"></div>
              </div>
            </div>
            <link rel="stylesheet" href="/lib/wowslide/slide.css">
            <script src="/lib/wowslide/jquery.min.js"></script>
            <script src="/lib/wowslide/slider.js"></script>
            <script>
              jQuery("#wowslider-container").wowSlider(
              {
                effect: "cube",
                prev: "",
                next: "",
                duration: 20 * 100,
                delay: 20 * 100,
                width: 716,
                height: 297,
                autoPlay: true,
                playPause: true,
                stopOnHover: false,
                loop: false,
                bullets: 0,
                caption: true,
                captionEffect: "slide",
                controls: true,
                onBeforeStep: 0,
                images: 0
              });

            </script>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8627.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8627.html" class="post-title-link" itemprop="url">严选高质量文章 - 爬虫工程师必看，深入解读字体反爬虫</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>内容选自<strong>即将出版</strong>的《Python3 反爬虫原理与绕过实战》，本次公开书稿范围为第 6 章——文本混淆反爬虫。本篇为第 6 章中的第 4 小节，其余小节将<strong>逐步放送</strong>。 <img src="http://can.sfhfpc.com/sfhfpc/20191226081009.jpg" alt=""></p>
                  <h2 id="字体反爬虫开篇概述"><a href="#字体反爬虫开篇概述" class="headerlink" title="字体反爬虫开篇概述"></a>字体反爬虫开篇概述</h2>
                  <p>在 CSS3 之前，Web 开发者必须使用用户计算机上已有的字体。但是在 CSS3 时代，开发者可以使用@font-face 为网页指定字体，对用户计算机字体的依赖。开发者可将心仪的字体文件放在 Web 服务器上，并在 CSS 样式中使用它。用户使用浏览器访问 Web 应用时，对应的字体会被浏览器下载到用户的计算机上。 在学习浏览器和页面渲染的相关知识时，我们了解到 CSS 的作用是修饰 HTML ，所以在页面渲染的时候不会改变 HTML 文档内容。由于字体的加载和映射工作是由 CSS 完成的，所以即使我们借助 Splash、Selenium 和 Puppeteer 工具也无法获得对应的文字内容。字体反爬虫正是利用了这个特点，将自定义字体应用到网页中重要的数据上，使得爬虫程序无法获得正确的数据。</p>
                  <h3 id="6-4-1-字体反爬虫示例"><a href="#6-4-1-字体反爬虫示例" class="headerlink" title="6.4.1 字体反爬虫示例"></a>6.4.1 字体反爬虫示例</h3>
                  <p>示例 7：字体反爬虫示例。 网址：<a href="http://www.porters.vip/confusion/movie.html" target="_blank" rel="noopener">http://www.porters.vip/confusion/movie.html</a>。 任务：爬取影片信息展示页中的影片评分、评价人数和票房数据，页面内容如图 6-32 所示。 <img src="http://can.sfhfpc.com/sfhfpc/20191226063524.jpg" alt=""> 图 6-32 示例 7 页面 在编写代码之前，我们需要确定目标数据的元素定位。定位时，我们在 HTML 中发现了一些奇怪的符号，HTML 代码如下：</p>
                  <figure class="highlight applescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;<span class="keyword">div</span> <span class="built_in">class</span>=<span class="string">"movie-index"</span>&gt; </span><br><span class="line">   &lt;p <span class="built_in">class</span>=<span class="string">"movie-index-title"</span>&gt;用户评分&lt;/p&gt; </span><br><span class="line">   &lt;<span class="keyword">div</span> <span class="built_in">class</span>=<span class="string">"movie-index-content score normal-score"</span>&gt; </span><br><span class="line">       &lt;span <span class="built_in">class</span>=<span class="string">"index-left info-num "</span>&gt; </span><br><span class="line">       &lt;span <span class="built_in">class</span>=<span class="string">"stonefont"</span>&gt; ☒.☒ &lt;/span&gt; </span><br><span class="line">       &lt;/span&gt; </span><br><span class="line">   &lt;<span class="keyword">div</span> <span class="built_in">class</span>=<span class="string">"index-right"</span>&gt; </span><br><span class="line">   &lt;<span class="keyword">div</span> <span class="built_in">class</span>=<span class="string">"star-wrapper"</span>&gt; </span><br><span class="line">   &lt;<span class="keyword">div</span> <span class="built_in">class</span>=<span class="string">"star-on"</span> style=<span class="string">"width:90%;"</span>&gt;&lt;/<span class="keyword">div</span>&gt; </span><br><span class="line">   &lt;/<span class="keyword">div</span>&gt; </span><br><span class="line">        &lt;span <span class="built_in">class</span>=<span class="string">"score-num"</span>&gt;&lt;span <span class="built_in">class</span>=<span class="string">"stonefont"</span>&gt; ☒☒. ☒☒ 万&lt;/span&gt;人评分&lt;/span&gt; </span><br><span class="line">   &lt;/<span class="keyword">div</span>&gt; </span><br><span class="line">   &lt;/<span class="keyword">div</span>&gt; </span><br><span class="line">&lt;/<span class="keyword">div</span>&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>页面中重要的数据都是一些奇怪的字符，本应该显示“9.7”的地方在 HTML 中显示的是“☒.☒”，而本应该显示“56.83”的地方在 HTML 中显示的是“☒☒.☒☒”。与 6.3 节中的映射反爬虫不同，案例中的文字都被“☒”符号代替了，根本无法分辨。这就很奇怪了，“☒”能代表这么多种数字吗？ 要注意的是，Chrome 开发者工具的元素面板中显示的内容不一定是相应正文的原文，要想知道“☒”符号是什么，还需要到网页源代码中确认。对应的网页源代码如下：</p>
                  <figure class="highlight applescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;<span class="keyword">div</span> <span class="built_in">class</span>=<span class="string">"movie-index"</span>&gt;</span><br><span class="line">    &lt;p <span class="built_in">class</span>=<span class="string">"movie-index-title"</span>&gt;用户评分&lt;/p&gt;</span><br><span class="line">    &lt;<span class="keyword">div</span> <span class="built_in">class</span>=<span class="string">"movie-index-content score normal-score"</span>&gt;</span><br><span class="line">        &lt;span <span class="built_in">class</span>=<span class="string">"index-left info-num "</span>&gt;</span><br><span class="line">            &lt;span <span class="built_in">class</span>=<span class="string">"stonefont"</span>&gt;&amp;<span class="comment">#xe624.&amp;#xe9c7&lt;/span&gt;</span></span><br><span class="line">        &lt;/span&gt;</span><br><span class="line">        &lt;<span class="keyword">div</span> <span class="built_in">class</span>=<span class="string">"index-right"</span>&gt;</span><br><span class="line">          &lt;<span class="keyword">div</span> <span class="built_in">class</span>=<span class="string">"star-wrapper"</span>&gt;</span><br><span class="line">            &lt;<span class="keyword">div</span> <span class="built_in">class</span>=<span class="string">"star-on"</span> style=<span class="string">"width:90%;"</span>&gt;&lt;/<span class="keyword">div</span>&gt;</span><br><span class="line">          &lt;/<span class="keyword">div</span>&gt;</span><br><span class="line">          &lt;span <span class="built_in">class</span>=<span class="string">"score-num"</span>&gt;&lt;span <span class="built_in">class</span>=<span class="string">"stonefont"</span>&gt;&amp;<span class="comment">#xf593&amp;#xe9c7&amp;#xe9c7.&amp;#xe624万&lt;/span&gt;人评分&lt;/span&gt;</span></span><br><span class="line">        &lt;/<span class="keyword">div</span>&gt;</span><br><span class="line">    &lt;/<span class="keyword">div</span>&gt;</span><br><span class="line">&lt;/<span class="keyword">div</span>&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>从网页源代码中看到的并不是符号，而是由&amp;#x 开头的一些字符，这与示例 6 中的 SVG 映射反爬虫非常相似。我们将页面显示的数字与网页源代码中的字符进行比较，映射关系如图 6-33 所示。 <img src="http://can.sfhfpc.com/sfhfpc/20191226064028.jpg" alt=""> 图 6-33 字符与数字的映射关系 字符与数字是一一对应的，我们只需要多找一些页面，将 0 ~ 9 数字对应的字符凑齐即可。但如果目标网站的字体是动态变化的呢？映射关系也是变化的呢？ 根据 6.3 节的学习和分析，我们知道人为映射并不能解决这些问题，必须找到映射关系的规律，并使用 Python 代码实现映射算法才行。继续往下分析，难道字符映射是先异步加载数据再使用 JavaScript 渲染的？ <img src="http://can.sfhfpc.com/sfhfpc/20191226064115.jpg" alt=""> 图 6-34 请求记录 网络请求记录如图 6-34 所示，请求记录中并没有发现异步请求，这个猜测并没有得到证实。CSS 样式方面有没有线索呢？页面中包裹符号的标签的 class 属性值都是 stonefont：</p>
                  <figure class="highlight clean">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;span <span class="keyword">class</span>=<span class="string">"stonefont"</span>&gt;&amp;#xe624.&amp;#xe9c7&lt;/span&gt; </span><br><span class="line">&lt;span <span class="keyword">class</span>=<span class="string">"stonefont"</span>&gt;&amp;#xf593&amp;#xe9c7&amp;#xe9c7.&amp;#xe624 万&lt;/span&gt; </span><br><span class="line">&lt;span <span class="keyword">class</span>=<span class="string">"stonefont"</span>&gt;&amp;#xea16&amp;#xe339.&amp;#xefd4&amp;#xf19a&lt;/span&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>但对应的 CSS 样式中仅设置了字体：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-class">.stonefont</span> &#123; </span><br><span class="line">    <span class="attribute">font-family</span>: stonefont; </span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>既然是自定义字体，就意味着会加载字体文件，我们可以在网络请求中找到加载的字体文件 movie.woff，并将其下载到本地，接着使用百度字体编辑器看一看里面的内容。 百度字体编辑器 FontEditor （详见 <a href="http://fontstore.baidu.com/static/editor/index.html）是一款在线字体编辑软件，能够打开本地或者远程的" target="_blank" rel="noopener">http://fontstore.baidu.com/static/editor/index.html）是一款在线字体编辑软件，能够打开本地或者远程的</a> ttf、woff、eot、otf 格式的字体文件，具备这些格式字体文件的导入和导出功能，并且提供字形编辑、轮廓编辑和字体实时预览功能，界面如图 6-35 所示。 <img src="http://can.sfhfpc.com/sfhfpc/20191226064238.jpg" alt=""> 图 6-35 百度字体编辑器界面 打开页面后，将 movie.woff 文件拖曳到百度字体编辑器的灰色区域即可，字体文件内容如图 6-36 所示。 <img src="http://can.sfhfpc.com/sfhfpc/20191226064335.jpg" alt=""> 图 6-36 字体文件 movie.woff 预览 该字体文件中共有 12 个字体块，其中包括 2 个空白字体块和 0 ~ 9 的数字字体块。我们可以大胆地猜测，评分数据和票房数据中使用的数字正是从此而来。 由此看来，我们还需要了解一些字体文件格式相关的知识，在了解文件格式和规律后，才能够找到更合理的解决办法。</p>
                  <h3 id="6-4-2-字体文件-WOFF"><a href="#6-4-2-字体文件-WOFF" class="headerlink" title="6.4.2 字体文件 WOFF"></a>6.4.2 字体文件 WOFF</h3>
                  <p>WOFF（Web Open Font Format，Web 开放字体格式）是一种网页所采用的字体格式标准。本质上基于 SFNT 字体（如 TrueType），所以它具备 TrueType 的字体结构，我们只需要了解 TrueType 字体的相关知识即可。 TrueType 字体是苹果公司与微软公司联合开发的一种计算机轮廓字体，TrueType 字体中的每个字形由网格上的一系列点描述，点是字体中的最小单位，字形与点的关系如图 6-37 所示。 <img src="http://can.sfhfpc.com/sfhfpc/20191226064442.jpg" alt=""> 图 6-37 字形与点的关系 字体文件中不仅包含字形数据和点信息，还包括字符到字形映射、字体标题、命名和水平指标等，这些信息存在对应的表中，所以我们也可以认为 TrueType 字体文件由一系列的表组成，其中常用的表 及其作用如图 6-38 所示。 <img src="http://can.sfhfpc.com/sfhfpc/20191226064528.jpg" alt=""> 图 6-38 构成字体文件的常用表及其作用 如何查看这些表的结构和所包含的信息呢？我们可以借助第三方 Python 库 fonttools 将 WOFF 等字体文件转换成 XML 文件，这样就能查看字体文件的结构和表信息了。首先我们要安装 fonttools 库， 安装命令为：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">$ pip <span class="keyword">install</span> fonttools</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>安装完毕后就可以利用该库转换文件类型，对应的 Python 代码为：</p>
                  <figure class="highlight clean">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> fontTools.ttLib <span class="keyword">import</span> TTFont </span><br><span class="line">font = TTFont(<span class="string">'movie.woff'</span>) # 打开当前目录的 movie.woff 文件</span><br><span class="line">font.saveXML(<span class="string">'movie.xml'</span>) # 另存为 movie.xml</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>代码运行后就会在当前目录生成名为 movie 的 XML 文件。文件中字符到字形映射表 cmap 的内容如下：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;cmap_format_4 platformID=<span class="string">"0"</span> platEncID=<span class="string">"3"</span> language=<span class="string">"0"</span>&gt; </span><br><span class="line">   &lt;<span class="meta">map</span> <span class="meta">code</span>=<span class="string">"0x78"</span> name=<span class="string">"x"</span>/&gt; </span><br><span class="line">   &lt;<span class="meta">map</span> <span class="meta">code</span>=<span class="string">"0xe339"</span> name=<span class="string">"uniE339"</span>/&gt; </span><br><span class="line">   &lt;<span class="meta">map</span> <span class="meta">code</span>=<span class="string">"0xe624"</span> name=<span class="string">"uniE624"</span>/&gt; </span><br><span class="line">   &lt;<span class="meta">map</span> <span class="meta">code</span>=<span class="string">"0xe7df"</span> name=<span class="string">"uniE7DF"</span>/&gt; </span><br><span class="line">   &lt;<span class="meta">map</span> <span class="meta">code</span>=<span class="string">"0xe9c7"</span> name=<span class="string">"uniE9C7"</span>/&gt; </span><br><span class="line">   &lt;<span class="meta">map</span> <span class="meta">code</span>=<span class="string">"0xea16"</span> name=<span class="string">"uniEA16"</span>/&gt; </span><br><span class="line">   &lt;<span class="meta">map</span> <span class="meta">code</span>=<span class="string">"0xee76"</span> name=<span class="string">"uniEE76"</span>/&gt; </span><br><span class="line">   &lt;<span class="meta">map</span> <span class="meta">code</span>=<span class="string">"0xefd4"</span> name=<span class="string">"uniEFD4"</span>/&gt; </span><br><span class="line">   &lt;<span class="meta">map</span> <span class="meta">code</span>=<span class="string">"0xf19a"</span> name=<span class="string">"uniF19A"</span>/&gt; </span><br><span class="line">   &lt;<span class="meta">map</span> <span class="meta">code</span>=<span class="string">"0xf57b"</span> name=<span class="string">"uniF57B"</span>/&gt; </span><br><span class="line">   &lt;<span class="meta">map</span> <span class="meta">code</span>=<span class="string">"0xf593"</span> name=<span class="string">"uniF593"</span>/&gt; </span><br><span class="line">&lt;/cmap_format_4&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>map 标签中的 code 代表字符，name 代表字形名称，关系如图 6-39 所示。 <img src="http://can.sfhfpc.com/sfhfpc/20191226064718.jpg" alt=""> 图 6-39 字符到字形映射关系示例 XML 中的字符 0xe339 与网页源代码中的字符 对应，这样我们就确定了 HTML 中的字符码与 movie.woff 字体文件中对应的字形关系。字形数据存储在 glyf 表中，每个字形的数据都是独立的，例如字形 uniE339 的字形数据如下：</p>
                  <figure class="highlight vim">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;TTGlyph name=<span class="string">"uniE339"</span> xMin=<span class="string">"0"</span> yMin=<span class="string">"-12"</span> xMax=<span class="string">"510"</span> yMax=<span class="string">"719"</span>&gt; </span><br><span class="line">   <span class="symbol">&lt;contour&gt;</span> </span><br><span class="line">     &lt;<span class="keyword">pt</span> <span class="keyword">x</span>=<span class="string">"410"</span> <span class="keyword">y</span>=<span class="string">"534"</span> <span class="keyword">on</span>=<span class="string">"1"</span>/&gt; </span><br><span class="line">     &lt;<span class="keyword">pt</span> <span class="keyword">x</span>=<span class="string">"398"</span> <span class="keyword">y</span>=<span class="string">"586"</span> <span class="keyword">on</span>=<span class="string">"0"</span>/&gt; </span><br><span class="line">     &lt;<span class="keyword">pt</span> <span class="keyword">x</span>=<span class="string">"377"</span> <span class="keyword">y</span>=<span class="string">"609"</span> <span class="keyword">on</span>=<span class="string">"1"</span>/&gt; </span><br><span class="line">     &lt;<span class="keyword">pt</span> <span class="keyword">x</span>=<span class="string">"341"</span> <span class="keyword">y</span>=<span class="string">"646"</span> <span class="keyword">on</span>=<span class="string">"0"</span>/&gt; </span><br><span class="line">     &lt;<span class="keyword">pt</span> <span class="keyword">x</span>=<span class="string">"289"</span> <span class="keyword">y</span>=<span class="string">"646"</span> <span class="keyword">on</span>=<span class="string">"1"</span>/&gt; </span><br><span class="line">     ... </span><br><span class="line">   &lt;/contour&gt; </span><br><span class="line">   <span class="symbol">&lt;contour&gt;</span> </span><br><span class="line">     &lt;<span class="keyword">pt</span> <span class="keyword">x</span>=<span class="string">"139"</span> <span class="keyword">y</span>=<span class="string">"232"</span> <span class="keyword">on</span>=<span class="string">"1"</span>/&gt; </span><br><span class="line">     &lt;<span class="keyword">pt</span> <span class="keyword">x</span>=<span class="string">"139"</span> <span class="keyword">y</span>=<span class="string">"188"</span> <span class="keyword">on</span>=<span class="string">"0"</span>/&gt; </span><br><span class="line">     &lt;<span class="keyword">pt</span> <span class="keyword">x</span>=<span class="string">"178"</span> <span class="keyword">y</span>=<span class="string">"103"</span> <span class="keyword">on</span>=<span class="string">"0"</span>/&gt; </span><br><span class="line">     ... </span><br><span class="line">   &lt;/contour&gt; </span><br><span class="line">   &lt;instructions/&gt; </span><br><span class="line">&lt;/TTGlyph&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>TTGlyph 标签中记录着字形的名称、<em>x</em> 轴坐标和 <em>y</em> 轴坐标（坐标也可以理解为字形的宽高）。contour 标签记录的是字形的轮廓信息，也就是多个点的坐标位置，正是这些点构成了如图 6-40 所示的字形。 <img src="http://can.sfhfpc.com/sfhfpc/20191226064821.jpg" alt=""> 图 6-40 字形 uniE339 的轮廓 我们可以在百度字体编辑器中调整点的位置，然后保存字体文件并将新字体文件转换为 XML 格式，相同名称的字形数据如下：</p>
                  <figure class="highlight applescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;TTGlyph <span class="built_in">name</span>=<span class="string">"uniE339"</span> xMin=<span class="string">"115"</span> yMin=<span class="string">"6"</span> xMax=<span class="string">"430"</span> yMax=<span class="string">"495"</span>&gt; </span><br><span class="line"> &lt;contour&gt; </span><br><span class="line">   &lt;pt x=<span class="string">"400"</span> y=<span class="string">"352"</span> <span class="keyword">on</span>=<span class="string">"1"</span>/&gt; </span><br><span class="line">   &lt;pt x=<span class="string">"356"</span> y=<span class="string">"406"</span> <span class="keyword">on</span>=<span class="string">"0"</span>/&gt; </span><br><span class="line">   &lt;pt x=<span class="string">"342"</span> y=<span class="string">"421"</span> <span class="keyword">on</span>=<span class="string">"1"</span>/&gt; </span><br><span class="line">   &lt;pt x=<span class="string">"318"</span> y=<span class="string">"446"</span> <span class="keyword">on</span>=<span class="string">"0"</span>/&gt; </span><br><span class="line">   &lt;pt x=<span class="string">"283"</span> y=<span class="string">"446"</span> <span class="keyword">on</span>=<span class="string">"1"</span>/&gt; </span><br><span class="line">   ... </span><br><span class="line"> &lt;/contour&gt; </span><br><span class="line"> &lt;instructions/&gt; </span><br><span class="line">&lt;/TTGlyph&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>接着将调整前的字形数据和调整后的字形数据进行对比。 如图 6-41 所示，点的位置调整后，字形数据也会发生相应的变化，如 xMin、xMax、yMin、yMax 还有 pt 标签中的 x 坐标 y 坐标都与之前的不同了。 <img src="http://can.sfhfpc.com/sfhfpc/20191226064932.jpg" alt=""> 图 6-41 字形数据对比 XML 文件中记录的是字形坐标信息，实际上，我们没有办法直接通过字形数据获得文字，只能从其他方面想办法。虽然目标网站使用多套字体，但相同文字的字形也是相同的。比如现在有 movie.woff 和 food.woff 这两套字体，它们包含的字形如下：</p>
                  <figure class="highlight applescript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># movie.woff </span></span><br><span class="line"><span class="comment"># 包含 10 个字形数据：[0123456789] </span></span><br><span class="line">&lt;cmap_format_4 platformID=<span class="string">"0"</span> platEncID=<span class="string">"3"</span> language=<span class="string">"0"</span>&gt; </span><br><span class="line">   &lt;map code=<span class="string">"0x78"</span> <span class="built_in">name</span>=<span class="string">"x"</span>/&gt; </span><br><span class="line">   &lt;map code=<span class="string">"0xe339"</span> <span class="built_in">name</span>=<span class="string">"uniE339"</span>/&gt; <span class="comment"># 数字 6 </span></span><br><span class="line">   &lt;map code=<span class="string">"0xe624"</span> <span class="built_in">name</span>=<span class="string">"uniE624"</span>/&gt; <span class="comment"># 数字 9 </span></span><br><span class="line">   &lt;map code=<span class="string">"0xe7df"</span> <span class="built_in">name</span>=<span class="string">"uniE7DF"</span>/&gt; <span class="comment"># 数字 2 </span></span><br><span class="line">   &lt;map code=<span class="string">"0xe9c7"</span> <span class="built_in">name</span>=<span class="string">"uniE9C7"</span>/&gt; <span class="comment"># 数字 7 </span></span><br><span class="line">   &lt;map code=<span class="string">"0xea16"</span> <span class="built_in">name</span>=<span class="string">"uniEA16"</span>/&gt; <span class="comment"># 数字 5 </span></span><br><span class="line">   &lt;map code=<span class="string">"0xee76"</span> <span class="built_in">name</span>=<span class="string">"uniEE76"</span>/&gt; <span class="comment"># 数字 0 </span></span><br><span class="line">   &lt;map code=<span class="string">"0xefd4"</span> <span class="built_in">name</span>=<span class="string">"uniEFD4"</span>/&gt; <span class="comment"># 数字 8 </span></span><br><span class="line">   &lt;map code=<span class="string">"0xf19a"</span> <span class="built_in">name</span>=<span class="string">"uniF19A"</span>/&gt; <span class="comment"># 数字 3 </span></span><br><span class="line">   &lt;map code=<span class="string">"0xf57b"</span> <span class="built_in">name</span>=<span class="string">"uniF57B"</span>/&gt; <span class="comment"># 数字 1</span></span><br><span class="line">   &lt;map code=<span class="string">"0xf593"</span> <span class="built_in">name</span>=<span class="string">"uniF593"</span>/&gt; <span class="comment"># 数字 4 </span></span><br><span class="line">&lt;/cmap_format_4&gt; </span><br><span class="line"></span><br><span class="line"><span class="comment"># food.woff </span></span><br><span class="line"><span class="comment"># 包含 3 个字形数据：[012] </span></span><br><span class="line">&lt;cmap_format_4 platformID=<span class="string">"0"</span> platEncID=<span class="string">"3"</span> language=<span class="string">"0"</span>&gt; </span><br><span class="line">   &lt;map code=<span class="string">"0x78"</span> <span class="built_in">name</span>=<span class="string">"x"</span>/&gt; </span><br><span class="line">   &lt;map code=<span class="string">"0xe556"</span> <span class="built_in">name</span>=<span class="string">"uniE556"</span>/&gt; <span class="comment"># 数字 0 </span></span><br><span class="line">   &lt;map code=<span class="string">"0xe667"</span> <span class="built_in">name</span>=<span class="string">"uniE667"</span>/&gt; <span class="comment"># 数字 1 </span></span><br><span class="line">   &lt;map code=<span class="string">"0xe778"</span> <span class="built_in">name</span>=<span class="string">"uniE778"</span>/&gt; <span class="comment"># 数字 2 </span></span><br><span class="line">&lt;/cmap_format_4&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>要实现自动识别文字，需要先准备参照字形，也就是人为地准备数字 0 ~ 9 的字形映射关系和字形数据，如：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># 0 和 7 与字形名称的映射伪代码，data 键对应的值是字形数据</span></span><br><span class="line"><span class="attr">font_mapping</span> = [ </span><br><span class="line">   &#123;<span class="string">'name'</span>: <span class="string">'uniE9C7'</span>, <span class="string">'words'</span>: <span class="string">'7'</span>, <span class="string">'data'</span>: <span class="string">'uniE9C7_contour_pt'</span>&#125;, </span><br><span class="line">   &#123;<span class="string">'name'</span>: <span class="string">'uniEE76'</span>, <span class="string">'words'</span>: <span class="string">'0'</span>, <span class="string">'data'</span>: <span class="string">'uniEE76_countr_pt'</span>&#125;, </span><br><span class="line">]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当我们遇到目标网站上其他字体文件时，就可以使用参照字形中的字形数据与目标字形进行匹配，如果字形数据非常接近，就认为这两个字形描述的是相同的文字。字形数据包含记录字形名称和字形起止坐标的 TTGlyph 标签以及记录点坐标的 pt 标签，起止坐标代表的是字形在画布上的位置，点坐标代表字形中每个点在画布上的位置。在起止坐标中，<em>x</em> 轴差值代表字形宽度，<em>y</em> 轴差值代表字形高度。 如图 6-42 所示，两个字形的起止坐标和宽高都有很大的差别，但是却能够描述相同的文字，所以字形在画布中的位置并不会影响描述的文字，字形宽度和字形高度也不会影响描述的文字。 <img src="http://can.sfhfpc.com/sfhfpc/20191226065110.jpg" alt=""> 图 6-42 描述相同文字的两个字形 点坐标的数量和坐标值可以作为比较条件吗？ 如图 6-43 所示，两个不同文字的字形数据是不一样的。虽然这两种字形的 name 都是 uniE9C7，但是字形数据中大部分 pt 标签 x 和 y 的差距都很大，所以我们可以判定这两个字形描述的并不是 同一个文字。你可能会想到点的数量也可以作为排除条件，也就是说如果点的数量不相同，那么这个 两个字形描述的就不是同一个文字。真的是这样吗？ <img src="http://can.sfhfpc.com/sfhfpc/20191226065656.jpg" alt=""> 图 6-43 描述不同文字的字形数据对比 在图 6-44 中，左侧描述文字 7 的字形有 17 个点，而右侧描述文字 7 的字形却有 20 个点。对应的字形信息如图 6-45 所示。 <img src="http://can.sfhfpc.com/sfhfpc/20191226065745.jpg" alt=""> 图 6-44 描述相同文字的字形 <img src="http://can.sfhfpc.com/sfhfpc/20191226065822.jpg" alt=""> 图 6-45 描述相同文字的字形信息 虽然点的数量不一样，但是它们的字形并没有太大的变化，也不会造成用户误读，所以点的数量并不能作为排除不同字形的条件。因此，只有起止坐标和点坐标数据完全相同的字形，描述的才是相同字符。</p>
                  <h3 id="6-4-3-字体反爬虫绕过实战"><a href="#6-4-3-字体反爬虫绕过实战" class="headerlink" title="6.4.3 字体反爬虫绕过实战"></a>6.4.3 字体反爬虫绕过实战</h3>
                  <p>要确定两组字形数据描述的是否为相同字符，我们必须取出 HTML 中对应的字形数据，然后将待确认的字形与我们准备好的基准字形数据进行对比。现在我们来整理一下这一系列工作的步骤。 (1) 准备基准字形描述信息。 (2) 访问目标网页。 (3) 从目标网页中读取字体编码字符。 (4) 下载 WOFF 文件并用 Python 代码打开。 (5) 根据字体编码字符找到 WOFF 文件中的字形轮廓信息。 (6) 将该字形轮廓信息与基准字形轮廓信息进行对比。 (7) 得出对比结果。 我们先完成前 4 个步骤的代码。下载 WOFF 文件并将其中字形描述的文字与人类认知的文字进行映射。由于字形数据比较庞大，所以我们可以将字形数据进行散列计算，这样得到的结果既简短又唯一，不会影响对比结果。这里以数字 0 ~ 9 为例：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">base_font </span>= &#123; </span><br><span class="line"> <span class="string">"font"</span>: [&#123;<span class="string">"name"</span>: <span class="string">"uniEE76"</span>, <span class="string">"value"</span>: <span class="string">"0"</span>, <span class="string">"hex"</span>: <span class="string">"fc170db1563e66547e9100cf7784951f"</span>&#125;, </span><br><span class="line"> &#123;<span class="string">"name"</span>: <span class="string">"uniF57B"</span>, <span class="string">"value"</span>: <span class="string">"1"</span>, <span class="string">"hex"</span>: <span class="string">"251357942c5160a003eec31c68a06f64"</span>&#125;, </span><br><span class="line"> &#123;<span class="string">"name"</span>: <span class="string">"uniE7DF"</span>, <span class="string">"value"</span>: <span class="string">"2"</span>, <span class="string">"hex"</span>: <span class="string">"8a3ab2e9ca7db2b13ce198521010bde4"</span>&#125;, </span><br><span class="line"> &#123;<span class="string">"name"</span>: <span class="string">"uniF19A"</span>, <span class="string">"value"</span>: <span class="string">"3"</span>, <span class="string">"hex"</span>: <span class="string">"712e4b5abd0ba2b09aff19be89e75146"</span>&#125;, </span><br><span class="line"> &#123;<span class="string">"name"</span>: <span class="string">"uniF593"</span>, <span class="string">"value"</span>: <span class="string">"4"</span>, <span class="string">"hex"</span>: <span class="string">"e5764c45cf9de7f0a4ada6b0370b81a1"</span>&#125;, </span><br><span class="line"> &#123;<span class="string">"name"</span>: <span class="string">"uniEA16"</span>, <span class="string">"value"</span>: <span class="string">"5"</span>, <span class="string">"hex"</span>: <span class="string">"c631abb5e408146eb1a17db4113f878f"</span>&#125;, </span><br><span class="line"> &#123;<span class="string">"name"</span>: <span class="string">"uniE339"</span>, <span class="string">"value"</span>: <span class="string">"6"</span>, <span class="string">"hex"</span>: <span class="string">"0833d3b4f61f02258217421b4e4bde24"</span>&#125;, </span><br><span class="line"> &#123;<span class="string">"name"</span>: <span class="string">"uniE9C7"</span>, <span class="string">"value"</span>: <span class="string">"7"</span>, <span class="string">"hex"</span>: <span class="string">"4aa5ac9a6741107dca4c5dd05176ec4c"</span>&#125;, </span><br><span class="line"> &#123;<span class="string">"name"</span>: <span class="string">"uniEFD4"</span>, <span class="string">"value"</span>: <span class="string">"8"</span>, <span class="string">"hex"</span>: <span class="string">"c37e95c05e0dd147b47f3cb1e5ac60d7"</span>&#125;, </span><br><span class="line"> &#123;<span class="string">"name"</span>: <span class="string">"uniE624"</span>, <span class="string">"value"</span>: <span class="string">"9"</span>, <span class="string">"hex"</span>: <span class="string">"704362b6e0feb6cd0b1303f10c000f95"</span>&#125;] </span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>字典中的 name 代表该字形的名称，value 代表该字形描述的文字，hex 代表字形信息的 MD5 值。 考虑到网络请求记录中的字体文件路径有可能会变化，我们必须找到 CSS 中设定的字体文件路径，引入 CSS 的 HTML 代码为：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&lt;link <span class="attribute">href</span>=<span class="string">"./css/movie.css"</span> <span class="attribute">rel</span>=<span class="string">"stylesheet"</span>&gt;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由引入代码得知该 CSS 文件的路径为 <a href="http://www.porters.vip/confusion/css/movie.css，文件中" target="_blank" rel="noopener">http://www.porters.vip/confusion/css/movie.css，文件中</a> @font-face 处就是设置字体的代码：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">@font-face</span> &#123; </span><br><span class="line">   <span class="attribute">font-family</span>: stonefont; </span><br><span class="line">   <span class="attribute">src</span>:<span class="built_in">url</span>(<span class="string">'../font/movie.woff'</span>) <span class="built_in">format</span>(<span class="string">'woff'</span>); </span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>字体文件路径为 <a href="http://www.porters.vip/confusion/font/movie.woff。找到文件后，我们就可以开始编写代码了，对应的" target="_blank" rel="noopener">http://www.porters.vip/confusion/font/movie.woff。找到文件后，我们就可以开始编写代码了，对应的</a> Python 代码如下：</p>
                  <figure class="highlight properties">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">import</span> <span class="string">re </span></span><br><span class="line"><span class="attr">from</span> <span class="string">parsel import Selector </span></span><br><span class="line"><span class="attr">from</span> <span class="string">urllib import parse </span></span><br><span class="line"><span class="attr">from</span> <span class="string">fontTools.ttLib import TTFont </span></span><br><span class="line"><span class="attr">url</span> = <span class="string">'http://www.porters.vip/confusion/movie.html' </span></span><br><span class="line"><span class="attr">resp</span> = <span class="string">requests.get(url) </span></span><br><span class="line"><span class="attr">sel</span> = <span class="string">Selector(resp.text) </span></span><br><span class="line"><span class="comment"># 提取页面加载的所有 css 文件路径</span></span><br><span class="line"><span class="attr">css_path</span> = <span class="string">sel.css('link[rel=stylesheet]::attr(href)').extract() </span></span><br><span class="line"><span class="attr">woffs</span> = <span class="string">[] </span></span><br><span class="line"><span class="attr">for</span> <span class="string">c in css_path: </span></span><br><span class="line"><span class="comment">   # 拼接正确的 css 文件路径</span></span><br><span class="line">   <span class="attr">css_url</span> = <span class="string">parse.urljoin(url, c) </span></span><br><span class="line"><span class="comment">   # 向 css 文件发起请求</span></span><br><span class="line">   <span class="attr">css_resp</span> = <span class="string">requests.get(css_url) </span></span><br><span class="line"><span class="comment">   # 匹配 css 文件中的 woff 文件路径</span></span><br><span class="line">   <span class="attr">woff_path</span> = <span class="string">re.findall("src:url('..(.*.woff)') format('woff');", </span></span><br><span class="line">   <span class="attr">css_resp.text)</span></span><br><span class="line">   <span class="attr">if</span> <span class="string">woff_path: </span></span><br><span class="line"><span class="comment">       # 如故路径存在则添加到 woffs 列表中</span></span><br><span class="line">       <span class="attr">woffs</span> <span class="string">+= woff_path </span></span><br><span class="line"><span class="attr">woff_url</span> = <span class="string">'http://www.porters.vip/confusion' + woffs.pop() </span></span><br><span class="line"><span class="attr">woff</span> = <span class="string">requests.get(woff_url) </span></span><br><span class="line"><span class="attr">filename</span> = <span class="string">'target.woff' </span></span><br><span class="line"><span class="attr">with</span> <span class="string">open(filename, 'wb') as f: </span></span><br><span class="line"><span class="comment">   # 将文件保存到本地</span></span><br><span class="line">   <span class="meta">f.write(woff.content)</span> <span class="string"></span></span><br><span class="line"><span class="comment"># 使用 TTFont 库打开刚才下载的 woff 文件</span></span><br><span class="line"><span class="attr">font</span> = <span class="string">TTFont(filename)</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>因为 TTFont 可以直接读取 woff 文件的结构，所以这里不需要将 woff 保存为 XML 文件。接着以评分数据 9.7 对应的编码 #xe624.#xe9c7 进行测试，在原来的代码中引入基准字体数据 base_font，然后新增以下代码：</p>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">web_code = <span class="string">'&amp;#xe624.&amp;#xe9c7'</span></span><br><span class="line"><span class="comment"># 编码文字替换</span></span><br><span class="line">woff_code = [i.<span class="built_in">upper</span>().<span class="built_in">replace</span>(<span class="string">'&amp;#X'</span>, <span class="string">'uni'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> web_code.<span class="built_in">split</span>(<span class="string">'.'</span>)] </span><br><span class="line">import hashlib </span><br><span class="line"><span class="built_in">result</span> = [] </span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> woff_code: </span><br><span class="line">   <span class="comment"># 从字体文件中取出对应编码的字形信息</span></span><br><span class="line">   content = font[<span class="string">'glyf'</span>].glyphs.<span class="built_in">get</span>(w).data </span><br><span class="line">   <span class="comment"># 字形信息 MD5 </span></span><br><span class="line">   glyph = hashlib.md5(content).hexdigest() </span><br><span class="line">   <span class="keyword">for</span> b <span class="keyword">in</span> base_font.<span class="built_in">get</span>(<span class="string">'font'</span>): </span><br><span class="line">       <span class="comment"># 与基准字形中的 MD5 值进行对比，如果相同则取出该字形描述的文字</span></span><br><span class="line">       <span class="keyword">if</span> b.<span class="built_in">get</span>(<span class="string">'hex'</span>) == glyph: </span><br><span class="line">           <span class="built_in">result</span>.append(b.<span class="built_in">get</span>(<span class="string">'value'</span>)) </span><br><span class="line">           break </span><br><span class="line"><span class="comment"># 打印映射结果</span></span><br><span class="line">print(<span class="built_in">result</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>以上代码运行结果为：</p>
                  <figure class="highlight scheme">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[<span class="symbol">'9</span>', <span class="symbol">'7</span>']</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果说明能够正确映射字体文件中字形描述的文字。</p>
                  <h3 id="6-4-4-小结"><a href="#6-4-4-小结" class="headerlink" title="6.4.4 小结"></a>6.4.4 小结</h3>
                  <p>字体反爬能给爬虫工程师带来很大的麻烦。虽然爬虫工程师找到了应对方法，但这种方法依赖的条件比较严苛，如果开发者频繁改动字体文件或准备多套字体文件并随机切换，那真是一件令爬虫工程师头疼的事。不过，这些工作对于开发者来说也不是轻松的事。</p>
                  <h2 id="新书福利"><a href="#新书福利" class="headerlink" title="新书福利"></a>新书福利</h2>
                  <p>真是翘首以盼！《Python3 反爬虫原理与绕过实战》一书终于要跟大家见面了！为了感谢大家对韦世东和本书的期待与支持，在新书发布时会举办多场送书活动和限时折扣活动。 <img src="http://can.sfhfpc.com/sfhfpc/20191226081009.jpg" alt=""> 想要与作者韦世东交流或者参加新书发布活动的朋友可以扫描二维码进群与我互动哦！</p>
                  <h3 id="转载说明"><a href="#转载说明" class="headerlink" title="转载说明"></a>转载说明</h3>
                  <p>本篇内容摘自出版图书《Python3 反爬虫原理与绕过实战》，欢迎各位好友与同行转载！ 记得带上相关的版权信息哦😊。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/韦世东学算法和反爬虫" class="author" itemprop="url" rel="index">韦世东学算法和反爬虫</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-26 08:46:01" itemprop="dateCreated datePublished" datetime="2019-12-26T08:46:01+08:00">2019-12-26</time>
                </span>
                <span id="/8627.html" class="post-meta-item leancloud_visitors" data-flag-title="严选高质量文章 - 爬虫工程师必看，深入解读字体反爬虫" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>11k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>10 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8602.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 技术杂谈 <i class="label-arrow"></i>
                  </a>
                  <a href="/8602.html" class="post-title-link" itemprop="url">如何通过 Tampermonkey 快速查找 JavaScript 加密入口</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>在很多情况下，我们可能想要在网页中自动执行某些代码，帮助我们完成一些操作。如自动抢票、自动刷单、自动爬虫等等，这些操作绝大部分都是借助 JavaScript 来实现的。那么问题来了？在浏览器里面怎样才能方便地执行我们所期望执行的 JavaScript 代码呢？在这里推荐一个插件，叫做 Tampermonkey。这个插件的功能非常强大，利用它我们几乎可以在网页中执行任何 JavaScript 代码，实现我们想要的功能。 当然不仅仅是自动抢票、自动刷单、自动爬虫，Tampermonkey 的用途远远不止这些，只要我们想要的功能能用 JavaScript 实现，Tampermonkey 就可以帮我们做到。比如我们可以将 Tampermonkey 应用到 JavaScript 逆向分析中，去帮助我们更方便地分析一些 JavaScript 加密和混淆代码。 本节我们就来介绍一下这个插件的使用方法，并结合一个实际案例，介绍下这个插件在 JavaScript 逆向分析中的用途。</p>
                  <h2 id="Tampermonkey"><a href="#Tampermonkey" class="headerlink" title="Tampermonkey"></a>Tampermonkey</h2>
                  <p>Tampermonkey，中文也叫作「油猴」，它是一款浏览器插件，支持 Chrome。利用它我们可以在浏览器加载页面时自动执行某些 JavaScript 脚本。由于执行的是 JavaScript，所以我们几乎可以在网页中完成任何我们想实现的效果，如自动爬虫、自动修改页面、自动响应事件等等。</p>
                  <h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2>
                  <p>首先我们需要安装 Tampermonkey，这里我们使用的浏览器是 Chrome。直接在 Chrome 应用商店或者在 Tampermonkey 的官网 <a href="https://www.tampermonkey.net/" target="_blank" rel="noopener">https://www.tampermonkey.net/</a> 下载安装即可。 安装完成之后，在 Chrome 浏览器的右上角会出现 Tampermonkey 的图标，这就代表安装成功了。 <img src="https://qiniu.cuiqingcai.com/2019-12-14-123401.png" alt=""></p>
                  <h2 id="获取脚本"><a href="#获取脚本" class="headerlink" title="获取脚本"></a>获取脚本</h2>
                  <p>Tampermonkey 运行的是 JavaScript 脚本，每个网站都能有对应的脚本运行，不同的脚本能完成不同的功能。这些脚本我们可以自定义，同样也可以用已经写好的很多脚本，毕竟有些轮子有了，我们就不需要再去造了。 我们可以在 <a href="https://greasyfork.org/zh-CN/scripts" target="_blank" rel="noopener">https://greasyfork.org/zh-CN/scripts</a> 这个网站找到一些非常实用的脚本，如全网视频去广告、百度云全网搜索等等，大家可以体验一下。</p>
                  <h2 id="脚本编写"><a href="#脚本编写" class="headerlink" title="脚本编写"></a>脚本编写</h2>
                  <p>除了使用别人已经写好的脚本，我们也可以自己编写脚本来实现想要的功能。编写脚本难不难呢？其实就是写 JavaScript 代码，只要懂一些 JavaScript 的语法就好了。另外除了懂 JavaScript 语法，我们还需要遵循脚本的一些写作规范，这其中就包括一些参数的设置。 下面我们就简单实现一个小的脚本，实现某个功能。 首先我们可以点击 Tampermonkey 插件图标，点击「管理面板」按钮，打开脚本管理页面。 <img src="https://qiniu.cuiqingcai.com/2019-12-14-130908.png" alt=""> 界面类似显示如下图所示。 <img src="https://qiniu.cuiqingcai.com/2019-12-14-130941.png" alt=""> 在这里显示了我们已经有的一些 Tampermonkey 脚本，包括我们自行创建的，也包括从第三方网站下载安装的。 另外这里也提供了编辑、调试、删除等管理功能，我们可以方便地对脚本进行管理。 接下来我们来创建一个新的脚本来试试，点击左侧的「+」号，会显示如图所示的页面。 <img src="https://qiniu.cuiqingcai.com/2019-12-14-131204.png" alt=""> 初始化的代码如下：</p>
                  <figure class="highlight php">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment">// ==UserScript==</span></span><br><span class="line"><span class="comment">// @name         New Userscript</span></span><br><span class="line"><span class="comment">// @namespace    http://tampermonkey.net/</span></span><br><span class="line"><span class="comment">// @version      0.1</span></span><br><span class="line"><span class="comment">// @description  try to take over the world!</span></span><br><span class="line"><span class="comment">// @author       You</span></span><br><span class="line"><span class="comment">// @match        https://www.tampermonkey.net/documentation.php?ext=dhdg</span></span><br><span class="line"><span class="comment">// @grant        none</span></span><br><span class="line"><span class="comment">// ==/UserScript==</span></span><br><span class="line"></span><br><span class="line">(<span class="function"><span class="keyword">function</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="string">'use strict'</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Your code here...</span></span><br><span class="line">&#125;)();</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里最上面是一些注释，但这些注释是非常有用的，这部分内容叫做 <code>UserScript Header</code> ，我们可以在里面配置一些脚本的信息，如名称、版本、描述、生效站点等等。 下面简单介绍下 <code>UserScript Header</code> 的一些参数定义。</p>
                  <ul>
                    <li>@name：脚本的名称，就是在控制面板显示的脚本名称。</li>
                    <li>@namespace：脚本的命名空间。</li>
                    <li>@version：脚本的版本，主要是做版本更新时用。</li>
                    <li>@author：作者。</li>
                    <li>@description：脚本描述。</li>
                    <li>@homepage, @homepageURL, @website，@source：作者主页，用于在Tampermonkey选项页面上从脚本名称点击跳转。请注意，如果 @namespace 标记以 <code>http://</code>开头，此处也要一样。</li>
                    <li>@icon, @iconURL and @defaulticon：低分辨率图标。</li>
                    <li>@icon64 and @icon64URL：64x64 高分辨率图标。</li>
                    <li>@updateURL：检查更新的网址，需要定义 @version。</li>
                    <li>@downloadURL：更新下载脚本的网址，如果定义成 <code>none</code> 就不会检查更新。</li>
                    <li>@supportURL：报告问题的网址。</li>
                    <li>
                      <p>@include：生效页面，可以配置多个，但注意这里并不支持 URL Hash。 例如：</p>
                      <figure class="highlight jboss-cli">
                        <table>
                          <tr>
                            <td class="gutter">
                              <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                            </td>
                            <td class="code">
                              <pre><span class="line"><span class="string">//</span> @include http:<span class="string">//www.tampermonkey.net/</span>*</span><br><span class="line"><span class="string">//</span> @include http:<span class="string">//</span>*</span><br><span class="line"><span class="string">//</span> @include https:<span class="string">//</span>*</span><br><span class="line"><span class="string">//</span> @include *</span><br></pre>
                            </td>
                          </tr>
                        </table>
                      </figure>
                    </li>
                    <li>
                      <p>@match：约等于 @include 标签，可以配置多个。</p>
                    </li>
                    <li>@exclude：不生效页面，可配置多个，优先级高于 @include 和 @match。</li>
                    <li>
                      <p>@require：附加脚本网址，相当于引入外部的脚本，这些脚本会在自定义脚本执行之前执行，比如引入一些必须的库，如 jQuery 等，这里可以支持配置多个 @require 参数。 例如：</p>
                      <figure class="highlight vim">
                        <table>
                          <tr>
                            <td class="gutter">
                              <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                            </td>
                            <td class="code">
                              <pre><span class="line">// @require http<span class="variable">s:</span>//code.jquery.<span class="keyword">com</span>/jquery-<span class="number">2.1</span>.<span class="number">4</span>.<span class="built_in">min</span>.js</span><br><span class="line">// @require http<span class="variable">s:</span>//code.jquery.<span class="keyword">com</span>/jquery-<span class="number">2.1</span>.<span class="number">3</span>.<span class="built_in">min</span>.js#sha256=<span class="number">23456</span>...</span><br><span class="line">// @require http<span class="variable">s:</span>//code.jquery.<span class="keyword">com</span>/jquery-<span class="number">2.1</span>.<span class="number">2</span>.<span class="built_in">min</span>.js#md5=<span class="number">34567</span>...,<span class="built_in">sha256</span>=<span class="number">6789</span>...</span><br></pre>
                            </td>
                          </tr>
                        </table>
                      </figure>
                    </li>
                    <li>
                      <p>@resource：预加载资源，可通过 GM_getResourceURL 和 GM_getResourceText 读取。</p>
                    </li>
                    <li>@connect：允许被 GM_xmlhttpRequest 访问的域名，每行一个。</li>
                    <li>@run-at：脚本注入的时刻，如页面刚加载时，某个事件发生后等等。 例如：<ul>
                        <li>document-start：尽可能地早执行此脚本。</li>
                        <li>document-body：DOM 的 body 出现时执行。</li>
                        <li>document-end：DOMContentLoaded 事件发生时或发生后后执行。</li>
                        <li>document-idle：DOMContentLoaded 事件发生后执行，即 DOM 加载完成之后执行，这是默认的选项。</li>
                        <li>context-menu：如果在浏览器上下文菜单（仅限桌面 Chrome 浏览器）中点击该脚本，则会注入该脚本。注意：如果使用此值，则将忽略所有 @include 和 @exclude 语句。</li>
                      </ul>
                    </li>
                    <li>
                      <p>@grant：用于添加 GM 函数到白名单，相当于授权某些 GM 函数的使用权限。 例如：</p>
                      <figure class="highlight pgsql">
                        <table>
                          <tr>
                            <td class="gutter">
                              <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                            </td>
                            <td class="code">
                              <pre><span class="line">// @<span class="keyword">grant</span> GM_setValue</span><br><span class="line">// @<span class="keyword">grant</span> GM_getValue</span><br><span class="line">// @<span class="keyword">grant</span> GM_setClipboard</span><br><span class="line">// @<span class="keyword">grant</span> unsafeWindow</span><br><span class="line">// @<span class="keyword">grant</span> <span class="keyword">window</span>.<span class="keyword">close</span></span><br><span class="line">// @<span class="keyword">grant</span> <span class="keyword">window</span>.focus</span><br></pre>
                            </td>
                          </tr>
                        </table>
                      </figure>
                      <p>如果没有定义过 @grant 选项，Tampermonkey 会猜测所需要的函数使用情况。</p>
                    </li>
                    <li>@noframes：此标记使脚本在主页面上运行，但不会在 iframe 上运行。</li>
                    <li>
                      <p>@nocompat：由于部分代码可能是专门为专门的浏览器所写，通过此标记，Tampermonkey 会知道脚本可以运行的浏览器。 例如：</p>
                      <figure class="highlight 1c">
                        <table>
                          <tr>
                            <td class="gutter">
                              <pre><span class="line">1</span><br></pre>
                            </td>
                            <td class="code">
                              <pre><span class="line"><span class="comment">// @nocompat Chrome</span></span><br></pre>
                            </td>
                          </tr>
                        </table>
                      </figure>
                      <p>这样就指定了脚本只在 Chrome 浏览器中运行。</p>
                    </li>
                  </ul>
                  <p>除此之外，Tampermonkey 还定义了一些 API，使得我们可以方便地完成某个操作，如：</p>
                  <ul>
                    <li>GM_log：将日志输出到控制台。</li>
                    <li>GM_setValue：将参数内容保存到 Storage 中。</li>
                    <li>GM_addValueChangeListener：为某个变量添加监听，当这个变量的值改变时，就会触发回调。</li>
                    <li>GM_xmlhttpRequest：发起 Ajax 请求。</li>
                    <li>GM_download：下载某个文件到磁盘。</li>
                    <li>GM_setClipboard：将某个内容保存到粘贴板。</li>
                  </ul>
                  <p>还有很多其他的 API，大家可以到 <a href="https://www.tampermonkey.net/documentation.php" target="_blank" rel="noopener">https://www.tampermonkey.net/documentation.php</a> 来查看更多的内容。 在 <code>UserScript Header</code> 下方是 JavaScript 函数和调用的代码，其中 <code>&#39;use strict&#39;</code> 标明代码使用 JavaScript 的严格模式，在严格模式下可以消除 Javascript 语法的一些不合理、不严谨之处，减少一些怪异行为，如不能直接使用未声明的变量，这样可以保证代码的运行安全，同时提高编译器的效率，提高运行速度。在下方 <code>// Your code here...</code> 这里我们就可以编写自己的代码了。</p>
                  <h2 id="实战-JavaScript-逆向"><a href="#实战-JavaScript-逆向" class="headerlink" title="实战 JavaScript 逆向"></a>实战 JavaScript 逆向</h2>
                  <p>下面我们来通过一个简单的 JavaScript 逆向案例来演示一下 Tampermonkey 的作用。 在 JavaScript 逆向的时候，我们经常会需要追踪某些方法的堆栈调用情况，但很多情况下，一些 JavaScript 的变量或者方法名经过混淆之后是非常难以捕捉的。 但如果我们能掌握一定的门路或规律，辅助以 Tampermonkey，就可以更轻松地找出一些 JavaScript 方法的断点位置，从而加速逆向过程。 在逆向过程中，一个非常典型的技术就是 Hook 技术。Hook 技术中文又叫做钩子技术，它就是在程序运行的过程中，对其中的某个方法进行重写，在原先的方法前后加入我们自定义的代码。相当于在系统没有调用该函数之前，钩子程序就先捕获该消息，钩子函数先得到控制权，这时钩子函数既可以加工处理（改变）该函数的执行行为，还可以强制结束消息的传递。 如果觉得比较抽象，看完下面的 Hook 案例就懂了。 例如，我们接下来使用 Tampermonkey 实现对某个 JavaScript 方法的 Hook，轻松找到某个方法执行的位置，从而快速定位到逆向入口。 接下来我们来这么一个简单的网站：<a href="https://scrape.cuiqingcai.com/login1，这个网站结构非常简单，就是一个用户名密码登录，但是不同的是，点击" target="_blank" rel="noopener">https://scrape.cuiqingcai.com/login1，这个网站结构非常简单，就是一个用户名密码登录，但是不同的是，点击</a> Submit 的时候，表单提交 POST 的内容并不是单纯的用户名和密码，而是一个加密后的 Token。 页面长这样： <img src="https://qiniu.cuiqingcai.com/2019-12-14-163945.png" alt=""> 我们随便输入用户名密码，点击登录按钮，观察一下网络请求的变化。 可以看到如下结果： <img src="https://qiniu.cuiqingcai.com/2019-12-14-165609.png" alt=""> 看到实际上控制台提交了一个 POST 请求，内容为：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"token"</span>:<span class="string">"eyJ1c2VybmFtZSI6ImFkbWluIiwicGFzc3dvcmQiOiJhZG1pbiJ9"</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>嗯，确实，没有诸如 username 和 password 的内容了，那这究竟是个啥？我要是做爬虫的话？怎么模拟登录呢？ 模拟登录的前提当然就是找到当前 token 生成的逻辑了，那么问题来了，到底这个 token 和用户名、密码什么关系呢？我们怎么来找寻其中的蛛丝马迹呢？ 这里我们就可能思考了，本身输入的是用户名和密码，但是提交的时候却变成了一个 token，经过观察 token 的内容还很像 Base64 编码。这就代表，网站可能首先将用户名密码混为了一个新的字符串，然后最后经过了一次 Base64 编码，最后将其赋值为 token 来提交了。所以，初步观察我们可以得出这么多信息。 好，那就来验证下吧，看看网站 JavaScript 代码里面是咋实现的。 接下来我们看看网站的源码，打开 Sources 面板，好家伙，看起来都是 Webpack 打包之后的内容，经过了一些混淆，类似结果如下： <img src="https://qiniu.cuiqingcai.com/2019-12-14-171158.png" alt=""> 这么多混淆代码，总不能一点点扒着看吧？这得找到猴年马月？那么遇到这种情形，这怎么去找 token 的生成位置呢？ 解决方法其实有两种。</p>
                  <h3 id="Ajax-断点"><a href="#Ajax-断点" class="headerlink" title="Ajax 断点"></a>Ajax 断点</h3>
                  <p>由于这个请求正好是一个 Ajax 请求，所以我们可以添加一个 XHR 断点监听，把 POST 的网址加到断点监听上面去，在 Sources 面板右侧添加这么一个 XHR 断点，如图所示： <img src="https://qiniu.cuiqingcai.com/2019-12-14-171458.png" alt="image-20191215011457030"> 这时候如果我们再次点击登录按钮的时候，正好发起一次 Ajax 请求，就进入到断点了，然后再看堆栈信息就可以一步步找到编码的入口了。 点击 Submit 之后，页面就进入了 Debug 状态停下来了，结果如下： <img src="https://qiniu.cuiqingcai.com/2019-12-14-171736.png" alt="image-20191215011734985"> 一步步找，我们最后其实可以找到入口其实是在 onSubmit 方法这里。但实际上，我们观察到，这里的断点的栈顶还会包括了一些 async Promise 等无关的内容，而我们真正想找的是用户名和密码经过处理，再进行 Base64 编码的地方，这些请求的调用实际上和我们找寻的入口是没有很大的关系的。 另外，如果我们想找的入口位置并不伴随这一次 Ajax 请求，这个方法就没法用了。 这个方法是奏效的，但是我们先不讲 onSubmit 方法里面究竟是什么逻辑，下一个方法再来讲。</p>
                  <h3 id="Hook-Function"><a href="#Hook-Function" class="headerlink" title="Hook Function"></a>Hook Function</h3>
                  <p>所以，这里介绍第二种可以快速定位入口的方法，那就是使用 Tampermonkey 自定义 JavaScript 实现某个 JavaScript 方法的 Hook。Hook 哪里呢？最明显的，Hook Base64 编码的位置就好了。 那么这里就涉及到一个小知识点，JavaScript 里面的 Base64 编码是怎么实现的。没错就是 btoa 方法，所以说，我们来 Hook btoa 方法就好了。 好，这里我们新建一个 Tampermonkey 脚本，内容如下：</p>
                  <figure class="highlight javascript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment">// ==UserScript==</span></span><br><span class="line"><span class="comment">// @name         HookBase64</span></span><br><span class="line"><span class="comment">// @namespace    https://scrape.cuiqingcai.com/</span></span><br><span class="line"><span class="comment">// @version      0.1</span></span><br><span class="line"><span class="comment">// @description  Hook Base64 encode function</span></span><br><span class="line"><span class="comment">// @author       Germey</span></span><br><span class="line"><span class="comment">// @match       https://scrape.cuiqingcai.com/login1</span></span><br><span class="line"><span class="comment">// @grant        none</span></span><br><span class="line"><span class="comment">// ==/UserScript==</span></span><br><span class="line">(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line"><span class="meta">    'use strict'</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">hook</span>(<span class="params">object, attr</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">var</span> func = object[attr]</span><br><span class="line">        object[attr] = <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">            <span class="built_in">console</span>.log(<span class="string">'hooked'</span>, object, attr)</span><br><span class="line">            <span class="keyword">var</span> ret = func.apply(object, <span class="built_in">arguments</span>)</span><br><span class="line">            <span class="keyword">debugger</span></span><br><span class="line">            <span class="keyword">return</span> ret</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    hook(<span class="built_in">window</span>, <span class="string">'btoa'</span>)</span><br><span class="line">&#125;)()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先我们定义了一些 UserScript Header，包括 @name、@match 等等，这里比较重要的就是 @name，表示脚本名称；另外一个就是 @match，代表脚本生效的网址。 脚本的内容如上所示。我们定义了一个 hook 方法，传入 object 和 attr 参数，意思就是 Hook object 对象的 attr 参数。例如我们如果想 Hook 一个 alert 方法，那就把 object 设置为 window，把 attr 设置为 alert 字符串。这里我们想要 Hook Base64 的编码方法，在 JavaScript 中，Based64 编码是用 btoa 方法实现的，那么这里我们就只需要 Hook window 对象的 btoa 方法就好了。 那么 Hook 是怎么实现的呢？我们来看下，首先一句 <code>var func = object[attr]</code>，相当于我们先把它赋值为一个变量，我们调用 func 方法就可以实现和原来相同的功能。接着，我们再直接改写这个方法的定义，直接改写 <code>object[attr]</code>，将其改写成一个新的方法，在新的方法中，通过 <code>func.apply</code> 方法又重新调用了原来的方法。这样我们就可以保证，前后方法的执行效果是不受什么影响的，之前这个方法该干啥就还是干啥的。但是和之前不同的是，我们自定义方法之后，现在可以在 <code>func</code> 方法执行的前后，再加入自己的代码，如 <code>console.log</code> 将信息输出到控制台，如 <code>debugger</code> 进入断点等等。这个过程中，我们先临时保存下来了 <code>func</code> 方法，然后定义一个新的方法，接管程序控制权，在其中自定义我们想要的实现，同时在新的方法里面再重新调回 <code>func</code> 方法，保证前后结果是不受影响的。所以，我们达到了在不影响原有方法效果的前提下，可以实现在方法的前后实现自定义的功能，就是 Hook 的过程。 最后，我们调用 hook 方法，传入 window 对象和 btoa 字符串，保存。 接下来刷新下页面，这时候我们就可以看到这个脚本就在当前页面生效了，如图所示。 <img src="https://qiniu.cuiqingcai.com/2019-12-14-174022.png" alt=""> 接下来，打开控制台，切换到 Sources 面板，这时候我们可以看到站点下面的资源多了一个叫做 Tampermonkey 的目录，展开之后，发现就是我们刚刚自定义的脚本。 <img src="https://qiniu.cuiqingcai.com/2019-12-14-174210.png" alt=""> 然后输入用户名、密码，点击提交。发现成功进入了断点模式停下来了，代码就卡在了我们自定义的 <code>debugger</code> 这一行代码的位置，如下图所示。 <img src="https://qiniu.cuiqingcai.com/2019-12-14-174338.png" alt=""> 成功 Hook 住了，这说明 JavaScript 代码在执行过程中调用到了 btoa 方法。 看下控制台，如下图所示。 <img src="https://qiniu.cuiqingcai.com/2019-12-14-174540.png" alt="image-20191215014538625"> 这里也输出了 window 对象和 btoa 方法，验证正确。 这样，我们就顺利找到了 Base64 编码操作这个路口，然后看看堆栈信息，也已经不会出现 async、Promise 这样的调用，很清晰地呈现了 btoa 方法逐层调用的过程，非常清晰明了了，如图所示。 <img src="https://qiniu.cuiqingcai.com/2019-12-14-174803.png" alt=""> 各个底层的 encode 方法略过，这样我们也非常顺利地找到了 onSubmit 方法里面的处理逻辑：</p>
                  <figure class="highlight javascript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">onSubmit: <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> e = c.encode(<span class="built_in">JSON</span>.stringify(<span class="keyword">this</span>.form));</span><br><span class="line">  <span class="keyword">this</span>.$http.post(a[<span class="string">"a"</span>].state.url.root, &#123;</span><br><span class="line">    token: e</span><br><span class="line">  &#125;).then((<span class="function"><span class="keyword">function</span>(<span class="params">e</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">"data"</span>, e)</span><br><span class="line">  &#125;))</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>仔细看看，encode 方法其实就是调用了一下 btoa 方法，就是一个 Base64 编码的过程。 另外堆栈信息中可以清晰地看到 Hook 的方法在执行前传入的参数值，即 arguments。另外执行的之后的结果 ret 也可以轻松地找到了，如图所示： <img src="https://qiniu.cuiqingcai.com/2019-12-23-211320.png" alt=""> 所以，现在我们知道了 token 和用户名、密码是什么关系了吧。 这里一目了然了，就是对表单进行了 JSON 序列化，然后调用了 encode 也就是 btoa 方法，并赋值为了 token，入口顺利解开。后面，我们只需要模拟这个过程就 OK 了。 所以，我们通过 Tampermonkey 自定义 JavaScript 脚本的方式实现了某个方法调用的 Hook，使得我们快速能定位到加密入口的位置，非常方便。 以后如果观察出来了一些门道，可以多使用这种方法来尝试，如 Hook encode 方法、decode 方法、stringify 方法、log 方法、alert 方法等等，简单而又高效。 以上便是通过 Tampermonkey 实现简单 Hook 的基础操作，当然这个仅仅是一个常见的基础案例，不过从中我们也可以总结出一些 Hook 的基本门道。 后面我们会继续介绍更多相关内容。</p>
                  <h2 id="参考来源"><a href="#参考来源" class="headerlink" title="参考来源"></a>参考来源</h2>
                  <ul>
                    <li>Hook 技术：<a href="https://www.jianshu.com/p/3382cc765b39" target="_blank" rel="noopener">https://www.jianshu.com/p/3382cc765b39</a></li>
                    <li>Tampermonkey：<a href="http://www.tampermonkey.net/" target="_blank" rel="noopener">http://www.tampermonkey.net/</a></li>
                    <li>Base64 编码：<a href="https://developer.mozilla.org/en-US/docs/Web/API/WindowOrWorkerGlobalScope/atob" target="_blank" rel="noopener">https://developer.mozilla.org/en-US/docs/Web/API/WindowOrWorkerGlobalScope/atob</a>。</li>
                    <li>示例网址：<a href="https://github.com/Python3WebSpider/Scrape" target="_blank" rel="noopener">https://github.com/Python3WebSpider/Scrape</a></li>
                  </ul>
                  <h2 id="注明"><a href="#注明" class="headerlink" title="注明"></a>注明</h2>
                  <p>本篇属于 JavaScript 逆向系列内容。由于某些原因，JavaScript 逆向是在爬虫中比较敏感的内容，因此文章中不会选取当前市面上任何一个商业网站作为案例，都是通过自建平台示例的方式来单独讲解某个知识点。另外大家不要将相关技术应用到非法用途，惜命惜命。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-24 05:47:08" itemprop="dateCreated datePublished" datetime="2019-12-24T05:47:08+08:00">2019-12-24</time>
                </span>
                <span id="/8602.html" class="post-meta-item leancloud_visitors" data-flag-title="如何通过 Tampermonkey 快速查找 JavaScript 加密入口" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>8.3k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>8 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8509.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8509.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 15.5–Gerapy 分布式管理</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="15-5-Gerapy-分布式管理"><a href="#15-5-Gerapy-分布式管理" class="headerlink" title="15.5 Gerapy 分布式管理"></a>15.5 Gerapy 分布式管理</h1>
                  <p>我们可以通过 Scrapyd-Client 将 Scrapy 项目部署到 Scrapyd 上，并且可以通过 Scrapyd API 来控制 Scrapy 的运行。那么，我们是否可以做到更优化？方法是否可以更方便可控？ 我们重新分析一下当前可以优化的问题。</p>
                  <ul>
                    <li>使用 Scrapyd-Client 部署时，需要在配置文件中配置好各台主机的地址，然后利用命令行执行部署过程。如果我们省去各台主机的地址配置，将命令行对接图形界面，只需要点击按钮即可实现批量部署，这样就更方便了。</li>
                    <li>使用 Scrapyd API 可以控制 Scrapy 任务的启动、终止等工作，但很多操作还是需要代码来实现，同时获取爬取日志还比较烦琐。如果我们有一个图形界面，只需要点击按钮即可启动和终止爬虫任务，同时还可以实时查看爬取日志报告，那这将大大节省我们的时间和精力。</li>
                  </ul>
                  <p>所以我们的终极目标是如下内容。</p>
                  <ul>
                    <li>更方便地控制爬虫运行</li>
                    <li>更直观地查看爬虫状态</li>
                    <li>更实时地查看爬取结果</li>
                    <li>更简单地实现项目部署</li>
                    <li>更统一地实现主机管理</li>
                  </ul>
                  <p>而这所有的工作均可通过 Gerapy 来实现。 Gerapy 是一个基于 Scrapyd、Scrapyd API、Django、Vue.js 搭建的分布式爬虫管理框架。接下来将简单介绍它的使用方法。</p>
                  <h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h3>
                  <p>在本节开始之前请确保已经正确安装好了 Gerapy，安装方式可以参考第一章。</p>
                  <h3 id="2-使用说明"><a href="#2-使用说明" class="headerlink" title="2. 使用说明"></a>2. 使用说明</h3>
                  <p>首先可以利用 gerapy 命令新建一个项目，命令如下：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">gerapy init</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样会在当前目录下生成一个 gerapy 文件夹，然后进入 gerapy 文件夹，会发现一个空的 projects 文件夹，我们后文会提及。 这时先对数据库进行初始化：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">gerapy migrate</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样即会生成一个 SQLite 数据库，数据库中会用于保存各个主机配置信息、部署版本等。 接下来启动 Gerapy 服务，命令如下：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">gerapy runserver</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样即可在默认 8000 端口上开启 Gerapy 服务，我们浏览器打开：<a href="http://localhost:8000" target="_blank" rel="noopener">http://localhost:8000</a> 即可进入 Gerapy 的管理页面，在这里提供了主机管理和项目管理的功能。 主机管理中，我们可以将各台主机的 Scrapyd 运行地址和端口添加，并加以名称标记，添加之后便会出现在主机列表中，Gerapy 会监控各台主机的运行状况并以不同的状态标识，如图 15-6 所示： <img src="./assets/15-6.jpg" alt=""> 图 15-6 主机列表 另外刚才我们提到在 gerapy 目录下有一个空的 projects 文件夹，这就是存放 Scrapy 目录的文件夹，如果我们想要部署某个 Scrapy 项目，只需要将该项目文件放到 projects 文件夹下即可。 比如这里我放了两个 Scrapy 项目，如图 15-7 所示： <img src="./assets/15-7.jpg" alt=""> 图 15-7 项目目录 这时重新回到 Gerapy 管理界面，点击项目管理，即可看到当前项目列表，如图 15-8 所示： <img src="./assets/15-8.jpg" alt=""> 图 15-8 项目列表 由于此处我有过打包和部署记录，在这里分别予以显示。 Gerapy 提供了项目在线编辑功能，我们可以点击编辑即可可视化地对项目进行编辑，如图 15-9 所示： <img src="./assets/15-9.jpg" alt=""> 图 15-9 可视化编辑 如果项目没有问题，可以点击部署进行打包和部署，部署之前需要打包项目，打包时可以指定版本描述，如图 15-10 所示： <img src="./assets/15-10.jpg" alt=""> 图 15-10 项目打包 打包完成之后可以直接点击部署按钮即可将打包好的 Scrapy 项目部署到对应的云主机上，同时也可以批量部署，如图 15-11 所示： <img src="./assets/15-11.jpg" alt=""> 图 15-11 部署页面 部署完毕之后就可以回到主机管理页面进行任务调度了，点击调度即可查看进入任务管理页面，可以当前主机所有任务的运行状态，如图 15-12 所示： <img src="./assets/15-12.jpg" alt=""> 图 15-12 任务运行状态 我们可以通过点击新任务、停止等按钮来实现任务的启动和停止等操作，同时也可以通过展开任务条目查看日志详情，如图 15-13 所示： <img src="./assets/15-13.jpg" alt=""> 图 15-13 查看日志 这样我们就可以实时查看到各个任务运行状态了。 以上便是 Gerapy 的一些功能的简单介绍，使用它我们可以更加方便地管理、部署和监控 Scrapy 项目，尤其是对分布式爬虫来说。 更多的信息可以查看 Gerapy 的 GitHub 地址：<a href="https://github.com/Gerapy" target="_blank" rel="noopener">https://github.com/Gerapy</a>。</p>
                  <h3 id="3-结语"><a href="#3-结语" class="headerlink" title="3. 结语"></a>3. 结语</h3>
                  <p>本节我们介绍了 Gerapy 的简单使用，利用它我们可以方便地实现 Scrapy 项目的部署、管理等操作，可以大大提高效率。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-12 09:28:36" itemprop="dateCreated datePublished" datetime="2019-12-12T09:28:36+08:00">2019-12-12</time>
                </span>
                <span id="/8509.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 15.5–Gerapy 分布式管理" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>1.7k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8506.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8506.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 15.4–Scrapyd 批量部署</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="15-4-Scrapyd-批量部署"><a href="#15-4-Scrapyd-批量部署" class="headerlink" title="15.4 Scrapyd 批量部署"></a>15.4 Scrapyd 批量部署</h1>
                  <p>我们在上一节实现了 Scrapyd 和 Docker 的对接，这样每台主机就不用再安装 Python 环境和安装 Scrapyd 了，直接执行一句 Docker 命令运行 Scrapyd 服务即可。但是这种做法有个前提，那就是每台主机都安装 Docker，然后再去运行 Scrapyd 服务。如果我们需要部署 10 台主机的话，工作量确实不小。 一种方案是，一台主机已经安装好各种开发环境，我们取到它的镜像，然后用镜像来批量复制多台主机，批量部署就可以轻松实现了。 另一种方案是，我们在新建主机的时候直接指定一个运行脚本，脚本里写好配置各种环境的命令，指定其在新建主机的时候自动执行，那么主机创建之后所有的环境就按照自定义的命令配置好了，这样也可以很方便地实现批量部署。 目前很多服务商都提供云主机服务，如阿里云、腾讯云、Azure、Amazon 等，不同的服务商提供了不同的批量部署云主机的方式。例如，腾讯云提供了创建自定义镜像的服务，在新建主机的时候使用自定义镜像创建新的主机即可，这样就可以批量生成多个相同的环境。Azure 提供了模板部署的服务，我们可以在模板中指定新建主机时执行的配置环境的命令，这样在主机创建之后环境就配置完成了。 本节我们就来看看这两种批量部署的方式，来实现 Docker 和 Scrapyd 服务的批量部署。</p>
                  <h3 id="1-镜像部署"><a href="#1-镜像部署" class="headerlink" title="1. 镜像部署"></a>1. 镜像部署</h3>
                  <p>以腾讯云为例进行说明。首先需要有一台已经安装好环境的云主机，Docker 和 Scrapyd 镜像均已经正确安装，Scrapyd 镜像启动加到开机启动脚本中，可以在开机时自动启动。 接下来我们来看下腾讯云下批量部署相同云服务的方法。 首先进入到腾讯云后台，可以点击更多选项制作镜像，如图 15-3 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-29-114145.png" alt=""> 图 15-3 制作镜像 然后输入镜像的一些配置信息，如图 15-4 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-29-114152.jpg" alt=""> 图 15-4 镜像配置 最后确认制作镜像即可，稍等片刻即可制作成功。 接下来我们可以创建新的主机，在新建主机时选择已经制作好的镜像即可，如图 15-5 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-29-114200.png" alt=""> 图 15-5 新建主机 后续配置过程按照提示进行即可。 配置完成之后登录新到云主机，即可看到当前主机 Docker 和 Scrapyd 镜像都已经安装好，Scrapyd 服务已经正常运行。 我们就通过自定义镜像的方式实现了相同环境的云主机的批量部署。</p>
                  <h3 id="2-模板部署"><a href="#2-模板部署" class="headerlink" title="2. 模板部署"></a>2. 模板部署</h3>
                  <p>Azure 的云主机在部署时都会使用一个部署模板，这个模板实际上是一个 JSON 文件，里面包含了很多部署时的配置选项，如主机名称、用户名、密码、主机型号等。在模板中我们可以指定新建完云主机之后执行的命令行脚本，如安装 Docker、运行镜像等。等部署工作全部完成之后，新创建的云主机就已经完成环境配置，同时运行相关服务。 这里提供一个部署 Linux 主机时自动安装 Docker 和运行 Scrapyd 镜像的模板，模板内容太多，源文件可以查看：<a href="https://github.com/Python3WebSpider/ScrapydDeploy/blob/master/azuredeploy.json。模板中" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapydDeploy/blob/master/azuredeploy.json。模板中</a> Microsoft.Compute/virtualMachines/extensions 部分有一个 commandToExecute 字段，它可以指定建立主机后自动执行的命令。这里的命令完成的是安装 Docker 并运行 Scrapyd 镜像服务的过程。 首先安装一个 Azure 组件，安装过程可以参考：<a href="https://docs.azure.cn/zh-cn/xplat-cli-install。之后就可以使用" target="_blank" rel="noopener">https://docs.azure.cn/zh-cn/xplat-cli-install。之后就可以使用</a> azure 命令行进行部署。 登录 Azure，这里登录的是中国区，命令如下：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">azure login -e AzureChinaCloud</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果没有资源组的话需要新建一个资源组，命令如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">azure<span class="built_in"> group </span>create myResourceGroup chinanorth</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中 myResourceGroup 就是资源组的名称，可以自行定义。 接下来就可以使用该模板进行部署了，命令如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">azure<span class="built_in"> group </span>deployment create --template-file azuredeploy.json myResourceGroup myDeploymentName</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里 myResourceGroup 就是资源组的名称，myDeploymentName 是部署任务的名称。 例如，部署一台 Linux 主机的过程如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">azure<span class="built_in"> group </span>deployment create --template-file azuredeploy.json MyResourceGroup SingleVMDeploy</span><br><span class="line">info:    Executing command<span class="built_in"> group </span>deployment create</span><br><span class="line">info:    Supply values <span class="keyword">for</span> the following parameters</span><br><span class="line">adminUsername:  datacrawl</span><br><span class="line">adminPassword:  DataCrawl123</span><br><span class="line">vmSize:  Standard_D2_v2</span><br><span class="line">vmName:  datacrawl-vm</span><br><span class="line">dnsLabelPrefix:  datacrawlvm</span><br><span class="line">storageAccountName:  datacrawlstorage</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行命令后会提示输入各个配置参数，如主机用户名、密码等。之后等待整个部署工作完成即可，命令行会自动退出。然后，我们登录云主机即可查看到 Docker 已经成功安装并且 Scrapyd 服务正常运行。</p>
                  <h3 id="3-结语"><a href="#3-结语" class="headerlink" title="3. 结语"></a>3. 结语</h3>
                  <p>以上内容便是批量部署的两种方法。在大规模分布式爬虫架构中，如果需要批量部署多个爬虫环境，使用如上方法可以快速批量完成环境的搭建工作，而不用再去逐个主机配置环境。 到此为止，我们解决了批量部署的问题，创建主机完毕之后即可直接使用 Scrapyd 服务。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-12 09:26:43" itemprop="dateCreated datePublished" datetime="2019-12-12T09:26:43+08:00">2019-12-12</time>
                </span>
                <span id="/8506.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 15.4–Scrapyd 批量部署" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>2.3k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8494.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8494.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 15.3–Scrapyd 对接 Docker</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="15-3-Scrapyd-对接-Docker"><a href="#15-3-Scrapyd-对接-Docker" class="headerlink" title="15.3 Scrapyd 对接 Docker"></a>15.3 Scrapyd 对接 Docker</h1>
                  <p>我们使用了 Scrapyd-Client 成功将 Scrapy 项目部署到 Scrapyd 运行，前提是需要提前在服务器上安装好 Scrapyd 并运行 Scrapyd 服务，而这个过程比较麻烦。如果同时将一个 Scrapy 项目部署到 100 台服务器上，我们需要手动配置每台服务器的 Python 环境，更改 Scrapyd 配置吗？如果这些服务器的 Python 环境是不同版本，同时还运行其他的项目，而版本冲突又会造成不必要的麻烦。 所以，我们需要解决一个痛点，那就是 Python 环境配置问题和版本冲突解决问题。如果我们将 Scrapyd 直接打包成一个 Docker 镜像，那么在服务器上只需要执行 Docker 命令就可以启动 Scrapyd 服务，这样就不用再关心 Python 环境问题，也不需要担心版本冲突问题。 接下来，我们就将 Scrapyd 打包制作成一个 Docker 镜像。</p>
                  <h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h3>
                  <p>请确保本机已经正确安装好了 Docker，如没有安装可以参考第 1 章的安装说明。</p>
                  <h3 id="2-对接-Docker"><a href="#2-对接-Docker" class="headerlink" title="2. 对接 Docker"></a>2. 对接 Docker</h3>
                  <p>接下来我们首先新建一个项目，然后新建一个 scrapyd.conf，即 Scrapyd 的配置文件，内容如下：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="section">[scrapyd]</span></span><br><span class="line"><span class="attr">eggs_dir</span>    = eggs</span><br><span class="line"><span class="attr">logs_dir</span>    = logs</span><br><span class="line"><span class="attr">items_dir</span>   =</span><br><span class="line"><span class="attr">jobs_to_keep</span> = <span class="number">5</span></span><br><span class="line"><span class="attr">dbs_dir</span>     = dbs</span><br><span class="line"><span class="attr">max_proc</span>    = <span class="number">0</span></span><br><span class="line"><span class="attr">max_proc_per_cpu</span> = <span class="number">10</span></span><br><span class="line"><span class="attr">finished_to_keep</span> = <span class="number">100</span></span><br><span class="line"><span class="attr">poll_interval</span> = <span class="number">5.0</span></span><br><span class="line"><span class="attr">bind_address</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="attr">http_port</span>   = <span class="number">6800</span></span><br><span class="line"><span class="attr">debug</span>       = <span class="literal">off</span></span><br><span class="line"><span class="attr">runner</span>      = scrapyd.runner</span><br><span class="line"><span class="attr">application</span> = scrapyd.app.application</span><br><span class="line"><span class="attr">launcher</span>    = scrapyd.launcher.Launcher</span><br><span class="line"><span class="attr">webroot</span>     = scrapyd.website.Root</span><br><span class="line"></span><br><span class="line"><span class="section">[services]</span></span><br><span class="line"><span class="attr">schedule.json</span>     = scrapyd.webservice.Schedule</span><br><span class="line"><span class="attr">cancel.json</span>       = scrapyd.webservice.Cancel</span><br><span class="line"><span class="attr">addversion.json</span>   = scrapyd.webservice.AddVersion</span><br><span class="line"><span class="attr">listprojects.json</span> = scrapyd.webservice.ListProjects</span><br><span class="line"><span class="attr">listversions.json</span> = scrapyd.webservice.ListVersions</span><br><span class="line"><span class="attr">listspiders.json</span>  = scrapyd.webservice.ListSpiders</span><br><span class="line"><span class="attr">delproject.json</span>   = scrapyd.webservice.DeleteProject</span><br><span class="line"><span class="attr">delversion.json</span>   = scrapyd.webservice.DeleteVersion</span><br><span class="line"><span class="attr">listjobs.json</span>     = scrapyd.webservice.ListJobs</span><br><span class="line"><span class="attr">daemonstatus.json</span> = scrapyd.webservice.DaemonStatus</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里实际上是修改自官方文档的配置文件：<a href="https://scrapyd.readthedocs.io/en/stable/config.html#example-configuration-file" target="_blank" rel="noopener">https://scrapyd.readthedocs.io/en/stable/config.html#example-configuration-file</a>，其中修改的地方有两个：</p>
                  <ul>
                    <li>max_proc_per_cpu = 10，原本是 4，即 CPU 单核最多运行 4 个 Scrapy 任务，也就是说 1 核的主机最多同时只能运行 4 个 Scrapy 任务，在这里设置上限为 10，也可以自行设置。</li>
                    <li>bind_address = 0.0.0.0，原本是 127.0.0.1，不能公开访问，在这里修改为 0.0.0.0 即可解除此限制。</li>
                  </ul>
                  <p>接下来新建一个 requirements.txt ，将一些 Scrapy 项目常用的库都列进去，内容如下：</p>
                  <figure class="highlight mipsasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">requests</span><br><span class="line">selenium</span><br><span class="line">aiohttp</span><br><span class="line"><span class="keyword">beautifulsoup4</span></span><br><span class="line"><span class="keyword">pyquery</span></span><br><span class="line"><span class="keyword">pymysql</span></span><br><span class="line"><span class="keyword">redis</span></span><br><span class="line"><span class="keyword">pymongo</span></span><br><span class="line"><span class="keyword">flask</span></span><br><span class="line"><span class="keyword">django</span></span><br><span class="line"><span class="keyword">scrapy</span></span><br><span class="line"><span class="keyword">scrapyd</span></span><br><span class="line"><span class="keyword">scrapyd-client</span></span><br><span class="line"><span class="keyword">scrapy-redis</span></span><br><span class="line"><span class="keyword">scrapy-splash</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果我们运行的 Scrapy 项目还有其他的库需要用到可以自行添加到此文件中。 最后我们新建一个 Dockerfile，内容如下：</p>
                  <figure class="highlight dockerfile">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">FROM</span> python:<span class="number">3.6</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /code</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /code</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> ./scrapyd.conf /etc/scrapyd/</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">6800</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> pip3 install -r requirements.txt</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> scrapyd</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第一行 FROM 是指在 python:3.6 这个镜像上构建，也就是说在构建时就已经有了 Python 3.6 的环境。 第二行 ADD 是将本地的代码放置到虚拟容器中，它有两个参数，第一个参数是 . ，即代表本地当前路径，/code 代表虚拟容器中的路径，也就是将本地项目所有内容放置到虚拟容器的 /code 目录下。 第三行 WORKDIR 是指定工作目录，在这里将刚才我们添加的代码路径设成工作路径，在这个路径下的目录结构和我们当前本地目录结构是相同的，所以可以直接执行库安装命令等。 第四行 COPY 是将当前目录下的 scrapyd.conf 文件拷贝到虚拟容器的 /etc/scrapyd/ 目录下，Scrapyd 在运行的时候会默认读取这个配置。 第五行 EXPOSE 是声明运行时容器提供服务端口，注意这里只是一个声明，在运行时不一定就会在此端口开启服务。这样的声明一是告诉使用者这个镜像服务的运行端口，以方便配置映射。另一个用处则是在运行时使用随机端口映射时，会自动随机映射 EXPOSE 的端口。 第六行 RUN 是执行某些命令，一般做一些环境准备工作，由于 Docker 虚拟容器内只有 Python3 环境，而没有我们所需要的一些 Python 库，所以在这里我们运行此命令来在虚拟容器中安装相应的 Python 库，这样项目部署到 Scrapyd 中便可以正常运行了。 第七行 CMD 是容器启动命令，在容器运行时，会直接执行此命令，在这里我们直接用 scrapyd 来启动 Scrapyd 服务。 到现在基本的工作就完成了，运行如下命令进行构建：</p>
                  <figure class="highlight mipsasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker <span class="keyword">build </span>-t <span class="keyword">scrapyd:latest </span>.</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>构建成功后即可运行测试：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker run -d -p <span class="number">6800</span>:<span class="number">6800</span> scrapyd</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行之后我们打开：<a href="http://localhost:6800" target="_blank" rel="noopener">http://localhost:6800</a> 即可观察到 Scrapyd 服务，如图 15-2 所示： <img src="https://qiniu.cuiqingcai.com/2019-11-29-114136.png" alt=""> 图 15-2 Scrapyd 主页 这样我们就完成了 Scrapyd Docker 镜像的构建并成功运行了。 然后我们可以将此镜像上传到 Docker Hub，例如我的 Docker Hub 用户名为 germey，新建了一个名为 scrapyd 的项目，首先可以打一个标签：</p>
                  <figure class="highlight crmsh">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker <span class="keyword">tag</span> <span class="title">scrapyd</span>:latest germey/scrapyd:latest</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里请自行替换成你的项目名称。 然后 Push 即可：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">docker</span> <span class="keyword">push </span>germey/scrapyd:latest</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>之后我们在其他主机运行此命令即可启动 Scrapyd 服务：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker run -d -p <span class="number">6800</span>:<span class="number">6800</span> germey/scrapyd</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>执行命令后会发现 Scrapyd 就可以成功在其他服务器上运行了。</p>
                  <h3 id="3-结语"><a href="#3-结语" class="headerlink" title="3. 结语"></a>3. 结语</h3>
                  <p>这样我们就利用 Docker 解决了 Python 环境的问题，在后一节我们再解决一个批量部署 Docker 的问题就可以解决批量部署问题了。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-11 10:13:48" itemprop="dateCreated datePublished" datetime="2019-12-11T10:13:48+08:00">2019-12-11</time>
                </span>
                <span id="/8494.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 15.3–Scrapyd 对接 Docker" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3.1k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>3 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8491.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8491.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 15.2–Scrapyd-Client 的使用</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="15-2-Scrapyd-Client-的使用"><a href="#15-2-Scrapyd-Client-的使用" class="headerlink" title="15.2 Scrapyd-Client 的使用"></a>15.2 Scrapyd-Client 的使用</h1>
                  <p>这里有现成的工具来完成部署过程，它叫作 Scrapyd-Client。本节将简单介绍使用 Scrapyd-Client 部署 Scrapy 项目的方法。</p>
                  <h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h3>
                  <p>请先确保 Scrapyd-Client 已经正确安装，安装方式可以参考第 1 章的内容。</p>
                  <h3 id="2-Scrapyd-Client-的功能"><a href="#2-Scrapyd-Client-的功能" class="headerlink" title="2. Scrapyd-Client 的功能"></a>2. Scrapyd-Client 的功能</h3>
                  <p>Scrapyd-Client 为了方便 Scrapy 项目的部署，提供两个功能：</p>
                  <ul>
                    <li>将项目打包成 Egg 文件。</li>
                    <li>将打包生成的 Egg 文件通过 addversion.json 接口部署到 Scrapyd 上。</li>
                  </ul>
                  <p>也就是说，Scrapyd-Client 帮我们把部署全部实现了，我们不需要再去关心 Egg 文件是怎样生成的，也不需要再去读 Egg 文件并请求接口上传了，这一切的操作只需要执行一个命令即可一键部署。</p>
                  <h3 id="3-Scrapyd-Client-部署"><a href="#3-Scrapyd-Client-部署" class="headerlink" title="3. Scrapyd-Client 部署"></a>3. Scrapyd-Client 部署</h3>
                  <p>要部署 Scrapy 项目，我们首先需要修改一下项目的配置文件，例如我们之前写的 Scrapy 微博爬虫项目，在项目的第一层会有一个 scrapy.cfg 文件，它的内容如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[settings]</span><br><span class="line">default = weibo.settings</span><br><span class="line"></span><br><span class="line">[deploy]</span><br><span class="line"><span class="comment">#url = http://localhost:6800/</span></span><br><span class="line">project = weibo</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们需要配置一下 deploy 部分，例如我们要将项目部署到 120.27.34.25 的 Scrapyd 上，就需要修改为如下内容：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="section">[deploy]</span></span><br><span class="line"><span class="attr">url</span> = http://<span class="number">120.27</span>.<span class="number">34.25</span>:<span class="number">6800</span>/</span><br><span class="line"><span class="attr">project</span> = weibo</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们再在 scrapy.cfg 文件所在路径执行如下命令：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapyd-deploy</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Packing <span class="keyword">version</span> <span class="number">1501682277</span></span><br><span class="line">Deploying <span class="keyword">to</span> project "weibo" <span class="keyword">in</span> http://<span class="number">120.27</span><span class="number">.34</span><span class="number">.25</span>:<span class="number">6800</span>/addversion.json</span><br><span class="line"><span class="keyword">Server</span> response (<span class="number">200</span>):</span><br><span class="line">&#123;"status": "ok", "spiders": <span class="number">1</span>, "node_name": "datacrawl-vm", "project": "weibo", "version": "1501682277"&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>返回这样的结果就代表部署成功了。 我们也可以指定项目版本，如果不指定的话默认为当前时间戳，指定的话通过 version 参数传递即可，例如：</p>
                  <figure class="highlight ada">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapyd-deploy <span class="comment">--version 201707131455</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>值得注意的是在 Python3 的 Scrapyd 1.2.0 版本中我们不要指定版本号为带字母的字符串，需要为纯数字，否则可能会出现报错。 另外如果我们有多台主机，我们可以配置各台主机的别名，例如可以修改配置文件为：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="section">[deploy:vm1]</span></span><br><span class="line"><span class="attr">url</span> = http://<span class="number">120.27</span>.<span class="number">34.24</span>:<span class="number">6800</span>/</span><br><span class="line"><span class="attr">project</span> = weibo</span><br><span class="line"></span><br><span class="line"><span class="section">[deploy:vm2]</span></span><br><span class="line"><span class="attr">url</span> = http://<span class="number">139.217</span>.<span class="number">26.30</span>:<span class="number">6800</span>/</span><br><span class="line"><span class="attr">project</span> = weibo</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>有多台主机的话就在此统一配置，一台主机对应一组配置，在 deploy 后面加上主机的别名即可，这样如果我们想将项目部署到 IP 为 139.217.26.30 的 vm2 主机，我们只需要执行如下命令：</p>
                  <figure class="highlight gcode">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapyd-deploy v<span class="name">m2</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就可以将项目部署到名称为 vm2 的主机上了。 如此一来，如果我们有多台主机，我们只需要在 scrapy.cfg 文件中配置好各台主机的 Scrapyd 地址，然后调用 scrapyd-deploy 命令加主机名称即可实现部署，非常方便。 如果 Scrapyd 设置了访问限制的话，我们可以在配置文件中加入用户名和密码的配置，同时端口修改一下，修改成 Nginx 代理端口，如在第一章我们使用的是 6801，那么这里就需要改成 6801，修改如下：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="section">[deploy:vm1]</span></span><br><span class="line"><span class="attr">url</span> = http://<span class="number">120.27</span>.<span class="number">34.24</span>:<span class="number">6801</span>/</span><br><span class="line"><span class="attr">project</span> = weibo</span><br><span class="line"><span class="attr">username</span> = admin</span><br><span class="line"><span class="attr">password</span> = admin</span><br><span class="line"></span><br><span class="line"><span class="section">[deploy:vm2]</span></span><br><span class="line"><span class="attr">url</span> = http://<span class="number">139.217</span>.<span class="number">26.30</span>:<span class="number">6801</span>/</span><br><span class="line"><span class="attr">project</span> = weibo</span><br><span class="line"><span class="attr">username</span> = germey</span><br><span class="line"><span class="attr">password</span> = germey</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样通过加入 username 和 password 字段我们就可以在部署时自动进行 Auth 验证，然后成功实现部署。</p>
                  <h3 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h3>
                  <p>本节介绍了利用 Scrapyd-Client 来方便地将项目部署到 Scrapyd 的过程，有了它部署不再是麻烦事。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-11 09:55:47" itemprop="dateCreated datePublished" datetime="2019-12-11T09:55:47+08:00">2019-12-11</time>
                </span>
                <span id="/8491.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 15.2–Scrapyd-Client 的使用" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>1.9k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>2 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8475.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8475.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 15.1–Scrapyd 分布式部署</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="15-1-Scrapyd-分布式部署"><a href="#15-1-Scrapyd-分布式部署" class="headerlink" title="15.1 Scrapyd 分布式部署"></a>15.1 Scrapyd 分布式部署</h1>
                  <p>分布式爬虫完成并可以成功运行了，但是有个环节非常烦琐，那就是代码部署。 我们设想下面的几个场景。</p>
                  <ul>
                    <li>如果采用上传文件的方式部署代码，我们首先将代码压缩，然后采用 SFTP 或 FTP 的方式将文件上传到服务器，之后再连接服务器将文件解压，每个服务器都需要这样配置。</li>
                    <li>如果采用 Git 同步的方式部署代码，我们可以先把代码 Push 到某个 Git 仓库里，然后再远程连接各台主机执行 Pull 操作，同步代码，每个服务器同样需要做一次操作。</li>
                  </ul>
                  <p>如果代码突然有更新，那我们必须更新每个服务器，而且万一哪台主机的版本没控制好，这可能会影响整体的分布式爬取状况。 所以我们需要一个更方便的工具来部署 Scrapy 项目，如果可以省去一遍遍逐个登录服务器部署的操作，那将会方便很多。 本节我们就来看看提供分布式部署的工具 Scrapyd。</p>
                  <h3 id="1-了解-Scrapyd"><a href="#1-了解-Scrapyd" class="headerlink" title="1. 了解 Scrapyd"></a>1. 了解 Scrapyd</h3>
                  <p>Scrapyd 是一个运行 Scrapy 爬虫的服务程序，它提供一系列 HTTP 接口来帮助我们部署、启动、停止、删除爬虫程序。Scrapyd 支持版本管理，同时还可以管理多个爬虫任务，利用它我们可以非常方便地完成 Scrapy 爬虫项目的部署任务调度。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保本机或服务器已经正确安装好了 Scrapyd，安装和配置的方法可以参见第 1 章的内容。</p>
                  <h3 id="3-访问-Scrapyd"><a href="#3-访问-Scrapyd" class="headerlink" title="3. 访问 Scrapyd"></a>3. 访问 Scrapyd</h3>
                  <p>安装并运行了 Scrapyd 之后，我们就可以访问服务器的 6800 端口看到一个 WebUI 页面了，例如我的服务器地址为 120.27.34.25，在上面安装好了 Scrapyd 并成功运行，那么我就可以在本地的浏览器中打开：<a href="http://120.27.34.25:6800" target="_blank" rel="noopener">http://120.27.34.25:6800</a>，就可以看到 Scrapyd 的首页，这里请自行替换成你的服务器地址查看即可，如图 15-1 所示： <img src="https://qiniu.cuiqingcai.com/2019-11-29-114054.png" alt=""> 图 15-1 Scrapyd 首页 如果可以成功访问到此页面，那么证明 Scrapyd 配置就没有问题了。</p>
                  <h3 id="4-Scrapyd-的功能"><a href="#4-Scrapyd-的功能" class="headerlink" title="4. Scrapyd 的功能"></a>4. Scrapyd 的功能</h3>
                  <p>Scrapyd 提供了一系列 HTTP 接口来实现各种操作，在这里我们可以将接口的功能梳理一下，以 Scrapyd 所在的 IP 为 120.27.34.25 为例：</p>
                  <h4 id="daemonstatus-json"><a href="#daemonstatus-json" class="headerlink" title="daemonstatus.json"></a>daemonstatus.json</h4>
                  <p>这个接口负责查看 Scrapyd 当前的服务和任务状态，我们可以用 curl 命令来请求这个接口，命令如下：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//139.217.26.30:6800/daemonstatus.json</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就会得到如下结果：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"finished"</span>: <span class="number">90</span>, <span class="attr">"running"</span>: <span class="number">9</span>, <span class="attr">"node_name"</span>: <span class="string">"datacrawl-vm"</span>, <span class="attr">"pending"</span>: <span class="number">0</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>返回结果是 Json 字符串，status 是当前运行状态， finished 代表当前已经完成的 Scrapy 任务，running 代表正在运行的 Scrapy 任务，pending 代表等待被调度的 Scrapyd 任务，node_name 就是主机的名称。</p>
                  <h4 id="addversion-json"><a href="#addversion-json" class="headerlink" title="addversion.json"></a>addversion.json</h4>
                  <p>这个接口主要是用来部署 Scrapy 项目用的，在部署的时候我们需要首先将项目打包成 Egg 文件，然后传入项目名称和部署版本。 我们可以用如下的方式实现项目部署：</p>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="keyword">http</span>://<span class="number">120.27</span><span class="number">.34</span><span class="number">.25</span>:<span class="number">6800</span>/addversion.json -F project=wenbo -F <span class="built_in">version</span>=<span class="keyword">first</span> -F egg=@weibo.egg</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里 -F 即代表添加一个参数，同时我们还需要将项目打包成 Egg 文件放到本地。 这样发出请求之后我们可以得到如下结果：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"spiders"</span>: <span class="number">3</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个结果表明部署成功，并且其中包含的 Spider 的数量为 3。 此方法部署可能比较繁琐，在后文会介绍更方便的工具来实现项目的部署。</p>
                  <h4 id="schedule-json"><a href="#schedule-json" class="headerlink" title="schedule.json"></a>schedule.json</h4>
                  <p>这个接口负责调度已部署好的 Scrapy 项目运行。 我们可以用如下接口实现任务调度：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/schedule.json -d project=weibo -d spider=weibocn</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要传入两个参数，project 即 Scrapy 项目名称，spider 即 Spider 名称。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"jobid"</span>: <span class="string">"6487ec79947edab326d6db28a2d86511e8247444"</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表 Scrapy 项目启动情况，jobid 代表当前正在运行的爬取任务代号。</p>
                  <h4 id="cancel-json"><a href="#cancel-json" class="headerlink" title="cancel.json"></a>cancel.json</h4>
                  <p>这个接口可以用来取消某个爬取任务，如果这个任务是 pending 状态，那么它将会被移除，如果这个任务是 running 状态，那么它将会被终止。 我们可以用下面的命令来取消任务的运行：</p>
                  <figure class="highlight dns">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl http://<span class="number">120.27.34.25</span>:<span class="number">6800</span>/cancel.json -d project=weibo -d job=<span class="number">6487</span>ec79947edab326d6db28a2d865<span class="number">11e8247444</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要传入两个参数，project 即项目名称，job 即爬取任务代号。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"prevstate"</span>: <span class="string">"running"</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，prevstate 代表之前的运行状态。</p>
                  <h4 id="listprojects-json"><a href="#listprojects-json" class="headerlink" title="listprojects.json"></a>listprojects.json</h4>
                  <p>这个接口用来列出部署到 Scrapyd 服务上的所有项目描述。 我们可以用下面的命令来获取 Scrapyd 服务器上的所有项目描述：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/listprojects.json</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里不需要传入任何参数。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"projects"</span>: [<span class="string">"weibo"</span>, <span class="string">"zhihu"</span>]&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，projects 是项目名称列表。</p>
                  <h4 id="listversions-json"><a href="#listversions-json" class="headerlink" title="listversions.json"></a>listversions.json</h4>
                  <p>这个接口用来获取某个项目的所有版本号，版本号是按序排列的，最后一个条目是最新的版本号。 我们可以用如下命令来获取项目的版本号：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/listversions.json?project=weibo</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要一个参数 project，就是项目的名称。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"versions"</span>: [<span class="string">"v1"</span>, <span class="string">"v2"</span>]&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，versions 是版本号列表。</p>
                  <h4 id="listspiders-json"><a href="#listspiders-json" class="headerlink" title="listspiders.json"></a>listspiders.json</h4>
                  <p>这个接口用来获取某个项目最新的一个版本的所有 Spider 名称。 我们可以用如下命令来获取项目的 Spider 名称：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/listspiders.json?project=weibo</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要一个参数 project，就是项目的名称。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>, <span class="attr">"spiders"</span>: [<span class="string">"weibocn"</span>]&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，spiders 是 Spider 名称列表。</p>
                  <h4 id="listjobs-json"><a href="#listjobs-json" class="headerlink" title="listjobs.json"></a>listjobs.json</h4>
                  <p>这个接口用来获取某个项目当前运行的所有任务详情。 我们可以用如下命令来获取所有任务详情：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/listjobs.json?project=weibo</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要一个参数 project，就是项目的名称。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>,</span><br><span class="line"> <span class="attr">"pending"</span>: [&#123;<span class="attr">"id"</span>: <span class="string">"78391cc0fcaf11e1b0090800272a6d06"</span>, <span class="attr">"spider"</span>: <span class="string">"weibocn"</span>&#125;],</span><br><span class="line"> <span class="attr">"running"</span>: [&#123;<span class="attr">"id"</span>: <span class="string">"422e608f9f28cef127b3d5ef93fe9399"</span>, <span class="attr">"spider"</span>: <span class="string">"weibocn"</span>, <span class="attr">"start_time"</span>: <span class="string">"2017-07-12 10:14:03.594664"</span>&#125;],</span><br><span class="line"> <span class="attr">"finished"</span>: [&#123;<span class="attr">"id"</span>: <span class="string">"2f16646cfcaf11e1b0090800272a6d06"</span>, <span class="attr">"spider"</span>: <span class="string">"weibocn"</span>, <span class="attr">"start_time"</span>: <span class="string">"2017-07-12 10:14:03.594664"</span>, <span class="attr">"end_time"</span>: <span class="string">"2017-07-12 10:24:03.594664"</span>&#125;]&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，pendings 代表当前正在等待的任务，running 代表当前正在运行的任务，finished 代表已经完成的任务。</p>
                  <h4 id="delversion-json"><a href="#delversion-json" class="headerlink" title="delversion.json"></a>delversion.json</h4>
                  <p>这个接口用来删除项目的某个版本。 我们可以用如下命令来删除项目版本：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/delversion.json -d project=weibo -d version=v1</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要一个参数 project，就是项目的名称，还需要一个参数 version，就是项目的版本。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，这样就代表删除成功了。</p>
                  <h4 id="delproject-json"><a href="#delproject-json" class="headerlink" title="delproject.json"></a>delproject.json</h4>
                  <p>这个接口用来删除某个项目。 我们可以用如下命令来删除某个项目：</p>
                  <figure class="highlight groovy">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl <span class="string">http:</span><span class="comment">//120.27.34.25:6800/delproject.json -d project=weibo</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里需要一个参数 project，就是项目的名称。 返回结果如下：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;<span class="attr">"status"</span>: <span class="string">"ok"</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>status 代表请求执行情况，这样就代表删除成功了。 以上就是 Scrapyd 所有的接口，我们可以直接请求 HTTP 接口即可控制项目的部署、启动、运行等操作。</p>
                  <h3 id="5-ScrapydAPI-的使用"><a href="#5-ScrapydAPI-的使用" class="headerlink" title="5. ScrapydAPI 的使用"></a>5. ScrapydAPI 的使用</h3>
                  <p>以上的这些接口可能使用起来还不是很方便，没关系，还有一个 ScrapydAPI 库对这些接口又做了一层封装，其安装方式也可以参考第一章的内容。 下面我们来看下 ScrapydAPI 的使用方法，其实核心原理和 HTTP 接口请求方式并无二致，只不过用 Python 封装后使用更加便捷。 我们可以用如下方式建立一个 ScrapydAPI 对象：</p>
                  <figure class="highlight clean">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapyd_api <span class="keyword">import</span> ScrapydAPI</span><br><span class="line">scrapyd = ScrapydAPI(<span class="string">'http://120.27.34.25:6800'</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>然后就可以调用它的方法来实现对应接口的操作了，例如部署的操作可以使用如下方式：</p>
                  <figure class="highlight sas">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">egg =<span class="meta"> open(</span><span class="string">'weibo.egg'</span>, <span class="string">'rb'</span>)</span><br><span class="line">scrapyd.add_versi<span class="meta">on(</span><span class="string">'weibo'</span>, <span class="string">'v1'</span>, egg)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就可以将项目打包为 Egg 文件，然后把本地打包的的 Egg 项目部署到远程 Scrapyd 了。 另外 ScrapydAPI 还实现了所有 Scrapyd 提供的 API 接口，名称都是相同的，参数也是相同的。 例如我们可以调用 list_projects() 方法即可列出 Scrapyd 中所有已部署的项目：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">scrapyd</span><span class="selector-class">.list_projects</span>()</span><br><span class="line"><span class="selector-attr">[<span class="string">'weibo'</span>, <span class="string">'zhihu'</span>]</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>另外还有其他的方法在此不再一一列举了，名称和参数都是相同的，更加详细的操作可以参考其官方文档：<a href="http://python-scrapyd-api.readthedocs.io/" target="_blank" rel="noopener">http://python-scrapyd-api.readthedocs.io/</a>。</p>
                  <h3 id="6-结语"><a href="#6-结语" class="headerlink" title="6. 结语"></a>6. 结语</h3>
                  <p>本节介绍了 Scrapyd 及 ScrapydAPI 的相关用法，我们可以通过它来部署项目，并通过 HTTP 接口来控制人物的运行，不过这里有一个不方便的地方就是部署过程，首先它需要打包 Egg 文件然后再上传，还是比较繁琐的，在下一节我们介绍一个更加方便的工具来完成部署过程。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-10 09:26:07" itemprop="dateCreated datePublished" datetime="2019-12-10T09:26:07+08:00">2019-12-10</time>
                </span>
                <span id="/8475.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 15.1–Scrapyd 分布式部署" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>4.7k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>4 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8472.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8472.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 14.4–Bloom Filter 的对接</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="14-4-Bloom-Filter-的对接"><a href="#14-4-Bloom-Filter-的对接" class="headerlink" title="14.4 Bloom Filter 的对接"></a>14.4 Bloom Filter 的对接</h1>
                  <p>首先回顾一下 Scrapy-Redis 的去重机制。Scrapy-Redis 将 Request 的指纹存储到了 Redis 集合中，每个指纹的长度为 40，例如 27adcc2e8979cdee0c9cecbbe8bf8ff51edefb61 就是一个指纹，它的每一位都是 16 进制数。 我们计算一下用这种方式耗费的存储空间。每个十六进制数占用 4 b，1 个指纹用 40 个十六进制数表示，占用空间为 20 B，1 万个指纹即占用空间 200 KB，1 亿个指纹占用 2 GB。当爬取数量达到上亿级别时，Redis 的占用的内存就会变得很大，而且这仅仅是指纹的存储。Redis 还存储了爬取队列，内存占用会进一步提高，更别说有多个 Scrapy 项目同时爬取的情况了。当爬取达到亿级别规模时，Scrapy-Redis 提供的集合去重已经不能满足我们的要求。所以我们需要使用一个更加节省内存的去重算法 Bloom Filter。</p>
                  <h3 id="1-了解-BloomFilter"><a href="#1-了解-BloomFilter" class="headerlink" title="1. 了解 BloomFilter"></a>1. 了解 BloomFilter</h3>
                  <p>Bloom Filter，中文名称叫作布隆过滤器，是 1970 年由 Bloom 提出的，它可以被用来检测一个元素是否在一个集合中。Bloom Filter 的空间利用效率很高，使用它可以大大节省存储空间。Bloom Filter 使用位数组表示一个待检测集合，并可以快速地通过概率算法判断一个元素是否存在于这个集合中。利用这个算法我们可以实现去重效果。 本节我们来了解 Bloom Filter 的基本算法，以及 Scrapy-Redis 中对接 Bloom Filter 的方法。</p>
                  <h3 id="2-BloomFilter-的算法"><a href="#2-BloomFilter-的算法" class="headerlink" title="2. BloomFilter 的算法"></a>2. BloomFilter 的算法</h3>
                  <p>在 Bloom Filter 中使用位数组来辅助实现检测判断。在初始状态下，我们声明一个包含 m 位的位数组，它的所有位都是 0，如图 14-7 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-29-113850.jpg" alt=""> 图 14-7 初始位数组 现在我们有了一个待检测集合，我们表示为 S={x1, x2, …, xn}，我们接下来需要做的就是检测一个 x 是否已经存在于集合 S 中。在 BloomFilter 算法中首先使用 k 个相互独立的、随机的哈希函数来将这个集合 S 中的每个元素 x1、x2、…、xn 映射到这个长度为 m 的位数组上，哈希函数得到的结果记作位置索引，然后将位数组该位置索引的位置 1。例如这里我们取 k 为 3，即有三个哈希函数，x1 经过三个哈希函数映射得到的结果分别为 1、4、8，x2 经过三个哈希函数映射得到的结果分别为 4、6、10，那么就会将位数组的 1、4、6、8、10 这五位置 1，如图 14-8 所示： <img src="https://qiniu.cuiqingcai.com/2019-11-29-114343.jpg" alt=""> 图 14-8 映射后位数组 这时如果再有一个新的元素 x，我们要判断 x 是否属于 S 这个集合，我们便会将仍然用 k 个哈希函数对 x 求映射结果，如果所有结果对应的位数组位置均为 1，那么我们就认为 x 属于 S 这个集合，否则如果有一个不为 1，则 x 不属于 S 集合。 例如一个新元素 x 经过三个哈希函数映射的结果为 4、6、8，对应的位置均为 1，则判断 x 属于 S 这个集合。如果结果为 4、6、7，7 对应的位置为 0，则判定 x 不属于 S 这个集合。 注意这里 m、n、k 满足的关系是 m&gt;nk，也就是说位数组的长度 m 要比集合元素 n 和哈希函数 k 的乘积还要大。 这样的判定方法很高效，但是也是有代价的，它可能把不属于这个集合的元素误认为属于这个集合，我们来估计一下它的错误率。当集合 S={x1, x2,…, xn} 的所有元素都被 k 个哈希函数映射到 m 位的位数组中时，这个位数组中某一位还是 0 的概率是： <img src="https://qiniu.cuiqingcai.com/2019-11-29-114353.jpg" alt=""> 因为哈希函数是随机的，所以任意一个哈希函数选中这一位的概率为 1/m，那么 1-1/m 就代表哈希函数一次没有选中这一位的概率，要把 S 完全映射到 m 位数组中，需要做 kn 次哈希运算，所以最后的概率就是 1-1/m 的 kn 次方。 一个不属于 S 的元素 x 如果要被误判定为在 S 中，那么这个概率就是 k 次哈希运算得到的结果对应的位数组位置都为 1，所以误判概率为： <img src="https://qiniu.cuiqingcai.com/2019-11-29-114430.jpg" alt=""> 根据： <img src="https://qiniu.cuiqingcai.com/2019-11-29-114441.jpg" alt=""> 可以将误判概率转化为： <img src="https://qiniu.cuiqingcai.com/2019-11-29-114445.jpg" alt=""> 在给定 m、n 时，可以求出使得 f 最小化的 k 值为： <img src="https://qiniu.cuiqingcai.com/2019-11-29-114452.jpg" alt=""> 在这里将误判概率归纳如下： 表 14-1　误判概率</p>
                  <p>m/n</p>
                  <p>最优 k</p>
                  <p>k=1</p>
                  <p>k=2</p>
                  <p>k=3</p>
                  <p>k=4</p>
                  <p>k=5</p>
                  <p>k=6</p>
                  <p>k=7</p>
                  <p>k=8</p>
                  <p>2</p>
                  <p>1.39</p>
                  <p>0.393</p>
                  <p>0.400</p>
                  <p>3</p>
                  <p>2.08</p>
                  <p>0.283</p>
                  <p>0.237</p>
                  <p>0.253</p>
                  <p>4</p>
                  <p>2.77</p>
                  <p>0.221</p>
                  <p>0.155</p>
                  <p>0.147</p>
                  <p>0.160</p>
                  <p>5</p>
                  <p>3.46</p>
                  <p>0.181</p>
                  <p>0.109</p>
                  <p>0.092</p>
                  <p>0.092</p>
                  <p>0.101</p>
                  <p>6</p>
                  <p>4.16</p>
                  <p>0.154</p>
                  <p>0.0804</p>
                  <p>0.0609</p>
                  <p>0.0561</p>
                  <p>0.0578</p>
                  <p>0.0638</p>
                  <p>7</p>
                  <p>4.85</p>
                  <p>0.133</p>
                  <p>0.0618</p>
                  <p>0.0423</p>
                  <p>0.0359</p>
                  <p>0.0347</p>
                  <p>0.0364</p>
                  <p>8</p>
                  <p>5.55</p>
                  <p>0.118</p>
                  <p>0.0489</p>
                  <p>0.0306</p>
                  <p>0.024</p>
                  <p>0.0217</p>
                  <p>0.0216</p>
                  <p>0.0229</p>
                  <p>9</p>
                  <p>6.24</p>
                  <p>0.105</p>
                  <p>0.0397</p>
                  <p>0.0228</p>
                  <p>0.0166</p>
                  <p>0.0141</p>
                  <p>0.0133</p>
                  <p>0.0135</p>
                  <p>0.0145</p>
                  <p>10</p>
                  <p>6.93</p>
                  <p>0.0952</p>
                  <p>0.0329</p>
                  <p>0.0174</p>
                  <p>0.0118</p>
                  <p>0.00943</p>
                  <p>0.00844</p>
                  <p>0.00819</p>
                  <p>0.00846</p>
                  <p>11</p>
                  <p>7.62</p>
                  <p>0.0869</p>
                  <p>0.0276</p>
                  <p>0.0136</p>
                  <p>0.00864</p>
                  <p>0.0065</p>
                  <p>0.00552</p>
                  <p>0.00513</p>
                  <p>0.00509</p>
                  <p>12</p>
                  <p>8.32</p>
                  <p>0.08</p>
                  <p>0.0236</p>
                  <p>0.0108</p>
                  <p>0.00646</p>
                  <p>0.00459</p>
                  <p>0.00371</p>
                  <p>0.00329</p>
                  <p>0.00314</p>
                  <p>13</p>
                  <p>9.01</p>
                  <p>0.074</p>
                  <p>0.0203</p>
                  <p>0.00875</p>
                  <p>0.00492</p>
                  <p>0.00332</p>
                  <p>0.00255</p>
                  <p>0.00217</p>
                  <p>0.00199</p>
                  <p>14</p>
                  <p>9.7</p>
                  <p>0.0689</p>
                  <p>0.0177</p>
                  <p>0.00718</p>
                  <p>0.00381</p>
                  <p>0.00244</p>
                  <p>0.00179</p>
                  <p>0.00146</p>
                  <p>0.00129</p>
                  <p>15</p>
                  <p>10.4</p>
                  <p>0.0645</p>
                  <p>0.0156</p>
                  <p>0.00596</p>
                  <p>0.003</p>
                  <p>0.00183</p>
                  <p>0.00128</p>
                  <p>0.001</p>
                  <p>0.000852</p>
                  <p>16</p>
                  <p>11.1</p>
                  <p>0.0606</p>
                  <p>0.0138</p>
                  <p>0.005</p>
                  <p>0.00239</p>
                  <p>0.00139</p>
                  <p>0.000935</p>
                  <p>0.000702</p>
                  <p>0.000574</p>
                  <p>17</p>
                  <p>11.8</p>
                  <p>0.0571</p>
                  <p>0.0123</p>
                  <p>0.00423</p>
                  <p>0.00193</p>
                  <p>0.00107</p>
                  <p>0.000692</p>
                  <p>0.000499</p>
                  <p>0.000394</p>
                  <p>18</p>
                  <p>12.5</p>
                  <p>0.054</p>
                  <p>0.0111</p>
                  <p>0.00362</p>
                  <p>0.00158</p>
                  <p>0.000839</p>
                  <p>0.000519</p>
                  <p>0.00036</p>
                  <p>0.000275</p>
                  <p>19</p>
                  <p>13.2</p>
                  <p>0.0513</p>
                  <p>0.00998</p>
                  <p>0.00312</p>
                  <p>0.0013</p>
                  <p>0.000663</p>
                  <p>0.000394</p>
                  <p>0.000264</p>
                  <p>0.000194</p>
                  <p>20</p>
                  <p>13.9</p>
                  <p>0.0488</p>
                  <p>0.00906</p>
                  <p>0.0027</p>
                  <p>0.00108</p>
                  <p>0.00053</p>
                  <p>0.000303</p>
                  <p>0.000196</p>
                  <p>0.00014</p>
                  <p>21</p>
                  <p>14.6</p>
                  <p>0.0465</p>
                  <p>0.00825</p>
                  <p>0.00236</p>
                  <p>0.000905</p>
                  <p>0.000427</p>
                  <p>0.000236</p>
                  <p>0.000147</p>
                  <p>0.000101</p>
                  <p>22</p>
                  <p>15.2</p>
                  <p>0.0444</p>
                  <p>0.00755</p>
                  <p>0.00207</p>
                  <p>0.000764</p>
                  <p>0.000347</p>
                  <p>0.000185</p>
                  <p>0.000112</p>
                  <p>7.46e-05</p>
                  <p>23</p>
                  <p>15.9</p>
                  <p>0.0425</p>
                  <p>0.00694</p>
                  <p>0.00183</p>
                  <p>0.000649</p>
                  <p>0.000285</p>
                  <p>0.000147</p>
                  <p>8.56e-05</p>
                  <p>5.55e-05</p>
                  <p>24</p>
                  <p>16.6</p>
                  <p>0.0408</p>
                  <p>0.00639</p>
                  <p>0.00162</p>
                  <p>0.000555</p>
                  <p>0.000235</p>
                  <p>0.000117</p>
                  <p>6.63e-05</p>
                  <p>4.17e-05</p>
                  <p>25</p>
                  <p>17.3</p>
                  <p>0.0392</p>
                  <p>0.00591</p>
                  <p>0.00145</p>
                  <p>0.000478</p>
                  <p>0.000196</p>
                  <p>9.44e-05</p>
                  <p>5.18e-05</p>
                  <p>3.16e-05</p>
                  <p>26</p>
                  <p>18</p>
                  <p>0.0377</p>
                  <p>0.00548</p>
                  <p>0.00129</p>
                  <p>0.000413</p>
                  <p>0.000164</p>
                  <p>7.66e-05</p>
                  <p>4.08e-05</p>
                  <p>2.42e-05</p>
                  <p>27</p>
                  <p>18.7</p>
                  <p>0.0364</p>
                  <p>0.0051</p>
                  <p>0.00116</p>
                  <p>0.000359</p>
                  <p>0.000138</p>
                  <p>6.26e-05</p>
                  <p>3.24e-05</p>
                  <p>1.87e-05</p>
                  <p>28</p>
                  <p>19.4</p>
                  <p>0.0351</p>
                  <p>0.00475</p>
                  <p>0.00105</p>
                  <p>0.000314</p>
                  <p>0.000117</p>
                  <p>5.15e-05</p>
                  <p>2.59e-05</p>
                  <p>1.46e-05</p>
                  <p>29</p>
                  <p>20.1</p>
                  <p>0.0339</p>
                  <p>0.00444</p>
                  <p>0.000949</p>
                  <p>0.000276</p>
                  <p>9.96e-05</p>
                  <p>4.26e-05</p>
                  <p>2.09e-05</p>
                  <p>1.14e-05</p>
                  <p>30</p>
                  <p>20.8</p>
                  <p>0.0328</p>
                  <p>0.00416</p>
                  <p>0.000862</p>
                  <p>0.000243</p>
                  <p>8.53e-05</p>
                  <p>3.55e-05</p>
                  <p>1.69e-05</p>
                  <p>9.01e-06</p>
                  <p>31</p>
                  <p>21.5</p>
                  <p>0.0317</p>
                  <p>0.0039</p>
                  <p>0.000785</p>
                  <p>0.000215</p>
                  <p>7.33e-05</p>
                  <p>2.97e-05</p>
                  <p>1.38e-05</p>
                  <p>7.16e-06</p>
                  <p>32</p>
                  <p>22.2</p>
                  <p>0.0308</p>
                  <p>0.00367</p>
                  <p>0.000717</p>
                  <p>0.000191</p>
                  <p>6.33e-05</p>
                  <p>2.5e-05</p>
                  <p>1.13e-05</p>
                  <p>5.73e-06</p>
                  <p>表 14-1 中第一列为 m/n 的值，第二列为最优 k 值，其后列为不同 k 值的误判概率，可以看到当 k 值确定时，随着 m/n 的增大，误判概率逐渐变小。当 m/n 的值确定时，当 k 越靠近最优 K 值，误判概率越小。另外误判概率总体来看都是极小的，在容忍此误判概率的情况下，大幅减小存储空间和判定速度是完全值得的。 接下来我们就将 BloomFilter 算法应用到 Scrapy-Redis 分布式爬虫的去重过程中，以解决 Redis 内存不足的问题。</p>
                  <h3 id="3-对接-Scrapy-Redis"><a href="#3-对接-Scrapy-Redis" class="headerlink" title="3. 对接 Scrapy-Redis"></a>3. 对接 Scrapy-Redis</h3>
                  <p>实现 BloomFilter 时，我们首先要保证不能破坏 Scrapy-Redis 分布式爬取的运行架构，所以我们需要修改 Scrapy-Redis 的源码，将它的去重类替换掉。同时 BloomFilter 的实现需要借助于一个位数组，所以既然当前架构还是依赖于 Redis 的，那么正好位数组的维护直接使用 Redis 就好了。 首先我们实现一个基本的哈希算法，可以实现将一个值经过哈希运算后映射到一个 m 位位数组的某一位上，代码实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashMap</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, m, seed)</span>:</span></span><br><span class="line">        self.m = m</span><br><span class="line">        self.seed = seed</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hash</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Hash Algorithm</span></span><br><span class="line"><span class="string">        :param value: Value</span></span><br><span class="line"><span class="string">        :return: Hash Value</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ret = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(value)):</span><br><span class="line">            ret += self.seed * ret + ord(value[i])</span><br><span class="line">        <span class="keyword">return</span> (self.m - <span class="number">1</span>) &amp; ret</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里新建了一个 HashMap 类，构造函数传入两个值，一个是 m 位数组的位数，另一个是种子值 seed，不同的哈希函数需要有不同的 seed，这样可以保证不同的哈希函数的结果不会碰撞。 在 hash() 方法的实现中，value 是要被处理的内容，在这里我们遍历了该字符的每一位并利用 ord() 方法取到了它的 ASCII 码值，然后混淆 seed 进行迭代求和运算，最终会得到一个数值。这个数值的结果就由 value 和 seed 唯一确定，然后我们再将它和 m 进行按位与运算，即可获取到 m 位数组的映射结果，这样我们就实现了一个由字符串和 seed 来确定的哈希函数。当 m 固定时，只要 seed 值相同，就代表是同一个哈希函数，相同的 value 必然会映射到相同的位置。所以如果我们想要构造几个不同的哈希函数，只需要改变其 seed 就好了，以上便是一个简易的哈希函数的实现。 接下来我们再实现 BloomFilter，BloomFilter 里面需要用到 k 个哈希函数，所以在这里我们需要对这几个哈希函数指定相同的 m 值和不同的 seed 值，在这里构造如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">BLOOMFILTER_HASH_NUMBER = 6</span><br><span class="line">BLOOMFILTER_BIT = 30</span><br><span class="line"></span><br><span class="line">class BloomFilter(object):</span><br><span class="line">    def __init__(self, server, key, <span class="attribute">bit</span>=BLOOMFILTER_BIT, <span class="attribute">hash_number</span>=BLOOMFILTER_HASH_NUMBER):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        Initialize BloomFilter</span></span><br><span class="line"><span class="string">        :param server: Redis Server</span></span><br><span class="line"><span class="string">        :param key: BloomFilter Key</span></span><br><span class="line"><span class="string">        :param bit: m = 2 ^ bit</span></span><br><span class="line"><span class="string">        :param hash_number: the number of hash function</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        #<span class="built_in"> default </span><span class="keyword">to</span> 1 &lt;&lt; 30 = 10,7374,1824 = 2^30 = 128MB, max<span class="built_in"> filter </span>2^30/hash_number = 1,7895,6970 fingerprints</span><br><span class="line">        self.m = 1 &lt;&lt; bit</span><br><span class="line">        self.seeds = range(hash_number)</span><br><span class="line">        self.maps = [HashMap(self.m, seed) <span class="keyword">for</span> seed <span class="keyword">in</span> self.seeds]</span><br><span class="line">        self.server = server</span><br><span class="line">        self.key = key</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>由于我们需要亿级别的数据的去重，即前文介绍的算法中的 n 为 1 亿以上，哈希函数的个数 k 大约取 10 左右的量级，而 m&gt;kn，所以这里 m 值大约保底在 10 亿，由于这个数值比较大，所以这里用移位操作来实现，传入位数 bit，定义 30，然后做一个移位操作 1 &lt;&lt; 30，相当于 2 的 30 次方，等于 1073741824，量级也是恰好在 10 亿左右，由于是位数组，所以这个位数组占用的大小就是 2^30b=128MB，而本文开头我们计算过 Scrapy-Redis 集合去重的占用空间大约在 2G 左右，可见 BloomFilter 的空间利用效率之高。 随后我们再传入哈希函数的个数，用它来生成几个不同的 seed，用不同的 seed 来定义不同的哈希函数，这样我们就可以构造一个哈希函数列表，遍历 seed，构造带有不同 seed 值的 HashMap 对象，保存成变量 maps 供后续使用。 另外 server 就是 Redis 连接对象，key 就是这个 m 位数组的名称。 接下来我们就要实现比较关键的两个方法了，一个是判定元素是否重复的方法 exists()，另一个是添加元素到集合中的方法 insert()，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exists</span><span class="params">(self, value)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    if value exists</span></span><br><span class="line"><span class="string">    :param value:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> value:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    exist = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> map <span class="keyword">in</span> self.maps:</span><br><span class="line">        offset = map.hash(value)</span><br><span class="line">        exist = exist &amp; self.server.getbit(self.key, offset)</span><br><span class="line">    <span class="keyword">return</span> exist</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert</span><span class="params">(self, value)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    add value to bloom</span></span><br><span class="line"><span class="string">    :param value:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> self.maps:</span><br><span class="line">        offset = f.hash(value)</span><br><span class="line">        self.server.setbit(self.key, offset, <span class="number">1</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先我们先看下 insert() 方法，BloomFilter 算法中会逐个调用哈希函数对放入集合中的元素进行运算得到在 m 位位数组中的映射位置，然后将位数组对应的位置置 1，所以这里在代码中我们遍历了初始化好的哈希函数，然后调用其 hash() 方法算出映射位置 offset，再利用 Redis 的 setbit() 方法将该位置 1。 在 exists() 方法中我们就需要实现判定是否重复的逻辑了，方法参数 value 即为待判断的元素，在这里我们首先定义了一个变量 exist，然后遍历了所有哈希函数对 value 进行哈希运算，得到映射位置，然后我们用 getbit() 方法取得该映射位置的结果，依次进行与运算。这样只有每次 getbit() 得到的结果都为 1 时，最后的 exist 才为 True，即代表 value 属于这个集合。如果其中只要有一次 getbit() 得到的结果为 0，即 m 位数组中有对应的 0 位，那么最终的结果 exist 就为 False，即代表 value 不属于这个集合。这样此方法最后的返回结果就是判定重复与否的结果了。 到现在为止 BloomFilter 的实现就已经完成了，我们可以用一个实例来测试一下，代码如下：</p>
                  <figure class="highlight vim">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">conn = StrictRedis(host=<span class="string">'localhost'</span>, port=<span class="number">6379</span>, password=<span class="string">'foobared'</span>)</span><br><span class="line"><span class="keyword">bf</span> = BloomFilter(conn, <span class="string">'testbf'</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">bf</span>.<span class="keyword">insert</span>(<span class="string">'Hello'</span>)</span><br><span class="line"><span class="keyword">bf</span>.<span class="keyword">insert</span>(<span class="string">'World'</span>)</span><br><span class="line">result = <span class="keyword">bf</span>.<span class="built_in">exists</span>(<span class="string">'Hello'</span>)</span><br><span class="line"><span class="keyword">print</span>(bool(result))</span><br><span class="line">result = <span class="keyword">bf</span>.<span class="built_in">exists</span>(<span class="string">'Python'</span>)</span><br><span class="line"><span class="keyword">print</span>(bool(result))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们首先定义了一个 Redis 连接对象，然后传递给 BloomFilter，为了避免内存占用过大这里传的位数 bit 比较小，设置为 5，哈希函数的个数设置为 6。 首先我们调用 insert() 方法插入了 Hello 和 World 两个字符串，随后判断了一下 Hello 和 Python 这两个字符串是否存在，最后输出它的结果，运行结果如下：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>很明显，结果完全没有问题，这样我们就借助于 Redis 成功实现了 BloomFilter 的算法。 接下来我们需要继续修改 Scrapy-Redis 的源码，将它的 dupefilter 逻辑替换为 BloomFilter 的逻辑，在这里主要是修改 RFPDupeFilter 类的 request_seen() 方法，实现如下：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(<span class="keyword">self</span>, request)</span></span><span class="symbol">:</span></span><br><span class="line">    fp = <span class="keyword">self</span>.request_fingerprint(request)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">self</span>.bf.exists(fp)<span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> True</span><br><span class="line">    <span class="keyword">self</span>.bf.insert(fp)</span><br><span class="line">    <span class="keyword">return</span> False</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先还是利用 request_fingerprint() 方法获取了 Request 的指纹，然后调用 BloomFilter 的 exists() 方法判定了该指纹是否存在，如果存在，则证明该 Request 是重复的，返回 True，否则调用 BloomFilter 的 insert() 方法将该指纹添加并返回 False，这样就成功利用 BloomFilter 替换了 Scrapy-Redis 的集合去重。 对于 BloomFilter 的初始化定义，我们可以将 <strong>init</strong>() 方法修改为如下内容：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def __init__(self, server, key, debug, bit, hash_number):</span><br><span class="line">    self.server = server</span><br><span class="line">    self.key = key</span><br><span class="line">    self.<span class="builtin-name">debug</span> = debug</span><br><span class="line">    self.bit = bit</span><br><span class="line">    self.hash_number = hash_number</span><br><span class="line">    self.logdupes = <span class="literal">True</span></span><br><span class="line">    self.bf = BloomFilter(server, self.key, bit, hash_number)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中 bit 和 hash_number 需要使用 from_settings() 方法传递，修改如下：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">@classmethod</span><br><span class="line">def from_settings(cls, settings):</span><br><span class="line">    <span class="keyword">server</span> = get_redis_from_settings(settings)</span><br><span class="line">    key = defaults.DUPEFILTER_KEY % &#123;<span class="string">'timestamp'</span>: <span class="type">int</span>(<span class="type">time</span>.time())&#125;</span><br><span class="line">    <span class="keyword">debug</span> = settings.getbool(<span class="string">'DUPEFILTER_DEBUG'</span>, DUPEFILTER_DEBUG)</span><br><span class="line">    <span class="type">bit</span> = settings.getint(<span class="string">'BLOOMFILTER_BIT'</span>, BLOOMFILTER_BIT)</span><br><span class="line">    hash_number = settings.getint(<span class="string">'BLOOMFILTER_HASH_NUMBER'</span>, BLOOMFILTER_HASH_NUMBER)</span><br><span class="line">    <span class="keyword">return</span> cls(<span class="keyword">server</span>, key=key, <span class="keyword">debug</span>=<span class="keyword">debug</span>, <span class="type">bit</span>=<span class="type">bit</span>, hash_number=hash_number)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中常量的定义 DUPEFILTER_DEBUG 和 BLOOMFILTER_BIT 统一定义在 defaults.py 中，默认如下：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">BLOOMFILTER_HASH_NUMBER</span> = <span class="number">6</span></span><br><span class="line"><span class="attr">BLOOMFILTER_BIT</span> = <span class="number">30</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>到此为止我们就成功实现了 BloomFilter 和 Scrapy-Redis 的对接。</p>
                  <h3 id="4-本节代码"><a href="#4-本节代码" class="headerlink" title="4. 本节代码"></a>4. 本节代码</h3>
                  <p>本节代码地址为：<a href="https://github.com/Python3WebSpider/ScrapyRedisBloomFilter" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyRedisBloomFilter</a>。</p>
                  <h3 id="5-使用"><a href="#5-使用" class="headerlink" title="5. 使用"></a>5. 使用</h3>
                  <p>为了方便使用，本节的代码已经打包成了一个 Python 包并发布到了 PyPi，链接为：<a href="https://pypi.python.org/pypi/scrapy-redis-bloomfilter" target="_blank" rel="noopener">https://pypi.python.org/pypi/scrapy-redis-bloomfilter</a>，因此我们以后如果想使用 ScrapyRedisBloomFilter 直接使用就好了，不需要再自己实现一遍。 我们可以直接使用 Pip 来安装，命令如下：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> scrapy-redis-bloomfilter</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>使用的方法和 Scrapy-Redis 基本相似，在这里说明几个关键配置：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># 去重类，要使用 BloomFilter 请替换 DUPEFILTER_CLASS</span></span><br><span class="line"><span class="attr">DUPEFILTER_CLASS</span> = <span class="string">"scrapy_redis_bloomfilter.dupefilter.RFPDupeFilter"</span></span><br><span class="line"><span class="comment"># 哈希函数的个数，默认为 6，可以自行修改</span></span><br><span class="line"><span class="attr">BLOOMFILTER_HASH_NUMBER</span> = <span class="number">6</span></span><br><span class="line"><span class="comment"># BloomFilter 的 bit 参数，默认 30，占用 128MB 空间，去重量级 1 亿</span></span><br><span class="line"><span class="attr">BLOOMFILTER_BIT</span> = <span class="number">30</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>DUPEFILTER_CLASS 是去重类，如果要使用 BloomFilter 需要将 DUPEFILTER_CLASS 修改为该包的去重类。 BLOOMFILTER_HASH_NUMBER 是 BloomFilter 使用的哈希函数的个数，默认为 6，可以根据去重量级自行修改。 BLOOMFILTER_BIT 即前文所介绍的 BloomFilter 类的 bit 参数，它决定了位数组的位数，如果 BLOOMFILTER_BIT 为 30，那么位数组位数为 2 的 30 次方，将占用 Redis 128MB 的存储空间，去重量级在 1 亿左右，即对应爬取量级 1 亿左右。如果爬取量级在 10 亿、20 亿甚至 100 亿，请务必将此参数对应调高。</p>
                  <h3 id="6-测试"><a href="#6-测试" class="headerlink" title="6. 测试"></a>6. 测试</h3>
                  <p>在源代码中附有一个测试项目，放在 tests 文件夹，该项目使用了 Scrapy-RedisBloomFilter 来去重，Spider 的实现如下：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy import Request, Spider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestSpider</span>(<span class="title">Spider</span>):</span></span><br><span class="line">    name = <span class="string">'test'</span></span><br><span class="line">    base_url = <span class="string">'https://www.baidu.com/s?wd='</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)<span class="symbol">:</span></span><br><span class="line">            url = <span class="keyword">self</span>.base_url + str(i)</span><br><span class="line">            <span class="keyword">yield</span> Request(url, callback=<span class="keyword">self</span>.parse)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Here contains 10 duplicated Requests    </span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>): </span><br><span class="line">            url = <span class="keyword">self</span>.base_url + str(i)</span><br><span class="line">            <span class="keyword">yield</span> Request(url, callback=<span class="keyword">self</span>.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.logger.debug(<span class="string">'Response of '</span> + response.url)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在 start_requests() 方法中首先循环 10 次，构造参数为 0-9 的 URL，然后重新循环了 100 次，构造了参数为 0-99 的 URL，那么这里就会包含 10 个重复的 Request，我们运行项目测试一下：</p>
                  <figure class="highlight bash">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy crawl <span class="built_in">test</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到最后的输出结果如下：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">&#123;'bloomfilter/filtered':</span> <span class="number">10</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'downloader/request_bytes':</span> <span class="number">34021</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'downloader/request_count':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'downloader/request_method_count/GET':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'downloader/response_bytes':</span> <span class="number">72943</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'downloader/response_count':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'downloader/response_status_count/200':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'finish_reason':</span> <span class="string">'finished'</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'finish_time':</span> <span class="string">datetime.datetime(2017,</span> <span class="number">8</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">9</span><span class="string">,</span> <span class="number">34</span><span class="string">,</span> <span class="number">30</span><span class="string">,</span> <span class="number">419597</span><span class="string">),</span></span><br><span class="line"> <span class="attr">'log_count/DEBUG':</span> <span class="number">202</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'log_count/INFO':</span> <span class="number">7</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'memusage/max':</span> <span class="number">54153216</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'memusage/startup':</span> <span class="number">54153216</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'response_received_count':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'scheduler/dequeued/redis':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'scheduler/enqueued/redis':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'start_time':</span> <span class="string">datetime.datetime(2017,</span> <span class="number">8</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">9</span><span class="string">,</span> <span class="number">34</span><span class="string">,</span> <span class="number">26</span><span class="string">,</span> <span class="number">495018</span><span class="string">)&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到最后统计的第一行的结果：</p>
                  <figure class="highlight sml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">'bloomfilter</span>/filtered': <span class="number">10</span>,</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这就是 BloomFilter 过滤后的统计结果，可以看到它的过滤个数为 10 个，也就是它成功将重复的 10 个 Reqeust 识别出来了，测试通过。</p>
                  <h3 id="7-结语"><a href="#7-结语" class="headerlink" title="7. 结语"></a>7. 结语</h3>
                  <p>以上便是 BloomFilter 的原理及对接实现，使用了 BloomFilter 可以大大节省 Redis 内存，在数据量大的情况下推荐使用此方案。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-10 09:24:45" itemprop="dateCreated datePublished" datetime="2019-12-10T09:24:45+08:00">2019-12-10</time>
                </span>
                <span id="/8472.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 14.4–Bloom Filter 的对接" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>10k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>9 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8468.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8468.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 14.3–Scrapy 分布式实现</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="14-3-Scrapy-分布式实现"><a href="#14-3-Scrapy-分布式实现" class="headerlink" title="14.3 Scrapy 分布式实现"></a>14.3 Scrapy 分布式实现</h1>
                  <p>接下来，我们会利用 Scrapy-Redis 来实现分布式的对接。</p>
                  <h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h3>
                  <p>请确保已经成功实现了 Scrapy 新浪微博爬虫，Scrapy-Redis 库已经正确安装，如果还没安装，请参考第 1 章的安装说明。</p>
                  <h3 id="2-搭建-Redis-服务器"><a href="#2-搭建-Redis-服务器" class="headerlink" title="2. 搭建 Redis 服务器"></a>2. 搭建 Redis 服务器</h3>
                  <p>要实现分布式部署，多台主机需要共享爬取队列和去重集合，而这两部分内容都是存于 Redis 数据库中的，我们需要搭建一个可公网访问的 Redis 服务器。 推荐使用 Linux 服务器，可以购买阿里云、腾讯云、Azure 等提供的云主机，一般都会配有公网 IP，具体的搭建方式可以参考第 1 章中 Redis 数据库的安装方式。 Redis 安装完成之后就可以远程连接了，注意部分商家（如阿里云、腾讯云）的服务器需要配置安全组放通 Redis 运行端口才可以远程访问。如果遇到不能远程连接的问题，可以排查安全组的设置。 需要记录 Redis 的运行 IP、端口、地址，供后面配置分布式爬虫使用。当前配置好的 Redis 的 IP 为服务器的 IP 120.27.34.25，端口为默认的 6379，密码为 foobared。</p>
                  <h3 id="3-部署代理池和-Cookies-池"><a href="#3-部署代理池和-Cookies-池" class="headerlink" title="3. 部署代理池和 Cookies 池"></a>3. 部署代理池和 Cookies 池</h3>
                  <p>新浪微博项目需要用到代理池和 Cookies 池，而之前我们的代理池和 Cookies 池都是在本地运行的。所以我们需要将二者放到可以被公网访问的服务器上运行，将代码上传到服务器，修改 Redis 的连接信息配置，用同样的方式运行代理池和 Cookies 池。 远程访问代理池和 Cookies 池提供的接口，来获取随机代理和 Cookies。如果不能远程访问，先确保其在 0.0.0.0 这个 Host 上运行，再检查安全组的配置。 如我当前配置好的代理池和 Cookies 池的运行 IP 都是服务器的 IP，120.27.34.25，端口分别为 5555 和 5556，如图 14-3 和图 14-4 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-29-113501.jpg" alt=""> 图 14-3 代理池接口 <img src="https://qiniu.cuiqingcai.com/2019-11-29-113506.jpg" alt=""> 图 14-4 Cookies 池接口 所以接下来我们就需要把 Scrapy 新浪微博项目中的访问链接修改如下：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">PROXY_URL</span> = <span class="string">'http://120.27.34.25:5555/random'</span></span><br><span class="line"><span class="attr">COOKIES_URL</span> = <span class="string">'http://120.27.34.25:5556/weibo/random'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>具体的修改方式根据实际配置的 IP 和端口做相应调整。</p>
                  <h3 id="4-配置-Scrapy-Redis"><a href="#4-配置-Scrapy-Redis" class="headerlink" title="4. 配置 Scrapy-Redis"></a>4. 配置 Scrapy-Redis</h3>
                  <p>配置 Scrapy-Redis 非常简单，只需要修改一下 settings.py 配置文件即可。</p>
                  <h4 id="核心配置"><a href="#核心配置" class="headerlink" title="核心配置"></a>核心配置</h4>
                  <p>首先最主要的是，需要将调度器的类和去重的类替换为 Scrapy-Redis 提供的类，在 settings.py 里面添加如下配置即可：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">SCHEDULER</span> = <span class="string">"scrapy_redis.scheduler.Scheduler"</span></span><br><span class="line"><span class="attr">DUPEFILTER_CLASS</span> = <span class="string">"scrapy_redis.dupefilter.RFPDupeFilter"</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="Redis-连接配置"><a href="#Redis-连接配置" class="headerlink" title="Redis 连接配置"></a>Redis 连接配置</h4>
                  <p>接下来配置 Redis 的连接信息，这里有两种配置方式。 第一种方式是通过连接字符串配置。我们可以用 Redis 的地址、端口、密码来构造一个 Redis 连接字符串，支持的连接形式如下所示：</p>
                  <figure class="highlight dts">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">redis:</span><span class="comment">//[:password]@host:port/db</span></span><br><span class="line"><span class="symbol">rediss:</span><span class="comment">//[:password]@host:port/db</span></span><br><span class="line"><span class="symbol">unix:</span><span class="comment">//[:password]@/path/to/socket.sock?db=db</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>password 是密码，比如要以冒号开头，中括号代表此选项可有可无，host 是 Redis 的地址，port 是运行端口，db 是数据库代号，其值默认是 0。 根据上文中提到我的 Redis 连接信息，构造这个 Redis 的连接字符串如下所示：</p>
                  <figure class="highlight avrasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">redis:</span>//:foobared<span class="subst">@120</span><span class="number">.27</span><span class="number">.34</span><span class="number">.25</span>:<span class="number">6379</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>直接在 settings.py 里面配置为 REDIS_URL 变量即可：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">REDIS_URL</span> = <span class="string">'redis://:foobared@120.27.34.25:6379'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第二种配置方式是分项单独配置。这个配置就更加直观明了，如根据我的 Redis 连接信息，可以在 settings.py 中配置如下代码：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">REDIS_HOST</span> = <span class="string">'120.27.34.25'</span></span><br><span class="line"><span class="attr">REDIS_PORT</span> = <span class="number">6379</span></span><br><span class="line"><span class="attr">REDIS_PASSWORD</span> = <span class="string">'foobared'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这段代码分开配置了 Redis 的地址、端口和密码。 注意，如果配置了 REDIS_URL，那么 Scrapy-Redis 将优先使用 REDIS_URL 连接，会覆盖上面的三项配置。如果想要分项单独配置的话，请不要配置 REDIS_URL。 在本项目中，我选择的是配置 REDIS_URL。</p>
                  <h4 id="配置调度队列"><a href="#配置调度队列" class="headerlink" title="配置调度队列"></a>配置调度队列</h4>
                  <p>此项配置是可选的，默认使用 PriorityQueue。如果想要更改配置，可以配置 SCHEDULER_QUEUE_CLASS 变量，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">SCHEDULER_QUEUE_CLASS</span> = <span class="string">'scrapy_redis.queue.PriorityQueue'</span></span><br><span class="line"><span class="attr">SCHEDULER_QUEUE_CLASS</span> = <span class="string">'scrapy_redis.queue.FifoQueue'</span></span><br><span class="line"><span class="attr">SCHEDULER_QUEUE_CLASS</span> = <span class="string">'scrapy_redis.queue.LifoQueue'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>以上三行任选其一配置，即可切换爬取队列的存储方式。 在本项目中不进行任何配置，我们使用默认配置。</p>
                  <h4 id="配置持久化"><a href="#配置持久化" class="headerlink" title="配置持久化"></a>配置持久化</h4>
                  <p>此配置是可选的，默认是 False。Scrapy-Redis 默认会在爬取全部完成后清空爬取队列和去重指纹集合。 如果不想自动清空爬取队列和去重指纹集合，可以增加如下配置：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">SCHEDULER_PERSIST</span> = <span class="literal">True</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>将 SCHEDULER_PERSIST 设置为 True 之后，爬取队列和去重指纹集合不会在爬取完成后自动清空，如果不配置，默认是 False，即自动清空。 值得注意的是，如果强制中断爬虫的运行，爬取队列和去重指纹集合是不会自动清空的。 在本项目中不进行任何配置，我们使用默认配置。</p>
                  <h4 id="配置重爬"><a href="#配置重爬" class="headerlink" title="配置重爬"></a>配置重爬</h4>
                  <p>此配置是可选的，默认是 False。如果配置了持久化或者强制中断了爬虫，那么爬取队列和指纹集合不会被清空，爬虫重新启动之后就会接着上次爬取。如果想重新爬取，我们可以配置重爬的选项：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">SCHEDULER_FLUSH_ON_START</span> = <span class="literal">True</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样将 SCHEDULER_FLUSH_ON_START 设置为 True 之后，爬虫每次启动时，爬取队列和指纹集合都会清空。所以要做分布式爬取，我们必须保证只能清空一次，否则每个爬虫任务在启动时都清空一次，就会把之前的爬取队列清空，势必会影响分布式爬取。 注意，此配置在单机爬取的时候比较方便，分布式爬取不常用此配置。 在本项目中不进行任何配置，我们使用默认配置。</p>
                  <h4 id="Pipeline-配置"><a href="#Pipeline-配置" class="headerlink" title="Pipeline 配置"></a>Pipeline 配置</h4>
                  <p>此配置是可选的，默认不启动 Pipeline。Scrapy-Redis 实现了一个存储到 Redis 的 Item Pipeline，启用了这个 Pipeline 的话，爬虫会把生成的 Item 存储到 Redis 数据库中。在数据量比较大的情况下，我们一般不会这么做。因为 Redis 是基于内存的，我们利用的是它处理速度快的特性，用它来做存储未免太浪费了，配置如下：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">ITEM_PIPELINES</span> = &#123;<span class="string">'scrapy_redis.pipelines.RedisPipeline'</span>: <span class="number">300</span>&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>本项目不进行任何配置，即不启动 Pipeline。 到此为止，Scrapy-Redis 的配置就完成了。有的选项我们没有配置，但是这些配置在其他 Scrapy 项目中可能用到，要根据具体情况而定。</p>
                  <h3 id="5-配置存储目标"><a href="#5-配置存储目标" class="headerlink" title="5. 配置存储目标"></a>5. 配置存储目标</h3>
                  <p>之前 Scrapy 新浪微博爬虫项目使用的存储是 MongoDB，而且 MongoDB 是本地运行的，即连接的是 localhost。但是，当爬虫程序分发到各台主机运行的时候，爬虫就会连接各自的的 MongoDB。所以我们需要在各台主机上都安装 MongoDB，这样有两个缺点：一是搭建 MongoDB 环境比较烦琐；二是这样各台主机的爬虫会把爬取结果分散存到各自主机上，不方便统一管理。 所以我们最好将存储目标存到同一个地方，例如都存到同一个 MongoDB 数据库中。我们可以在服务器上搭建一个 MongoDB 服务，或者直接购买 MongoDB 数据存储服务。 这里使用的就是服务器上搭建的的 MongoDB 服务，IP 仍然为 120.27.34.25，用户名为 admin，密码为 admin123。 修改配置 MONGO_URI 为如下：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">MONGO_URI</span> = <span class="string">'mongodb://admin:admin123@120.27.34.25:27017'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>到此为止，我们就成功完成了 Scrapy 分布式爬虫的配置了。</p>
                  <h3 id="6-运行"><a href="#6-运行" class="headerlink" title="6. 运行"></a>6. 运行</h3>
                  <p>接下来将代码部署到各台主机上，记得每台主机都需要配好对应的 Python 环境。 每台主机上都执行如下命令，即可启动爬取：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl weibocn</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>每台主机启动了此命令之后，就会从配置的 Redis 数据库中调度 Request，做到爬取队列共享和指纹集合共享。同时每台主机占用各自的带宽和处理器，不会互相影响，爬取效率成倍提高。</p>
                  <h3 id="7-结果"><a href="#7-结果" class="headerlink" title="7. 结果"></a>7. 结果</h3>
                  <p>一段时间后，我们可以用 RedisDesktop 观察远程 Redis 数据库的信息。这里会出现两个 Key：一个叫作 weibocn:dupefilter，用来储存指纹；另一个叫作 weibocn:requests，即爬取队列，如图 14-5 和图 14-6 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-29-113521.jpg" alt=""> 图 14-5 去重指纹 <img src="/Users/maqian/Downloads/python3webspiderupload/assets/14-6.jpg" alt=""> 图 14-6 爬取队列 随着时间的推移，指纹集合会不断增长，爬取队列会动态变化，爬取的数据也会被储存到 MongoDB 数据库中。 至此 Scrapy 分布式的配置已全部完成。</p>
                  <h3 id="8-本节代码"><a href="#8-本节代码" class="headerlink" title="8. 本节代码"></a>8. 本节代码</h3>
                  <p>本节代码地址为：<a href="https://github.com/Python3WebSpider/Weibo/tree/distributed" target="_blank" rel="noopener">https://github.com/Python3WebSpider/Weibo/tree/distributed</a>，注意这里是 distributed 分支。</p>
                  <h3 id="9-结语"><a href="#9-结语" class="headerlink" title="9. 结语"></a>9. 结语</h3>
                  <p>本节通过对接 Scrapy-Redis 成功实现了分布式爬虫，但是部署还是有很多不方便的地方。另外，如果爬取量特别大的话，Redis 的内存也是个问题。在后文我们会继续了解相关优化方案。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-09 10:14:40" itemprop="dateCreated datePublished" datetime="2019-12-09T10:14:40+08:00">2019-12-09</time>
                </span>
                <span id="/8468.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 14.3–Scrapy 分布式实现" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>4.1k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>4 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8465.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8465.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 14.2–Scrapy-Redis 源码解析</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="14-2-Scrapy-Redis-源码解析"><a href="#14-2-Scrapy-Redis-源码解析" class="headerlink" title="14.2 Scrapy-Redis 源码解析"></a>14.2 Scrapy-Redis 源码解析</h1>
                  <p>Scrapy-Redis 库已经为我们提供了 Scrapy 分布式的队列、调度器、去重等功能，其 GitHub 地址为：<a href="https://github.com/rmax/scrapy-redis" target="_blank" rel="noopener">https://github.com/rmax/scrapy-redis</a>。 本节我们深入了解一下，利用 Redis 如何实现 Scrapy 分布式。</p>
                  <h3 id="1-获取源码"><a href="#1-获取源码" class="headerlink" title="1. 获取源码"></a>1. 获取源码</h3>
                  <p>可以把源码克隆下来，执行如下命令：</p>
                  <figure class="highlight crmsh">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">git <span class="keyword">clone</span> <span class="title">https</span>://github.com/rmax/scrapy-redis.git</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>核心源码在 scrapy-redis/src/scrapy_redis 目录下。</p>
                  <h3 id="2-爬取队列"><a href="#2-爬取队列" class="headerlink" title="2. 爬取队列"></a>2. 爬取队列</h3>
                  <p>从爬取队列入手，看看它的具体实现。源码文件为 queue.py，它有三个队列的实现，首先它实现了一个父类 Base，提供一些基本方法和属性，如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Per-spider base queue class"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, server, spider, key, serializer=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> serializer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            serializer = picklecompat</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(serializer, <span class="string">'loads'</span>):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"serializer does not implement 'loads' function: % r"</span></span><br><span class="line">                            % serializer)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(serializer, <span class="string">'dumps'</span>):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"serializer '% s' does not implement 'dumps' function: % r"</span></span><br><span class="line">                            % serializer)</span><br><span class="line">        self.server = server</span><br><span class="line">        self.spider = spider</span><br><span class="line">        self.key = key % &#123;<span class="string">'spider'</span>: spider.name&#125;</span><br><span class="line">        self.serializer = serializer</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_encode_request</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        obj = request_to_dict(request, self.spider)</span><br><span class="line">        <span class="keyword">return</span> self.serializer.dumps(obj)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_decode_request</span><span class="params">(self, encoded_request)</span>:</span></span><br><span class="line">        obj = self.serializer.loads(encoded_request)</span><br><span class="line">        <span class="keyword">return</span> request_from_dict(obj, self.spider)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the length of the queue"""</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Push a request"""</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self, timeout=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Pop a request"""</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Clear queue/stack"""</span></span><br><span class="line">        self.server.delete(self.key)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先看一下 <em>encode<em>request() 和 _decode_request() 方法，因为我们需要把一 个 Request 对象存储到数据库中，但数据库无法直接存储对象，所以需要将 Request 序列化转成字符串再存储，而这两个方法就分别是序列化和反序列化的操作，利用 pickle 库来实现，一般在调用 push() 将 Request 存入数据库时会调用 _encode_request() 方法进行序列化，在调用 pop() 取出 Request 的时候会调用 _decode_request() 进行反序列化。 在父类中 __len</em></em>()、push() 和 pop() 方法都是未实现的，会直接抛出 NotImplementedError，因此这个类是不能直接被使用的，所以必须要实现一个子类来重写这三个方法，而不同的子类就会有不同的实现，也就有着不同的功能。 那么接下来就需要定义一些子类来继承 Base 类，并重写这几个方法，那在源码中就有三个子类的实现，它们分别是 FifoQueue、PriorityQueue、LifoQueue，我们分别来看下它们的实现原理。 首先是 FifoQueue：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FifoQueue</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="string">"""Per-spider FIFO queue"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the length of the queue"""</span></span><br><span class="line">        <span class="keyword">return</span> self.server.llen(self.key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Push a request"""</span></span><br><span class="line">        self.server.lpush(self.key, self._encode_request(request))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self, timeout=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Pop a request"""</span></span><br><span class="line">        <span class="keyword">if</span> timeout &gt; <span class="number">0</span>:</span><br><span class="line">            data = self.server.brpop(self.key, timeout)</span><br><span class="line">            <span class="keyword">if</span> isinstance(data, tuple):</span><br><span class="line">                data = data[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data = self.server.rpop(self.key)</span><br><span class="line">        <span class="keyword">if</span> data:</span><br><span class="line">            <span class="keyword">return</span> self._decode_request(data)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>可以看到这个类继承了 Base 类，并重写了 <strong>len</strong>()、push()、pop() 这三个方法，在这三个方法中都是对 server 对象的操作，而 server 对象就是一个 Redis 连接对象，我们可以直接调用其操作 Redis 的方法对数据库进行操作，可以看到这里的操作方法有 llen()、lpush()、rpop() 等，那这就代表此爬取队列是使用的 Redis 的列表，序列化后的 Request 会被存入列表中，就是列表的其中一个元素，<strong>len</strong>() 方法是获取列表的长度，push() 方法中调用了 lpush() 操作，这代表从列表左侧存入数据，pop() 方法中调用了 rpop() 操作，这代表从列表右侧取出数据。 所以 Request 在列表中的存取顺序是左侧进、右侧出，所以这是有序的进出，即先进先出，英文叫做 First Input First Output，也被简称作 Fifo，而此类的名称就叫做 FifoQueue。 另外还有一个与之相反的实现类，叫做 LifoQueue，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LifoQueue</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="string">"""Per-spider LIFO queue."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the length of the stack"""</span></span><br><span class="line">        <span class="keyword">return</span> self.server.llen(self.key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Push a request"""</span></span><br><span class="line">        self.server.lpush(self.key, self._encode_request(request))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self, timeout=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Pop a request"""</span></span><br><span class="line">        <span class="keyword">if</span> timeout &gt; <span class="number">0</span>:</span><br><span class="line">            data = self.server.blpop(self.key, timeout)</span><br><span class="line">            <span class="keyword">if</span> isinstance(data, tuple):</span><br><span class="line">                data = data[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data = self.server.lpop(self.key)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> data:</span><br><span class="line">            <span class="keyword">return</span> self._decode_request(data)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>与 FifoQueue 不同的就是它的 pop() 方法，在这里使用的是 lpop() 操作，也就是从左侧出，而 push() 方法依然是使用的 lpush() 操作，是从左侧入。那么这样达到的效果就是先进后出、后进先出，英文叫做 Last In First Out，简称为 Lifo，而此类名称就叫做 LifoQueue。同时这个存取方式类似栈的操作，所以其实也可以称作 StackQueue。 另外在源码中还有一个子类实现，叫做 PriorityQueue，顾名思义，它叫做优先级队列，实现如下：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PriorityQueue</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="string">"""Per-spider priority queue abstraction using redis' sorted set"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the length of the queue"""</span></span><br><span class="line">        <span class="keyword">return</span> self.server.zcard(self.key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Push a request"""</span></span><br><span class="line">        data = self._encode_request(request)</span><br><span class="line">        score = -request.priority</span><br><span class="line">        self.server.execute_command(<span class="string">'ZADD'</span>, self.key, score, data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self, timeout=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Pop a request</span></span><br><span class="line"><span class="string">        timeout not support in this queue class</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        pipe = self.server.pipeline()</span><br><span class="line">        pipe.multi()</span><br><span class="line">        pipe.zrange(self.key, <span class="number">0</span>, <span class="number">0</span>).zremrangebyrank(self.key, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">        results, count = pipe.execute()</span><br><span class="line">        <span class="keyword">if</span> results:</span><br><span class="line">            <span class="keyword">return</span> self._decode_request(results[<span class="number">0</span>])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们可以看到 <strong>len</strong>()、push()、pop() 方法中使用了 server 对象的 zcard()、zadd()、zrange() 操作，可以知道这里使用的存储结果是有序集合 Sorted Set，在这个集合中每个元素都可以设置一个分数，那么这个分数就代表优先级。 在 <strong>len</strong>() 方法里调用了 zcard() 操作，返回的就是有序集合的大小，也就是爬取队列的长度，在 push() 方法中调用了 zadd() 操作，就是向集合中添加元素，这里的分数指定成 Request 的优先级的相反数，因为分数低的会排在集合的前面，所以这里高优先级的 Request 就会存在集合的最前面。pop() 方法是首先调用了 zrange() 操作取出了集合的第一个元素，因为最高优先级的 Request 会存在集合最前面，所以第一个元素就是最高优先级的 Request，然后再调用 zremrangebyrank() 操作将这个元素删除，这样就完成了取出并删除的操作。 此队列是默认使用的队列，也就是爬取队列默认是使用有序集合来存储的。</p>
                  <h3 id="3-去重过滤"><a href="#3-去重过滤" class="headerlink" title="3. 去重过滤"></a>3. 去重过滤</h3>
                  <p>前面说过 Scrapy 的去重是利用集合来实现的，而在 Scrapy 分布式中的去重就需要利用共享的集合，那么这里使用的就是 Redis 中的集合数据结构。我们来看看去重类是怎样实现的，源码文件是 dupefilter.py，其内实现了一个 RFPDupeFilter 类，如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RFPDupeFilter</span><span class="params">(BaseDupeFilter)</span>:</span></span><br><span class="line">    <span class="string">"""Redis-based request duplicates filter.</span></span><br><span class="line"><span class="string">    This class can also be used with default Scrapy's scheduler.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    logger = logger</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, server, key, debug=False)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize the duplicates filter.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        server : redis.StrictRedis</span></span><br><span class="line"><span class="string">            The redis server instance.</span></span><br><span class="line"><span class="string">        key : str</span></span><br><span class="line"><span class="string">            Redis key Where to store fingerprints.</span></span><br><span class="line"><span class="string">        debug : bool, optional</span></span><br><span class="line"><span class="string">            Whether to log filtered requests.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.server = server</span><br><span class="line">        self.key = key</span><br><span class="line">        self.debug = debug</span><br><span class="line">        self.logdupes = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></span><br><span class="line">        <span class="string">"""Returns an instance from given settings.</span></span><br><span class="line"><span class="string">        This uses by default the key ``dupefilter:&lt;timestamp&gt;``. When using the</span></span><br><span class="line"><span class="string">        ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as</span></span><br><span class="line"><span class="string">        it needs to pass the spider name in the key.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        settings : scrapy.settings.Settings</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        RFPDupeFilter</span></span><br><span class="line"><span class="string">            A RFPDupeFilter instance.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        server = get_redis_from_settings(settings)</span><br><span class="line">        key = defaults.DUPEFILTER_KEY % &#123;<span class="string">'timestamp'</span>: int(time.time())&#125;</span><br><span class="line">        debug = settings.getbool(<span class="string">'DUPEFILTER_DEBUG'</span>)</span><br><span class="line">        <span class="keyword">return</span> cls(server, key=key, debug=debug)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="string">"""Returns instance from crawler.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        crawler : scrapy.crawler.Crawler</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        RFPDupeFilter</span></span><br><span class="line"><span class="string">            Instance of RFPDupeFilter.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> cls.from_settings(crawler.settings)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Returns True if request was already seen.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        request : scrapy.http.Request</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        bool</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        fp = self.request_fingerprint(request)</span><br><span class="line">        added = self.server.sadd(self.key, fp)</span><br><span class="line">        <span class="keyword">return</span> added == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_fingerprint</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""Returns a fingerprint for a given request.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        request : scrapy.http.Request</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> request_fingerprint(request)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self, reason=<span class="string">''</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Delete data on close. Called by Scrapy's scheduler.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        reason : str, optional</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.clear()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Clears fingerprints data."""</span></span><br><span class="line">        self.server.delete(self.key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="string">"""Logs given request.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        request : scrapy.http.Request</span></span><br><span class="line"><span class="string">        spider : scrapy.spiders.Spider</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.debug:</span><br><span class="line">            msg = <span class="string">"Filtered duplicate request: %(request) s"</span></span><br><span class="line">            self.logger.debug(msg, &#123;<span class="string">'request'</span>: request&#125;, extra=&#123;<span class="string">'spider'</span>: spider&#125;)</span><br><span class="line">        <span class="keyword">elif</span> self.logdupes:</span><br><span class="line">            msg = (<span class="string">"Filtered duplicate request %(request) s"</span></span><br><span class="line">                   <span class="string">"- no more duplicates will be shown"</span></span><br><span class="line">                   <span class="string">"(see DUPEFILTER_DEBUG to show all duplicates)"</span>)</span><br><span class="line">            self.logger.debug(msg, &#123;<span class="string">'request'</span>: request&#125;, extra=&#123;<span class="string">'spider'</span>: spider&#125;)</span><br><span class="line">            self.logdupes = <span class="literal">False</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里同样实现了一个 request_seen() 方法，和 Scrapy 中的 request_seen() 方法实现极其类似。不过这里集合使用的是 server 对象的 sadd() 操作，也就是集合不再是一个简单数据结构了，而是直接换成了数据库的存储方式。 鉴别重复的方式还是使用指纹，指纹同样是依靠 request_fingerprint() 方法来获取的。获取指纹之后就直接向集合添加指纹，如果添加成功，说明这个指纹原本不存在于集合中，返回值 1。代码中最后的返回结果是判定添加结果是否为 0，如果刚才的返回值为 1，那这个判定结果就是 False，也就是不重复，否则判定为重复。 这样我们就成功利用 Redis 的集合完成了指纹的记录和重复的验证。</p>
                  <h3 id="4-调度器"><a href="#4-调度器" class="headerlink" title="4. 调度器"></a>4. 调度器</h3>
                  <p>Scrapy-Redis 还帮我们实现了配合 Queue、DupeFilter 使用的调度器 Scheduler，源文件名称是 scheduler.py。我们可以指定一些配置，如 SCHEDULER_FLUSH_ON_START 即是否在爬取开始的时候清空爬取队列，SCHEDULER_PERSIST 即是否在爬取结束后保持爬取队列不清除。我们可以在 settings.py 里自由配置，而此调度器很好地实现了对接。 接下来我们看看两个核心的存取方法，实现如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enqueue_request</span><span class="params">(<span class="keyword">self</span>, request)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> request.dont_filter <span class="keyword">and</span> <span class="keyword">self</span>.df.request_seen(request)<span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.df.log(request, <span class="keyword">self</span>.spider)</span><br><span class="line">        <span class="keyword">return</span> False</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">stats:</span></span><br><span class="line">        <span class="keyword">self</span>.stats.inc_value(<span class="string">'scheduler/enqueued/redis'</span>, spider=<span class="keyword">self</span>.spider)</span><br><span class="line">    <span class="keyword">self</span>.queue.push(request)</span><br><span class="line">    <span class="keyword">return</span> True</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_request</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    block_pop_timeout = <span class="keyword">self</span>.idle_before_close</span><br><span class="line">    request = <span class="keyword">self</span>.queue.pop(block_pop_timeout)</span><br><span class="line">    <span class="keyword">if</span> request <span class="keyword">and</span> <span class="keyword">self</span>.<span class="symbol">stats:</span></span><br><span class="line">        <span class="keyword">self</span>.stats.inc_value(<span class="string">'scheduler/dequeued/redis'</span>, spider=<span class="keyword">self</span>.spider)</span><br><span class="line">    <span class="keyword">return</span> request</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>enqueue_request() 可以向队列中添加 Request，核心操作就是调用 Queue 的 push() 操作，还有一些统计和日志操作。next_request() 就是从队列中取 Request，核心操作就是调用 Queue 的 pop() 操作，此时如果队列中还有 Request，则 Request 会直接取出来，爬取继续，否则如果队列为空，爬取则会重新开始。</p>
                  <h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3>
                  <p>那么到现在为止我们就把之前所说的三个分布式的问题解决了，总结如下：</p>
                  <ul>
                    <li>爬取队列的实现，在这里提供了三种队列，使用了 Redis 的列表或有序集合来维护。</li>
                    <li>去重的实现，使用了 Redis 的集合来保存 Request 的指纹来提供重复过滤。</li>
                    <li>中断后重新爬取的实现，中断后 Redis 的队列没有清空，再次启动时调度器的 next_request() 会从队列中取到下一个 Request，继续爬取。</li>
                  </ul>
                  <h3 id="6-结语"><a href="#6-结语" class="headerlink" title="6. 结语"></a>6. 结语</h3>
                  <p>以上内容便是 Scrapy-Redis 的核心源码解析。Scrapy-Redis 中还提供了 Spider、Item Pipeline 的实现，不过它们并不是必须使用。 在下一节，我们会将 Scrapy-Redis 集成到之前所实现的 Scrapy 新浪微博项目中，实现多台主机协同爬取。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-09 10:08:42" itemprop="dateCreated datePublished" datetime="2019-12-09T10:08:42+08:00">2019-12-09</time>
                </span>
                <span id="/8465.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 14.2–Scrapy-Redis 源码解析" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>8.6k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>8 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8456.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8456.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 14.1–分布式爬虫原理</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="14-1-分布式爬虫原理"><a href="#14-1-分布式爬虫原理" class="headerlink" title="14.1 分布式爬虫原理"></a>14.1 分布式爬虫原理</h1>
                  <p>我们在前面已经实现了 Scrapy 微博爬虫，虽然爬虫是异步加多线程的，但是我们只能在一台主机上运行，所以爬取效率还是有限的，分布式爬虫则是将多台主机组合起来，共同完成一个爬取任务，这将大大提高爬取的效率。</p>
                  <h3 id="1-分布式爬虫架构"><a href="#1-分布式爬虫架构" class="headerlink" title="1. 分布式爬虫架构"></a>1. 分布式爬虫架构</h3>
                  <p>在了解分布式爬虫架构之前，首先回顾一下 Scrapy 的架构，如图 13-1 所示。 Scrapy 单机爬虫中有一个本地爬取队列 Queue，这个队列是利用 deque 模块实现的。如果新的 Request 生成就会放到队列里面，随后 Request 被 Scheduler 调度。之后，Request 交给 Downloader 执行爬取，简单的调度架构如图 14-1 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-29-113355.jpg" alt=""> 图 14-1 调度架构 如果两个 Scheduler 同时从队列里面取 Request，每个 Scheduler 都有其对应的 Downloader，那么在带宽足够、正常爬取且不考虑队列存取压力的情况下，爬取效率会有什么变化？没错，爬取效率会翻倍。 这样，Scheduler 可以扩展多个，Downloader 也可以扩展多个。而爬取队列 Queue 必须始终为一个，也就是所谓的共享爬取队列。这样才能保证 Scheduer 从队列里调度某个 Request 之后，其他 Scheduler 不会重复调度此 Request，就可以做到多个 Schduler 同步爬取。这就是分布式爬虫的基本雏形，简单调度架构如图 14-2 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-29-113406.jpg" alt=""> 图 14-2 调度架构 我们需要做的就是在多台主机上同时运行爬虫任务协同爬取，而协同爬取的前提就是共享爬取队列。这样各台主机就不需要各自维护爬取队列，而是从共享爬取队列存取 Request。但是各台主机还是有各自的 Scheduler 和 Downloader，所以调度和下载功能分别完成。如果不考虑队列存取性能消耗，爬取效率还是会成倍提高。</p>
                  <h3 id="2-维护爬取队列"><a href="#2-维护爬取队列" class="headerlink" title="2. 维护爬取队列"></a>2. 维护爬取队列</h3>
                  <p>那么这个队列用什么维护来好呢？我们首先需要考虑的就是性能问题，什么数据库存取效率高？我们自然想到基于内存存储的 Redis，而且 Redis 还支持多种数据结构，例如列表 List、集合 Set、有序集合 Sorted Set 等等，存取的操作也非常简单，所以在这里我们采用 Redis 来维护爬取队列。 这几种数据结构存储实际各有千秋，分析如下：</p>
                  <ul>
                    <li>列表数据结构有 lpush()、lpop()、rpush()、rpop() 方法，所以我们可以用它来实现一个先进先出式爬取队列，也可以实现一个先进后出栈式爬取队列。</li>
                    <li>集合的元素是无序的且不重复的，这样我们可以非常方便地实现一个随机排序的不重复的爬取队列。</li>
                    <li>有序集合带有分数表示，而 Scrapy 的 Request 也有优先级的控制，所以用有集合我们可以实现一个带优先级调度的队列。</li>
                  </ul>
                  <p>这些不同的队列我们需要根据具体爬虫的需求灵活选择。</p>
                  <h3 id="3-怎样来去重"><a href="#3-怎样来去重" class="headerlink" title="3. 怎样来去重"></a>3. 怎样来去重</h3>
                  <p>Scrapy 有自动去重，它的去重使用了 Python 中的集合。这个集合记录了 Scrapy 中每个 Request 的指纹，这个指纹实际上就是 Request 的散列值。我们可以看看 Scrapy 的源代码，如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import hashlib</span><br><span class="line">def request<span class="constructor">_fingerprint(<span class="params">request</span>, <span class="params">include_headers</span>=None)</span>:</span><br><span class="line">    <span class="keyword">if</span> include_headers:</span><br><span class="line">        include_headers = tuple(<span class="keyword">to</span><span class="constructor">_bytes(<span class="params">h</span>.<span class="params">lower</span>()</span>)</span><br><span class="line">                                 for h <span class="keyword">in</span> sorted(include_headers))</span><br><span class="line">    cache = <span class="module-access"><span class="module"><span class="identifier">_fingerprint_cache</span>.</span></span>setdefault(request, &#123;&#125;)</span><br><span class="line">    <span class="keyword">if</span> include_headers not <span class="keyword">in</span> cache:</span><br><span class="line">        fp = hashlib.sha1<span class="literal">()</span></span><br><span class="line">        fp.update(<span class="keyword">to</span><span class="constructor">_bytes(<span class="params">request</span>.<span class="params">method</span>)</span>)</span><br><span class="line">        fp.update(<span class="keyword">to</span><span class="constructor">_bytes(<span class="params">canonicalize_url</span>(<span class="params">request</span>.<span class="params">url</span>)</span>))</span><br><span class="line">        fp.update(request.body <span class="keyword">or</span> b'')</span><br><span class="line">        <span class="keyword">if</span> include_headers:</span><br><span class="line">            for hdr <span class="keyword">in</span> include_headers:</span><br><span class="line">                <span class="keyword">if</span> hdr <span class="keyword">in</span> request.headers:</span><br><span class="line">                    fp.update(hdr)</span><br><span class="line">                    for v <span class="keyword">in</span> request.headers.getlist(hdr):</span><br><span class="line">                        fp.update(v)</span><br><span class="line">        cache<span class="literal">[<span class="identifier">include_headers</span>]</span> = fp.hexdigest<span class="literal">()</span></span><br><span class="line">    return cache<span class="literal">[<span class="identifier">include_headers</span>]</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>request_fingerprint() 就是计算 Request 指纹的方法，其方法内部使用的是 hashlib 的 sha1() 方法。计算的字段包括 Request 的 Method、URL、Body、Headers 这几部分内容，这里只要有一点不同，那么计算的结果就不同。计算得到的结果是加密后的字符串，也就是指纹。每个 Request 都有独有的指纹，指纹就是一个字符串，判定字符串是否重复比判定 Request 对象是否重复容易得多，所以指纹可以作为判定 Request 是否重复的依据。 那么我们如何判定重复呢？Scrapy 是这样实现的，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">    <span class="keyword">self</span>.fingerprints = set()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(<span class="keyword">self</span>, request)</span></span><span class="symbol">:</span></span><br><span class="line">    fp = <span class="keyword">self</span>.request_fingerprint(request)</span><br><span class="line">    <span class="keyword">if</span> fp <span class="keyword">in</span> <span class="keyword">self</span>.<span class="symbol">fingerprints:</span></span><br><span class="line">        <span class="keyword">return</span> True</span><br><span class="line">    <span class="keyword">self</span>.fingerprints.add(fp)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在去重的类 RFPDupeFilter 中，有一个 request_seen() 方法，这个方法有一个参数 request，它的作用就是检测该 Request 对象是否重复。这个方法调用 request_fingerprint() 获取该 Request 的指纹，检测这个指纹是否存在于 fingerprints 变量中，而 fingerprints 是一个集合，集合的元素都是不重复的。如果指纹存在，那么就返回 True，说明该 Request 是重复的，否则这个指纹加入到集合中。如果下次还有相同的 Request 传递过来，指纹也是相同的，那么这时指纹就已经存在于集合中，Request 对象就会直接判定为重复。这样去重的目的就实现了。 Scrapy 的去重过程就是，利用集合元素的不重复特性来实现 Request 的去重。 对于分布式爬虫来说，我们肯定不能再用每个爬虫各自的集合来去重了。因为这样还是每个主机单独维护自己的集合，不能做到共享。多台主机如果生成了相同的 Request，只能各自去重，各个主机之间就无法做到去重了。 那么要实现去重，这个指纹集合也需要是共享的，Redis 正好有集合的存储数据结构，我们可以利用 Redis 的集合作为指纹集合，那么这样去重集合也是利用 Redis 共享的。每台主机新生成 Request 之后，把该 Request 的指纹与集合比对，如果指纹已经存在，说明该 Request 是重复的，否则将 Request 的指纹加入到这个集合中即可。利用同样的原理不同的存储结构我们也实现了分布式 Reqeust 的去重。</p>
                  <h3 id="4-防止中断"><a href="#4-防止中断" class="headerlink" title="4. 防止中断"></a>4. 防止中断</h3>
                  <p>在 Scrapy 中，爬虫运行时的 Request 队列放在内存中。爬虫运行中断后，这个队列的空间就被释放，此队列就被销毁了。所以一旦爬虫运行中断，爬虫再次运行就相当于全新的爬取过程。 要做到中断后继续爬取，我们可以将队列中的 Request 保存起来，下次爬取直接读取保存数据即可获取上次爬取的队列。我们在 Scrapy 中指定一个爬取队列的存储路径即可，这个路径使用 JOB_DIR 变量来标识，我们可以用如下命令来实现：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy crawl spider -s <span class="attribute">JOBDIR</span>=crawls/spider</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>更加详细的使用方法可以参见官方文档，链接为：<a href="https://doc.scrapy.org/en/latest/topics/jobs.html" target="_blank" rel="noopener">https://doc.scrapy.org/en/latest/topics/jobs.html</a>。 在 Scrapy 中，我们实际是把爬取队列保存到本地，第二次爬取直接读取并恢复队列即可。那么在分布式架构中我们还用担心这个问题吗？不需要。因为爬取队列本身就是用数据库保存的，如果爬虫中断了，数据库中的 Request 依然是存在的，下次启动就会接着上次中断的地方继续爬取。 所以，当 Redis 的队列为空时，爬虫会重新爬取；当 Redis 的队列不为空时，爬虫便会接着上次中断之处继续爬取。</p>
                  <h3 id="5-架构实现"><a href="#5-架构实现" class="headerlink" title="5. 架构实现"></a>5. 架构实现</h3>
                  <p>我们接下来就需要在程序中实现这个架构了。首先实现一个共享的爬取队列，还要实现去重的功能。另外，重写一个 Scheduer 的实现，使之可以从共享的爬取队列存取 Request。 幸运的是，已经有人实现了这些逻辑和架构，并发布成叫 Scrapy-Redis 的 Python 包。接下来，我们看看 Scrapy-Redis 的源码实现，以及它的详细工作原理。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-08 09:52:07" itemprop="dateCreated datePublished" datetime="2019-12-08T09:52:07+08:00">2019-12-08</time>
                </span>
                <span id="/8456.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 14.1–分布式爬虫原理" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3.6k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>3 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8453.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8453.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.13–Scrapy 爬取新浪微博</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-13-Scrapy-爬取新浪微博"><a href="#13-13-Scrapy-爬取新浪微博" class="headerlink" title="13.13 Scrapy 爬取新浪微博"></a>13.13 Scrapy 爬取新浪微博</h1>
                  <p>前面讲解了 Scrapy 中各个模块基本使用方法以及代理池、Cookies 池。接下来我们以一个反爬比较强的网站新浪微博为例，来实现一下 Scrapy 的大规模爬取。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>本次爬取的目标是新浪微博用户的公开基本信息，如用户昵称、头像、用户的关注、粉丝列表以及发布的微博等，这些信息抓取之后保存至 MongoDB。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保前文所讲的代理池、Cookies 池已经实现并可以正常运行，安装 Scrapy、PyMongo 库，如没有安装可以参考前文内容。</p>
                  <h3 id="3-爬取思路"><a href="#3-爬取思路" class="headerlink" title="3. 爬取思路"></a>3. 爬取思路</h3>
                  <p>首先我们要实现用户的大规模爬取。这里采用的爬取方式是，以微博的几个大 V 为起始点，爬取他们各自的粉丝和关注列表，然后获取粉丝和关注列表的粉丝和关注列表，以此类推，这样下去就可以实现递归爬取。如果一个用户与其他用户有社交网络上的关联，那他们的信息就会被爬虫抓取到，这样我们就可以做到对所有用户的爬取。通过这种方式，我们可以得到用户的唯一 ID，再根据 ID 获取每个用户发布的微博即可。</p>
                  <h3 id="4-爬取分析"><a href="#4-爬取分析" class="headerlink" title="4. 爬取分析"></a>4. 爬取分析</h3>
                  <p>这里我们选取的爬取站点是：<a href="https://m.weibo.cn，此站点是微博移动端的站点。打开该站点会跳转到登录页面，这是因为主页做了登录限制。不过我们可以直接打开某个用户详情页面，如图" target="_blank" rel="noopener">https://m.weibo.cn，此站点是微博移动端的站点。打开该站点会跳转到登录页面，这是因为主页做了登录限制。不过我们可以直接打开某个用户详情页面，如图</a> 13-32 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034836.jpg" alt=""> 图 13-32 个人详情页面 我们在页面最上方可以看到她的关注和粉丝数量。我们点击关注，进入到她的关注列表，如图 13-33 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034839.jpg" alt=""> 图 13-33 关注列表 我们打开开发者工具，切换到 XHR 过滤器，一直下拉关注列表，即可看到下方会出现很多 Ajax 请求，这些请求就是获取关注列表的 Ajax 请求，如图 13-34 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034852.png" alt=""> 图 13-34 请求列表 我们打开第一个 Ajax 请求看一下，发现它的链接为：<a href="https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_1916655407&amp;luicode=10000011&amp;lfid=1005051916655407&amp;featurecode=20000320&amp;type=uid&amp;value=1916655407&amp;page=2" target="_blank" rel="noopener">https://m.weibo.cn/api/container/getIndex?containerid=231051<em>-_followers</em>-_1916655407&amp;luicode=10000011&amp;lfid=1005051916655407&amp;featurecode=20000320&amp;type=uid&amp;value=1916655407&amp;page=2</a>，详情如图 13-35 和 13-36 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034901.jpg" alt=""> 图 13-35 请求详情 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034906.jpg" alt=""> 图 13-36 响应结果 请求类型是 GET 类型，返回结果是 JSON 格式，我们将其展开之后即可看到其关注的用户的基本信息。接下来我们只需要构造这个请求的参数。此链接一共有 7 个参数，如图 13-37 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034931.jpg" alt=""> 图 13-37 参数信息 其中最主要的参数就是 containerid 和 page。有了这两个参数，我们同样可以获取请求结果。我们可以将接口精简为：<a href="https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_1916655407&amp;page=2" target="_blank" rel="noopener">https://m.weibo.cn/api/container/getIndex?containerid=231051<em>-_followers</em>-_1916655407&amp;page=2</a>，这里的 containerid 的前半部分是固定的，后半部分是用户的 id。所以这里参数就可以构造出来了，只需要修改 containerid 最后的 id 和 page 参数即可获取分页形式的关注列表信息。 利用同样的方法，我们也可以分析用户详情的 Ajax 链接、用户微博列表的 Ajax 链接，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># 用户详情 API</span></span><br><span class="line"><span class="attr">user_url</span> = <span class="string">'https://m.weibo.cn/api/container/getIndex?uid=&#123;uid&#125;&amp;type=uid&amp;value=&#123;uid&#125;&amp;containerid=100505&#123;uid&#125;'</span></span><br><span class="line"><span class="comment"># 关注列表 API</span></span><br><span class="line"><span class="attr">follow_url</span> = <span class="string">'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_&#123;uid&#125;&amp;page=&#123;page&#125;'</span></span><br><span class="line"><span class="comment"># 粉丝列表 API</span></span><br><span class="line"><span class="attr">fan_url</span> = <span class="string">'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_fans_-_&#123;uid&#125;&amp;page=&#123;page&#125;'</span></span><br><span class="line"><span class="comment"># 微博列表 API</span></span><br><span class="line"><span class="attr">weibo_url</span> = <span class="string">'https://m.weibo.cn/api/container/getIndex?uid=&#123;uid&#125;&amp;type=uid&amp;page=&#123;page&#125;&amp;containerid=107603&#123;uid&#125;'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此处的 uid 和 page 分别代表用户 ID 和分页页码。 注意，这个 API 可能随着时间的变化或者微博的改版而变化，以实测为准。 我们从几个大 V 开始抓取，抓取他们的粉丝、关注列表、微博信息，然后递归抓取他们的粉丝和关注列表的粉丝、关注列表、微博信息，递归抓取，最后保存微博用户的基本信息、关注和粉丝列表、发布的微博。 我们选择 MongoDB 作为存储的数据库，可以更方便地存储用户的粉丝和关注列表。</p>
                  <h3 id="5-新建项目"><a href="#5-新建项目" class="headerlink" title="5. 新建项目"></a>5. 新建项目</h3>
                  <p>接下来，我们用 Scrapy 来实现这个抓取过程。首先创建一个项目，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy startproject weibo</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>进入项目中，新建一个 Spider，名为 weibocn，命令如下所示：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">genspider</span> <span class="selector-tag">weibocn</span> <span class="selector-tag">m</span><span class="selector-class">.weibo</span><span class="selector-class">.cn</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们首先修改 Spider，配置各个 Ajax 的 URL，选取几个大 V，将他们的 ID 赋值成一个列表，实现 start_requests() 方法，也就是依次抓取各个大 V 的个人详情，然后用 parse_user() 进行解析，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy import Request, Spider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeiboSpider</span>(<span class="title">Spider</span>):</span></span><br><span class="line">    name = <span class="string">'weibocn'</span></span><br><span class="line">    allowed_domains = [<span class="string">'m.weibo.cn'</span>]</span><br><span class="line">    user_url = <span class="string">'https://m.weibo.cn/api/container/getIndex?uid=&#123;uid&#125;&amp;type=uid&amp;value=&#123;uid&#125;&amp;containerid=100505&#123;uid&#125;'</span></span><br><span class="line">    follow_url = <span class="string">'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_&#123;uid&#125;&amp;page=&#123;page&#125;'</span></span><br><span class="line">    fan_url = <span class="string">'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_fans_-_&#123;uid&#125;&amp;page=&#123;page&#125;'</span></span><br><span class="line">    weibo_url = <span class="string">'https://m.weibo.cn/api/container/getIndex?uid=&#123;uid&#125;&amp;type=uid&amp;page=&#123;page&#125;&amp;containerid=107603&#123;uid&#125;'</span></span><br><span class="line">    start_users = [<span class="string">'3217179555'</span>, <span class="string">'1742566624'</span>, <span class="string">'2282991915'</span>, <span class="string">'1288739185'</span>, <span class="string">'3952070245'</span>, <span class="string">'5878659096'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">for</span> uid <span class="keyword">in</span> <span class="keyword">self</span>.<span class="symbol">start_users:</span></span><br><span class="line">            <span class="keyword">yield</span> Request(<span class="keyword">self</span>.user_url.format(uid=uid), callback=<span class="keyword">self</span>.parse_user)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_user</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.logger.debug(response)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="6-创建-Item"><a href="#6-创建-Item" class="headerlink" title="6. 创建 Item"></a>6. 创建 Item</h3>
                  <p>接下来，我们解析用户的基本信息并生成 Item。这里我们先定义几个 Item，如用户、用户关系、微博的 Item，如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item, Field</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="symbol">UserItem</span>(<span class="symbol">Item</span>):</span><br><span class="line">    <span class="symbol">collection</span> = '<span class="symbol">users</span>'</span><br><span class="line">    <span class="symbol">id</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">name</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">avatar</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">cover</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">gender</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">description</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">fans_count</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">follows_count</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">weibos_count</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">verified</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">verified_reason</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">verified_type</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">follows</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">fans</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">crawled_at</span> = <span class="symbol">Field</span>()</span><br><span class="line"></span><br><span class="line"><span class="symbol">class</span> <span class="symbol">UserRelationItem</span>(<span class="symbol">Item</span>):</span><br><span class="line">    <span class="symbol">collection</span> = '<span class="symbol">users</span>'</span><br><span class="line">    <span class="symbol">id</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">follows</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">fans</span> = <span class="symbol">Field</span>()</span><br><span class="line"></span><br><span class="line"><span class="symbol">class</span> <span class="symbol">WeiboItem</span>(<span class="symbol">Item</span>):</span><br><span class="line">    <span class="symbol">collection</span> = '<span class="symbol">weibos</span>'</span><br><span class="line">    <span class="symbol">id</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">attitudes_count</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">comments_count</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">reposts_count</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">picture</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">pictures</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">source</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">text</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">raw_text</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">thumbnail</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">user</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">created_at</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">crawled_at</span> = <span class="symbol">Field</span>()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里定义了 collection 字段，指明保存的 Collection 的名称。用户的关注和粉丝列表直接定义为一个单独的 UserRelationItem，其中 id 就是用户的 ID，follows 就是用户关注列表，fans 是粉丝列表，但这并不意味着我们会将关注和粉丝列表存到一个单独的 Collection 里。后面我们会用 Pipeline 对各个 Item 进行处理、合并存储到用户的 Collection 里，因此 Item 和 Collection 并不一定是完全对应的。</p>
                  <h3 id="7-提取数据"><a href="#7-提取数据" class="headerlink" title="7. 提取数据"></a>7. 提取数据</h3>
                  <p>我们开始解析用户的基本信息，实现 parse_user() 方法，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse_user(self, response):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    解析用户信息</span></span><br><span class="line"><span class="string">    :param response: Response 对象</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    result = json.loads(response.text)</span><br><span class="line">    <span class="keyword">if</span> result.<span class="builtin-name">get</span>(<span class="string">'userInfo'</span>):</span><br><span class="line">        user_info = result.<span class="builtin-name">get</span>(<span class="string">'userInfo'</span>)</span><br><span class="line">        user_item = UserItem()</span><br><span class="line">        field_map = &#123;</span><br><span class="line">            <span class="string">'id'</span>: <span class="string">'id'</span>, <span class="string">'name'</span>: <span class="string">'screen_name'</span>, <span class="string">'avatar'</span>: <span class="string">'profile_image_url'</span>, <span class="string">'cover'</span>: <span class="string">'cover_image_phone'</span>,</span><br><span class="line">            <span class="string">'gender'</span>: <span class="string">'gender'</span>, <span class="string">'description'</span>: <span class="string">'description'</span>, <span class="string">'fans_count'</span>: <span class="string">'followers_count'</span>,</span><br><span class="line">            <span class="string">'follows_count'</span>: <span class="string">'follow_count'</span>, <span class="string">'weibos_count'</span>: <span class="string">'statuses_count'</span>, <span class="string">'verified'</span>: <span class="string">'verified'</span>,</span><br><span class="line">            <span class="string">'verified_reason'</span>: <span class="string">'verified_reason'</span>, <span class="string">'verified_type'</span>: <span class="string">'verified_type'</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> field, attr <span class="keyword">in</span> field_map.items():</span><br><span class="line">            user_item[field] = user_info.<span class="builtin-name">get</span>(attr)</span><br><span class="line">        yield user_item</span><br><span class="line">        # 关注</span><br><span class="line">        uid = user_info.<span class="builtin-name">get</span>(<span class="string">'id'</span>)</span><br><span class="line">        yield Request(self.follow_url.format(<span class="attribute">uid</span>=uid, <span class="attribute">page</span>=1), <span class="attribute">callback</span>=self.parse_follows,</span><br><span class="line">                      meta=&#123;<span class="string">'page'</span>: 1, <span class="string">'uid'</span>: uid&#125;)</span><br><span class="line">        # 粉丝</span><br><span class="line">        yield Request(self.fan_url.format(<span class="attribute">uid</span>=uid, <span class="attribute">page</span>=1), <span class="attribute">callback</span>=self.parse_fans,</span><br><span class="line">                      meta=&#123;<span class="string">'page'</span>: 1, <span class="string">'uid'</span>: uid&#125;)</span><br><span class="line">        # 微博</span><br><span class="line">        yield Request(self.weibo_url.format(<span class="attribute">uid</span>=uid, <span class="attribute">page</span>=1), <span class="attribute">callback</span>=self.parse_weibos,</span><br><span class="line">                      meta=&#123;<span class="string">'page'</span>: 1, <span class="string">'uid'</span>: uid&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们一共完成了两个操作。</p>
                  <ul>
                    <li>解析 JSON 提取用户信息并生成 UserItem 返回。我们并没有采用常规的逐个赋值的方法，而是定义了一个字段映射关系。我们定义的字段名称可能和 JSON 中用户的字段名称不同，所以在这里定义成一个字典，然后遍历字典的每个字段实现逐个字段的赋值。</li>
                    <li>构造用户的关注、粉丝、微博的第一页的链接，并生成 Request，这里需要的参数只有用户的 ID。另外，初始分页页码直接设置为 1 即可。</li>
                  </ul>
                  <p>接下来，我们还需要保存用户的关注和粉丝列表。以关注列表为例，其解析方法为 parse_follows()，实现如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse_follows(self, response):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    解析用户关注</span></span><br><span class="line"><span class="string">    :param response: Response 对象</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    result = json.loads(response.text)</span><br><span class="line">    <span class="keyword">if</span> result.<span class="builtin-name">get</span>(<span class="string">'ok'</span>) <span class="keyword">and</span> result.<span class="builtin-name">get</span>(<span class="string">'cards'</span>) <span class="keyword">and</span> len(result.<span class="builtin-name">get</span>(<span class="string">'cards'</span>)) <span class="keyword">and</span> result.<span class="builtin-name">get</span>(<span class="string">'cards'</span>)[-1].<span class="builtin-name">get</span>(<span class="string">'card_group'</span>):</span><br><span class="line">        # 解析用户</span><br><span class="line">        follows = result.<span class="builtin-name">get</span>(<span class="string">'cards'</span>)[-1].<span class="builtin-name">get</span>(<span class="string">'card_group'</span>)</span><br><span class="line">        <span class="keyword">for</span> follow <span class="keyword">in</span> follows:</span><br><span class="line">            <span class="keyword">if</span> follow.<span class="builtin-name">get</span>(<span class="string">'user'</span>):</span><br><span class="line">                uid = follow.<span class="builtin-name">get</span>(<span class="string">'user'</span>).<span class="builtin-name">get</span>(<span class="string">'id'</span>)</span><br><span class="line">                yield Request(self.user_url.format(<span class="attribute">uid</span>=uid), <span class="attribute">callback</span>=self.parse_user)</span><br><span class="line">        # 关注列表</span><br><span class="line">        uid = response.meta.<span class="builtin-name">get</span>(<span class="string">'uid'</span>)</span><br><span class="line">        user_relation_item = UserRelationItem()</span><br><span class="line">        follows = [&#123;<span class="string">'id'</span>: follow.<span class="builtin-name">get</span>(<span class="string">'user'</span>).<span class="builtin-name">get</span>(<span class="string">'id'</span>), <span class="string">'name'</span>: follow.<span class="builtin-name">get</span>(<span class="string">'user'</span>).<span class="builtin-name">get</span>(<span class="string">'screen_name'</span>)&#125; <span class="keyword">for</span> follow <span class="keyword">in</span></span><br><span class="line">                   follows]</span><br><span class="line">        user_relation_item[<span class="string">'id'</span>] = uid</span><br><span class="line">        user_relation_item[<span class="string">'follows'</span>] = follows</span><br><span class="line">        user_relation_item[<span class="string">'fans'</span>] = []</span><br><span class="line">        yield user_relation_item</span><br><span class="line">        # 下一页关注</span><br><span class="line">       <span class="built_in"> page </span>= response.meta.<span class="builtin-name">get</span>(<span class="string">'page'</span>) + 1</span><br><span class="line">        yield Request(self.follow_url.format(<span class="attribute">uid</span>=uid, <span class="attribute">page</span>=page),</span><br><span class="line">                      <span class="attribute">callback</span>=self.parse_follows, meta=&#123;<span class="string">'page'</span>: page, <span class="string">'uid'</span>: uid&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>那么在这个方法里面我们做了如下三件事。</p>
                  <ul>
                    <li>解析关注列表中的每个用户信息并发起新的解析请求。我们首先解析关注列表的信息，得到用户的 ID，然后再利用 user_url 构造访问用户详情的 Request，回调就是刚才所定义的 parse_user() 方法。</li>
                    <li>提取用户关注列表内的关键信息并生成 UserRelationItem。id 字段直接设置成用户的 ID，JSON 返回数据中的用户信息有很多冗余字段。在这里我们只提取了关注用户的 ID 和用户名，然后把它们赋值给 follows 字段，fans 字段设置成空列表。这样我们就建立了一个存有用户 ID 和用户部分关注列表的 UserRelationItem，之后合并且保存具有同一个 ID 的 UserRelationItem 的关注和粉丝列表。</li>
                    <li>提取下一页关注。只需要将此请求的分页页码加 1 即可。分页页码通过 Request 的 meta 属性进行传递，Response 的 meta 来接收。这样我们构造并返回下一页的关注列表的 Request。</li>
                  </ul>
                  <p>抓取粉丝列表的原理和抓取关注列表原理相同，在此不再赘述。 接下来我们还差一个方法的实现，即 parse_weibos()，它用来抓取用户的微博信息，实现如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse_weibos(self, response):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    解析微博列表</span></span><br><span class="line"><span class="string">    :param response: Response 对象</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    result = json.loads(response.text)</span><br><span class="line">    <span class="keyword">if</span> result.<span class="builtin-name">get</span>(<span class="string">'ok'</span>) <span class="keyword">and</span> result.<span class="builtin-name">get</span>(<span class="string">'cards'</span>):</span><br><span class="line">        weibos = result.<span class="builtin-name">get</span>(<span class="string">'cards'</span>)</span><br><span class="line">        <span class="keyword">for</span> weibo <span class="keyword">in</span> weibos:</span><br><span class="line">            mblog = weibo.<span class="builtin-name">get</span>(<span class="string">'mblog'</span>)</span><br><span class="line">            <span class="keyword">if</span> mblog:</span><br><span class="line">                weibo_item = WeiboItem()</span><br><span class="line">                field_map = &#123;</span><br><span class="line">                    <span class="string">'id'</span>: <span class="string">'id'</span>, <span class="string">'attitudes_count'</span>: <span class="string">'attitudes_count'</span>, <span class="string">'comments_count'</span>: <span class="string">'comments_count'</span>, <span class="string">'created_at'</span>: <span class="string">'created_at'</span>,</span><br><span class="line">                    <span class="string">'reposts_count'</span>: <span class="string">'reposts_count'</span>, <span class="string">'picture'</span>: <span class="string">'original_pic'</span>, <span class="string">'pictures'</span>: <span class="string">'pics'</span>,</span><br><span class="line">                    <span class="string">'source'</span>: <span class="string">'source'</span>, <span class="string">'text'</span>: <span class="string">'text'</span>, <span class="string">'raw_text'</span>: <span class="string">'raw_text'</span>, <span class="string">'thumbnail'</span>: <span class="string">'thumbnail_pic'</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">for</span> field, attr <span class="keyword">in</span> field_map.items():</span><br><span class="line">                    weibo_item[field] = mblog.<span class="builtin-name">get</span>(attr)</span><br><span class="line">                weibo_item[<span class="string">'user'</span>] = response.meta.<span class="builtin-name">get</span>(<span class="string">'uid'</span>)</span><br><span class="line">                yield weibo_item</span><br><span class="line">        # 下一页微博</span><br><span class="line">        uid = response.meta.<span class="builtin-name">get</span>(<span class="string">'uid'</span>)</span><br><span class="line">       <span class="built_in"> page </span>= response.meta.<span class="builtin-name">get</span>(<span class="string">'page'</span>) + 1</span><br><span class="line">        yield Request(self.weibo_url.format(<span class="attribute">uid</span>=uid, <span class="attribute">page</span>=page), <span class="attribute">callback</span>=self.parse_weibos,</span><br><span class="line">                      meta=&#123;<span class="string">'uid'</span>: uid, <span class="string">'page'</span>: page&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里 parse_weibos() 方法完成了两件事。</p>
                  <ul>
                    <li>提取用户的微博信息，并生成 WeiboItem。这里同样建立了一个字段映射表，实现批量字段赋值。</li>
                    <li>提取下一页的微博列表。这里同样需要传入用户 ID 和分页页码。</li>
                  </ul>
                  <p>到目前为止，微博的 Spider 已经完成。后面还需要对数据进行数据清洗存储，以及对接代理池、Cookies 池来防止反爬虫。</p>
                  <h3 id="8-数据清洗"><a href="#8-数据清洗" class="headerlink" title="8. 数据清洗"></a>8. 数据清洗</h3>
                  <p>有些微博的时间可能不是标准的时间，比如它可能显示为刚刚、几分钟前、几小时前、昨天等。这里我们需要统一转化这些时间，实现一个 parse_time() 方法，如下所示：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse_time(self, <span class="type">date</span>):</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">' 刚刚 '</span>, <span class="type">date</span>):</span><br><span class="line">        <span class="type">date</span> = <span class="type">time</span>.strftime(<span class="string">'% Y-% m-% d % H:% M'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>(<span class="type">time</span>.time()))</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">'d + 分钟前 '</span>, <span class="type">date</span>):</span><br><span class="line">        minute = re.match(<span class="string">'(d+)'</span>, <span class="type">date</span>).<span class="keyword">group</span>(<span class="number">1</span>)</span><br><span class="line">        <span class="type">date</span> = <span class="type">time</span>.strftime(<span class="string">'% Y-% m-% d % H:% M'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>(<span class="type">time</span>.time() - <span class="type">float</span>(minute) * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">'d + 小时前 '</span>, <span class="type">date</span>):</span><br><span class="line">        hour = re.match(<span class="string">'(d+)'</span>, <span class="type">date</span>).<span class="keyword">group</span>(<span class="number">1</span>)</span><br><span class="line">        <span class="type">date</span> = <span class="type">time</span>.strftime(<span class="string">'% Y-% m-% d % H:% M'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>(<span class="type">time</span>.time() - <span class="type">float</span>(hour) * <span class="number">60</span> * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">' 昨天.*'</span>, <span class="type">date</span>):</span><br><span class="line">        <span class="type">date</span> = re.match(<span class="string">' 昨天 (.*)'</span>, <span class="type">date</span>).<span class="keyword">group</span>(<span class="number">1</span>).strip()</span><br><span class="line">        <span class="type">date</span> = <span class="type">time</span>.strftime(<span class="string">'% Y-% m-% d'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>() - <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>) + <span class="string">' '</span> + <span class="type">date</span></span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">'d&#123;2&#125;-d&#123;2&#125;'</span>, <span class="type">date</span>):</span><br><span class="line">        <span class="type">date</span> = <span class="type">time</span>.strftime(<span class="string">'% Y-'</span>, <span class="type">time</span>.<span class="built_in">localtime</span>()) + <span class="type">date</span> + <span class="string">' 00:00'</span></span><br><span class="line">    <span class="keyword">return</span> <span class="type">date</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们用正则来提取一些关键数字，用 time 库来实现标准时间的转换。 以 X 分钟前的处理为例，爬取的时间会赋值为 created_at 字段。我们首先用正则匹配这个时间，表达式写作 d + 分钟前，如果提取到的时间符合这个表达式，那么就提取出其中的数字，这样就可以获取分钟数了。接下来使用 time 模块的 strftime() 方法，第一个参数传入要转换的时间格式，第二个参数就是时间戳。这里我们用当前的时间戳减去此分钟数乘以 60 就是当时的时间戳，这样我们就可以得到格式化后的正确时间了。 然后 Pipeline 可以实现如下处理：</p>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">class WeiboPipeline():</span><br><span class="line">    def process_item(self, <span class="keyword">item</span>, spider):</span><br><span class="line">        <span class="keyword">if</span> isinstance(<span class="keyword">item</span>, WeiboItem):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">item</span>.<span class="built_in">get</span>(<span class="string">'created_at'</span>):</span><br><span class="line">                <span class="keyword">item</span>[<span class="string">'created_at'</span>] = <span class="keyword">item</span>[<span class="string">'created_at'</span>].strip()</span><br><span class="line">                <span class="keyword">item</span>[<span class="string">'created_at'</span>] = self.parse_time(<span class="keyword">item</span>.<span class="built_in">get</span>(<span class="string">'created_at'</span>))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们在 Spider 里没有对 crawled_at 字段赋值，它代表爬取时间，我们可以统一将其赋值为当前时间，实现如下所示：</p>
                  <figure class="highlight haskell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">TimePipeline</span>():</span></span><br><span class="line"><span class="class">    def process_item(<span class="title">self</span>, <span class="title">item</span>, <span class="title">spider</span>):</span></span><br><span class="line"><span class="class">        if isinstance(<span class="title">item</span>, <span class="type">UserItem</span>) or isinstance(<span class="title">item</span>, <span class="type">WeiboItem</span>):</span></span><br><span class="line"><span class="class">            now = time.strftime('% <span class="type">Y</span>-% <span class="title">m</span>-% <span class="title">d</span> % <span class="type">H</span>:% <span class="type">M</span>', <span class="title">time</span>.<span class="title">localtime</span>())</span></span><br><span class="line"><span class="class">            item['crawled_at'] = now</span></span><br><span class="line"><span class="class">        return item</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们判断了 item 如果是 UserItem 或 WeiboItem 类型，那么就给它的 crawled_at 字段赋值为当前时间。 通过上面的两个 Pipeline，我们便完成了数据清洗工作，这里主要是时间的转换。</p>
                  <h3 id="9-数据存储"><a href="#9-数据存储" class="headerlink" title="9. 数据存储"></a>9. 数据存储</h3>
                  <p>数据清洗完毕之后，我们就要将数据保存到 MongoDB 数据库。我们在这里实现 MongoPipeline 类，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, mongo_uri, mongo_db)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.mongo_uri = mongo_uri</span><br><span class="line">        <span class="keyword">self</span>.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> cls(mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>), mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client = pymongo.MongoClient(<span class="keyword">self</span>.mongo_uri)</span><br><span class="line">        <span class="keyword">self</span>.db = <span class="keyword">self</span>.client[<span class="keyword">self</span>.mongo_db]</span><br><span class="line">        <span class="keyword">self</span>.db[UserItem.collection].create_index([(<span class="string">'id'</span>, pymongo.ASCENDING)])</span><br><span class="line">        <span class="keyword">self</span>.db[WeiboItem.collection].create_index([(<span class="string">'id'</span>, pymongo.ASCENDING)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(item, UserItem) <span class="keyword">or</span> isinstance(item, WeiboItem)<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">self</span>.db[item.collection].update(&#123;<span class="string">'id'</span>: item.get(<span class="string">'id'</span>)&#125;, &#123;<span class="string">'$set'</span>: item&#125;, True)</span><br><span class="line">        <span class="keyword">if</span> isinstance(item, UserRelationItem)<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">self</span>.db[item.collection].update(&#123;<span class="string">'id'</span>: item.get(<span class="string">'id'</span>)&#125;,</span><br><span class="line">                &#123;<span class="string">'$addToSet'</span><span class="symbol">:</span></span><br><span class="line">                    &#123;<span class="string">'follows'</span>: &#123;<span class="string">'$each'</span>: item[<span class="string">'follows'</span>]&#125;,</span><br><span class="line">                        <span class="string">'fans'</span>: &#123;<span class="string">'$each'</span>: item[<span class="string">'fans'</span>]&#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;, True)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>当前的 MongoPipeline 和前面我们所写的有所不同，主要有以下几点。</p>
                  <ul>
                    <li>在 open_spider() 方法里面添加了 Collection 的索引，在这里为两个 Item 都做了索引，索引的字段是 id，由于我们这次是大规模爬取，同时在爬取过程中涉及到数据的更新问题，所以我们为每个 Collection 建立了索引，建立了索引之后可以大大提高检索效率。</li>
                    <li>在 process_item() 方法里存储使用的是 update() 方法，第一个参数是查询条件，第二个参数是爬取的 Item，这里我们使用了 $set 操作符，这样我们如果爬取到了重复的数据即可对数据进行更新，同时不会删除已存在的字段，如果这里不加 $set 操作符，那么会直接进行 item 替换，这样可能会导致已存在的字段如关注和粉丝列表清空，所以这里必须要加上 $set 操作符。第三个参数我们设置为了 True，这个参数起到的作用是如果数据不存在，则插入数据。这样我们就可以做到数据存在即更新、数据不存在即插入，这样就达到了去重的效果。</li>
                    <li>对于用户的关注和粉丝列表，我们在这里使用了一个新的操作符，叫做 $addToSet，这个操作符可以向列表类型的字段插入数据同时去重，接下来它的值就是需要操作的字段名称，我们在这里又利用了 $each 操作符对需要插入的列表数据进行了遍历，这样就可以逐条插入用户的关注或粉丝数据到指定的字段了，关于该操作更多的解释可以参考 MongoDB 的官方文档，链接为：<a href="https://docs.mongodb.com/manual/reference/operator/update/addToSet/" target="_blank" rel="noopener">https://docs.mongodb.com/manual/reference/operator/update/addToSet/</a>。</li>
                  </ul>
                  <h3 id="10-Cookies-池对接"><a href="#10-Cookies-池对接" class="headerlink" title="10. Cookies 池对接"></a>10. Cookies 池对接</h3>
                  <p>新浪微博的反爬能力非常强，我们需要做一些防范反爬虫的措施才可以顺利完成数据爬取。 如果没有登录而直接请求微博的 API 接口，这非常容易导致 403 状态码。这个情况我们在 10.2 节也提过。所以在这里我们实现一个 Middleware，为每个 Request 添加随机的 Cookies。 我们先开启 Cookies 池，使 API 模块正常运行。例如在本地运行 5000 端口，访问：<a href="http://localhost:5000/weibo/random" target="_blank" rel="noopener">http://localhost:5000/weibo/random</a> 即可获取随机的 Cookies，当然也可以将 Cookies 池部署到远程的服务器，这样只需要更改一下访问的链接就好了。 那么在这里我们将 Cookies 池在本地启动起来，再实现一个 Middleware 如下：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CookiesMiddleware</span>():</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, cookies_url)</span></span><span class="symbol">:</span></span><br><span class="line">       <span class="keyword">self</span>.logger = logging.getLogger(__name_<span class="number">_</span>)</span><br><span class="line">       <span class="keyword">self</span>.cookies_url = cookies_url</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">get_random_cookies</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">       <span class="symbol">try:</span></span><br><span class="line">           response = requests.get(<span class="keyword">self</span>.cookies_url)</span><br><span class="line">           <span class="keyword">if</span> response.status_code == <span class="number">200</span><span class="symbol">:</span></span><br><span class="line">               cookies = json.loads(response.text)</span><br><span class="line">               <span class="keyword">return</span> cookies</span><br><span class="line">       except requests.<span class="symbol">ConnectionError:</span></span><br><span class="line">           <span class="keyword">return</span> False</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(<span class="keyword">self</span>, request, spider)</span></span><span class="symbol">:</span></span><br><span class="line">       <span class="keyword">self</span>.logger.debug(<span class="string">' 正在获取 Cookies'</span>)</span><br><span class="line">       cookies = <span class="keyword">self</span>.get_random_cookies()</span><br><span class="line">       <span class="keyword">if</span> <span class="symbol">cookies:</span></span><br><span class="line">           request.cookies = cookies</span><br><span class="line">           <span class="keyword">self</span>.logger.debug(<span class="string">' 使用 Cookies '</span> + json.dumps(cookies))</span><br><span class="line"></span><br><span class="line">   @classmethod</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span></span><span class="symbol">:</span></span><br><span class="line">       settings = crawler.settings</span><br><span class="line">       <span class="keyword">return</span> cls(cookies_url=settings.get(<span class="string">'COOKIES_URL'</span>)</span><br><span class="line">       )</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们首先利用 from_crawler() 方法获取了 COOKIES_URL 变量，它定义在 settings.py 里，这就是刚才我们所说的接口。接下来实现 get_random_cookies() 方法，这个方法主要就是请求此 Cookies 池接口并获取接口返回的随机 Cookies。如果成功获取，则返回 Cookies；否则返回 False。 接下来，在 process_request() 方法里，我们给 request 对象的 cookies 属性赋值，其值就是获取的随机 Cookies，这样我们就成功地为每一次请求赋值 Cookies 了。 如果启用了该 Middleware，每个请求都会被赋值随机的 Cookies。这样我们就可以模拟登录之后的请求，403 状态码基本就不会出现。</p>
                  <h3 id="11-代理池对接"><a href="#11-代理池对接" class="headerlink" title="11. 代理池对接"></a>11. 代理池对接</h3>
                  <p>微博还有一个反爬措施就是，检测到同一 IP 请求量过大时就会出现 414 状态码。如果遇到这样的情况可以切换代理。例如，在本地 5555 端口运行，获取随机可用代理的地址为：<a href="http://localhost:5555/random" target="_blank" rel="noopener">http://localhost:5555/random</a>，访问这个接口即可获取一个随机可用代理。接下来我们再实现一个 Middleware，代码如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">class ProxyMiddleware():</span><br><span class="line">    def __init__(self, proxy_url):</span><br><span class="line">        self.logger = logging.getLogger(__name__)</span><br><span class="line">        self.proxy_url = proxy_url</span><br><span class="line"></span><br><span class="line">    def get_random_proxy(self):</span><br><span class="line">        try:</span><br><span class="line">            response = requests.<span class="builtin-name">get</span>(self.proxy_url)</span><br><span class="line">            <span class="keyword">if</span> response.status_code == 200:</span><br><span class="line">               <span class="built_in"> proxy </span>= response.text</span><br><span class="line">                return proxy</span><br><span class="line">        except requests.ConnectionError:</span><br><span class="line">            return <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        <span class="keyword">if</span> request.meta.<span class="builtin-name">get</span>(<span class="string">'retry_times'</span>):</span><br><span class="line">           <span class="built_in"> proxy </span>= self.get_random_proxy()</span><br><span class="line">            <span class="keyword">if</span> proxy:</span><br><span class="line">                uri = <span class="string">'https://&#123;proxy&#125;'</span>.format(<span class="attribute">proxy</span>=proxy)</span><br><span class="line">                self.logger.<span class="builtin-name">debug</span>(<span class="string">' 使用代理 '</span> + proxy)</span><br><span class="line">                request.meta[<span class="string">'proxy'</span>] = uri</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">       <span class="built_in"> settings </span>= crawler.settings</span><br><span class="line">        return cls(<span class="attribute">proxy_url</span>=settings.get('PROXY_URL')</span><br><span class="line">        )</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>同样的原理，我们实现了一个 get_random_proxy() 方法用于请求代理池的接口获取随机代理。如果获取成功，则返回改代理，否则返回 False。在 process_request() 方法中，我们给 request 对象的 meta 属性赋值一个 proxy 字段，该字段的值就是代理。 另外，赋值代理的判断条件是当前 retry_times 不为空，也就是说第一次请求失败之后才启用代理，因为使用代理后访问速度会慢一些。所以我们在这里设置了只有重试的时候才启用代理，否则直接请求。这样就可以保证在没有被封禁的情况下直接爬取，保证了爬取速度。</p>
                  <h3 id="12-启用-Middleware"><a href="#12-启用-Middleware" class="headerlink" title="12. 启用 Middleware"></a>12. 启用 Middleware</h3>
                  <p>接下来，我们在配置文件中启用这两个 Middleware，修改 settings.py 如下所示：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">DOWNLOADER_MIDDLEWARES</span> <span class="string">=</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'weibo.middlewares.CookiesMiddleware':</span> <span class="number">554</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'weibo.middlewares.ProxyMiddleware':</span> <span class="number">555</span><span class="string">,</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>注意这里的优先级设置，前文提到了 Scrapy 的默认 Downloader Middleware 的设置如下：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware':</span> <span class="number">300</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware':</span> <span class="number">350</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware':</span> <span class="number">400</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware':</span> <span class="number">500</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.retry.RetryMiddleware':</span> <span class="number">550</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware':</span> <span class="number">560</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware':</span> <span class="number">580</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware':</span> <span class="number">590</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.redirect.RedirectMiddleware':</span> <span class="number">600</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.cookies.CookiesMiddleware':</span> <span class="number">700</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware':</span> <span class="number">750</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.stats.DownloaderStats':</span> <span class="number">850</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware':</span> <span class="number">900</span><span class="string">,</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>要使得我们自定义的 CookiesMiddleware 生效，它在内置的 CookiesMiddleware 之前调用。内置的 CookiesMiddleware 的优先级为 700，所以这里我们设置一个比 700 小的数字即可。 要使得我们自定义的 ProxyMiddleware 生效，它在内置的 HttpProxyMiddleware 之前调用。内置的 HttpProxyMiddleware 的优先级为 750，所以这里我们设置一个比 750 小的数字即可。</p>
                  <h3 id="13-运行"><a href="#13-运行" class="headerlink" title="13. 运行"></a>13. 运行</h3>
                  <p>到此为止，整个微博爬虫就实现完毕了，我们运行如下命令启动一下爬虫：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl weibocn</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>类似的输出结果如下：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-11</span> <span class="number">17</span><span class="string">:27:34</span> <span class="string">[urllib3.connectionpool]</span> <span class="attr">DEBUG:</span> <span class="string">http://localhost:5000</span> <span class="string">"GET /weibo/random HTTP/1.1"</span> <span class="number">200</span> <span class="number">339</span></span><br><span class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-11</span> <span class="number">17</span><span class="string">:27:34</span> <span class="string">[weibo.middlewares]</span> <span class="attr">DEBUG:</span> <span class="string">使用</span> <span class="string">Cookies</span> <span class="string">&#123;"SCF":</span> <span class="string">"AhzwTr_DxIGjgri_dt46_DoPzUqq-PSupu545JdozdHYJ7HyEb4pD3pe05VpbIpVyY1ciKRRWwUgojiO3jYwlBE."</span><span class="string">,</span> <span class="attr">"_T_WM":</span> <span class="string">"8fe0bc1dad068d09b888d8177f1c1218"</span><span class="string">,</span> <span class="attr">"SSOLoginState":</span> <span class="string">"1501496388"</span><span class="string">,</span> <span class="attr">"M_WEIBOCN_PARAMS":</span> <span class="string">"uicode%3D20000174"</span><span class="string">,</span> <span class="attr">"SUHB":</span> <span class="string">"0tKqV4asxqYl4J"</span><span class="string">,</span> <span class="attr">"SUB":</span> <span class="string">"_2A250e3QUDeRhGeBM6VYX8y7NwjiIHXVXhBxcrDV6PUJbkdBeLXjckW2fUT8MWloekO4FCWVlIYJGJdGLnA.."</span><span class="string">&#125;</span></span><br><span class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-11</span> <span class="number">17</span><span class="string">:27:34</span> <span class="string">[weibocn]</span> <span class="attr">DEBUG:</span> <span class="string">&lt;200</span> <span class="string">https://m.weibo.cn/api/container/getIndex?uid=1742566624&amp;type=uid&amp;value=1742566624&amp;containerid=1005051742566624&gt;</span></span><br><span class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-11</span> <span class="number">17</span><span class="string">:27:34</span> <span class="string">[scrapy.core.scraper]</span> <span class="attr">DEBUG:</span> <span class="string">Scraped</span> <span class="string">from</span> <span class="string">&lt;200</span> <span class="string">https://m.weibo.cn/api/container/getIndex?uid=1742566624&amp;type=uid&amp;value=1742566624&amp;containerid=1005051742566624&gt;</span></span><br><span class="line"><span class="string">&#123;'avatar':</span> <span class="string">'https://tva4.sinaimg.cn/crop.0.0.180.180.180/67dd74e0jw1e8qgp5bmzyj2050050aa8.jpg'</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'cover':</span> <span class="string">'https://tva3.sinaimg.cn/crop.0.0.640.640.640/6ce2240djw1e9oaqhwllzj20hs0hsdir.jpg'</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'crawled_at':</span> <span class="string">'2017-07-11 17:27'</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'description':</span> <span class="string">' 成长，就是一个不断觉得以前的自己是个傻逼的过程 '</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'fans_count':</span> <span class="number">19202906</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'follows_count':</span> <span class="number">1599</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'gender':</span> <span class="string">'m'</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'id':</span> <span class="number">1742566624</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'name':</span> <span class="string">' 思想聚焦 '</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'verified':</span> <span class="literal">True</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'verified_reason':</span> <span class="string">' 微博知名博主，校导网编辑 '</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'verified_type':</span> <span class="number">0</span><span class="string">,</span></span><br><span class="line"> <span class="attr">'weibos_count':</span> <span class="number">58393</span><span class="string">&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行一段时间后，我们便可以到 MongoDB 数据库查看数据，爬取下来的数据如图 13-38 和图 13-39 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034947.jpg" alt=""> 图 13-38 用户信息 <img src="https://qiniu.cuiqingcai.com/2019-11-27-035001.jpg" alt=""> 图 13-39 微博信息 针对用户信息，我们不仅爬取了其基本信息，还把关注和粉丝列表加到了 follows 和 fans 字段并做了去重操作。针对微博信息，我们成功进行了时间转换处理，同时还保存了微博的图片列表信息。</p>
                  <h3 id="14-本节代码"><a href="#14-本节代码" class="headerlink" title="14. 本节代码"></a>14. 本节代码</h3>
                  <p>本节代码地址：<a href="https://github.com/Python3WebSpider/Weibo" target="_blank" rel="noopener">https://github.com/Python3WebSpider/Weibo</a>。</p>
                  <h3 id="15-结语"><a href="#15-结语" class="headerlink" title="15. 结语"></a>15. 结语</h3>
                  <p>本节实现了新浪微博的用户及其粉丝关注列表和微博信息的爬取，还对接了 Cookies 池和代理池来处理反爬虫。不过现在是针对单机的爬取，后面我们会将此项目修改为分布式爬虫，以进一步提高抓取效率。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-08 09:50:48" itemprop="dateCreated datePublished" datetime="2019-12-08T09:50:48+08:00">2019-12-08</time>
                </span>
                <span id="/8453.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.13–Scrapy 爬取新浪微博" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>17k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>15 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8448.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8448.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.12–Scrapy 对接 Docker</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-12-Scrapy-对接-Docker"><a href="#13-12-Scrapy-对接-Docker" class="headerlink" title="13.12 Scrapy 对接 Docker"></a>13.12 Scrapy 对接 Docker</h1>
                  <p>环境配置问题可能一直是我们头疼的，我们可能遇到过如下的情况：</p>
                  <ul>
                    <li>我们在本地写好了一个 Scrapy 爬虫项目，想要把它放到服务器上运行，但是服务器上没有安装 Python 环境。</li>
                    <li>别人给了我们一个 Scrapy 爬虫项目，项目中使用包的版本和我们本地环境版本不一致，无法直接运行。</li>
                    <li>我们需要同时管理不同版本的 Scrapy 项目，如早期的项目依赖于 Scrapy 0.25，现在的项目依赖于 Scrapy 1.4.0。</li>
                  </ul>
                  <p>在这些情况下，我们需要解决的就是环境的安装配置、环境的版本冲突解决等问题。 对于 Python 来说，VirtualEnv 的确可以解决版本冲突的问题。但是，VirtualEnv 不太方便做项目部署，我们还是需要安装 Python 环境， 如何解决上述问题呢？答案是用 Docker。Docker 可以提供操作系统级别的虚拟环境，一个 Docker 镜像一般都包含一个完整的操作系统，而这些系统内也有已经配置好的开发环境，如 Python 3.6 环境等。 我们可以直接使用此 Docker 的 Python 3 镜像运行一个容器，将项目直接放到容器里运行，就不用再额外配置 Python 3 环境。这样就解决了环境配置的问题。 我们也可以进一步将 Scrapy 项目制作成一个新的 Docker 镜像，镜像里只包含适用于本项目的 Python 环境。如果要部署到其他平台，只需要下载该镜像并运行就好了，因为 Docker 运行时采用虚拟环境，和宿主机是完全隔离的，所以也不需要担心环境冲突问题。 如果我们能够把 Scrapy 项目制作成一个 Docker 镜像，只要其他主机安装了 Docker，那么只要将镜像下载并运行即可，而不必再担心环境配置问题或版本冲突问题。 接下来，我们尝试把一个 Scrapy 项目制作成一个 Docker 镜像。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>我们要实现把前文 Scrapy 的入门项目打包成一个 Docker 镜像的过程。项目爬取的网址为：<a href="http://quotes.toscrape.com/" target="_blank" rel="noopener">http://quotes.toscrape.com/</a>，本章 Scrapy 入门一节已经实现了 Scrapy 对此站点的爬取过程，项目代码为：<a href="https://github.com/Python3WebSpider/ScrapyTutorial" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyTutorial</a>，如果本地不存在的话可以 Clone 下来。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保已经安装好 Docker 和 MongoDB 并可以正常运行，如果没有安装可以参考第 1 章的安装说明。</p>
                  <h3 id="3-创建-Dockerfile"><a href="#3-创建-Dockerfile" class="headerlink" title="3. 创建 Dockerfile"></a>3. 创建 Dockerfile</h3>
                  <p>首先在项目的根目录下新建一个 requirements.txt 文件，将整个项目依赖的 Python 环境包都列出来，如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy</span></span><br><span class="line"><span class="attribute">pymongo</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果库需要特定的版本，我们还可以指定版本号，如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scrapy&gt;=<span class="number">1.4</span><span class="number">.0</span></span><br><span class="line">pymongo&gt;=<span class="number">3.4</span><span class="number">.0</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在项目根目录下新建一个 Dockerfile 文件，文件不加任何后缀名，修改内容如下所示：</p>
                  <figure class="highlight dockerfile">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">FROM</span> python:<span class="number">3.6</span></span><br><span class="line"><span class="keyword">ENV</span> PATH /usr/local/bin:$PATH</span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /code</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /code</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> pip3 install -r requirements.txt</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> scrapy crawl quotes</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第一行的 FROM 代表使用的 Docker 基础镜像，在这里我们直接使用 python:3.6 的镜像，在此基础上运行 Scrapy 项目。 第二行 ENV 是环境变量设置，将 /usr/local/bin:$PATH 赋值给 PATH，即增加 /usr/local/bin 这个环境变量路径。 第三行 ADD 是将本地的代码放置到虚拟容器中。它有两个参数：第一个参数是.，代表本地当前路径；第二个参数是 /code，代表虚拟容器中的路径，也就是将本地项目所有内容放置到虚拟容器的 /code 目录下，以便于在虚拟容器中运行代码。 第四行 WORKDIR 是指定工作目录，这里将刚才添加的代码路径设成工作路径。这个路径下的目录结构和当前本地目录结构是相同的，所以我们可以直接执行库安装命令、爬虫运行命令等。 第五行 RUN 是执行某些命令来做一些环境准备工作。由于 Docker 虚拟容器内只有 Python 3 环境，而没有所需要的 Python 库，所以我们运行此命令来在虚拟容器中安装相应的 Python 库如 Scrapy，这样就可以在虚拟容器中执行 Scrapy 命令了。 第六行 CMD 是容器启动命令。在容器运行时，此命令会被执行。在这里我们直接用 scrapy crawl quotes 来启动爬虫。</p>
                  <h3 id="4-修改-MongoDB-连接"><a href="#4-修改-MongoDB-连接" class="headerlink" title="4. 修改 MongoDB 连接"></a>4. 修改 MongoDB 连接</h3>
                  <p>接下来我们需要修改 MongoDB 的连接信息。如果我们继续用 localhost 是无法找到 MongoDB 的，因为在 Docker 虚拟容器里 localhost 实际指向容器本身的运行 IP，而容器内部并没有安装 MongoDB，所以爬虫无法连接 MongoDB。 这里的 MongoDB 地址可以有如下两种选择。</p>
                  <ul>
                    <li>如果只想在本机测试，我们可以将地址修改为宿主机的 IP，也就是容器外部的本机 IP，一般是一个局域网 IP，使用 ifconfig 命令即可查看。</li>
                    <li>如果要部署到远程主机运行，一般 MongoDB 都是可公网访问的地址，修改为此地址即可。</li>
                  </ul>
                  <p>在本节中，我们的目标是将项目打包成一个镜像，让其他远程主机也可运行这个项目。所以我们直接将此处 MongoDB 地址修改为某个公网可访问的远程数据库地址，修改 MONGO_URI 如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">MONGO_URI</span> = <span class="string">'mongodb://admin:admin123@120.27.34.25:27017'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此处地址可以修改为自己的远程 MongoDB 数据库地址。 这样项目的配置就完成了。</p>
                  <h3 id="5-构建镜像"><a href="#5-构建镜像" class="headerlink" title="5. 构建镜像"></a>5. 构建镜像</h3>
                  <p>接下来我们便可以构建镜像了，执行如下命令：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">docker</span> <span class="keyword">build </span>-t quotes:latest .</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样的输出就说明镜像构建成功。这时我们查看一下构建的镜像，如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Sending build context to Docker daemon <span class="number">191.5</span> kB</span><br><span class="line">Step <span class="number">1</span>/<span class="number">6</span> : FROM python:<span class="number">3.6</span></span><br><span class="line"> ---&gt; <span class="number">968120</span>d8cbe8</span><br><span class="line">Step <span class="number">2</span>/<span class="number">6</span> : ENV PATH /usr/local/bin:$PATH</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; <span class="number">387</span>abbba1189</span><br><span class="line">Step <span class="number">3</span>/<span class="number">6</span> : ADD . /code</span><br><span class="line"> ---&gt; a844ee0db9c6</span><br><span class="line">Removing <span class="built_in">int</span>ermediate container <span class="number">4</span>dc41779c573</span><br><span class="line">Step <span class="number">4</span>/<span class="number">6</span> : WORKDIR /code</span><br><span class="line"> ---&gt; <span class="number">619</span>b2c064ae9</span><br><span class="line">Removing <span class="built_in">int</span>ermediate container bcd7cd7f7337</span><br><span class="line">Step <span class="number">5</span>/<span class="number">6</span> : RUN pip3 install -r requirements.txt</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> <span class="number">9452</span>c83a12c5</span><br><span class="line">...</span><br><span class="line">Removing <span class="built_in">int</span>ermediate container <span class="number">9452</span>c83a12c5</span><br><span class="line">Step <span class="number">6</span>/<span class="number">6</span> : CMD scrapy crawl quotes</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> c092b5557ab8</span><br><span class="line"> ---&gt; c8101aca6e2a</span><br><span class="line">Removing <span class="built_in">int</span>ermediate container c092b5557ab8</span><br><span class="line">Successfully built c8101aca6e2a</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>出现类似输出就证明镜像构建成功了，这时执行如我们查看一下构建的镜像：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">docker images</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>返回结果中其中有一行就是：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">quotes  latest  <span class="number">41</span>c8499ce210    <span class="number">2</span> minutes ago   <span class="number">769</span> MB</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这就是我们新构建的镜像。</p>
                  <h3 id="6-运行"><a href="#6-运行" class="headerlink" title="6. 运行"></a>6. 运行</h3>
                  <p>我们可以先在本地测试运行，执行如下命令：</p>
                  <figure class="highlight dockerfile">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker <span class="keyword">run</span><span class="bash"> quotes</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就利用此镜像新建并运行了一个 Docker 容器，运行效果完全一致，如图 13-29 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034816.jpg" alt=""> 图 13-32 运行结果 如果出现类似图 13-29 的运行结果，这就证明构建的镜像没有问题。</p>
                  <h3 id="7-推送至-Docker-Hub"><a href="#7-推送至-Docker-Hub" class="headerlink" title="7. 推送至 Docker Hub"></a>7. 推送至 Docker Hub</h3>
                  <p>构建完成之后，我们可以将镜像 Push 到 Docker 镜像托管平台，如 Docker Hub 或者私有的 Docker Registry 等，这样我们就可以从远程服务器下拉镜像并运行了。 以 Docker Hub 为例，如果项目包含一些私有的连接信息（如数据库），我们最好将 Repository 设为私有或者直接放到私有的 Docker Registry。 首先在 <a href="https://hub.docker.com" target="_blank" rel="noopener">https://hub.docker.com</a> 注册一个账号，新建一个 Repository，名为 quotes。比如，我的用户名为 germey，新建的 Repository 名为 quotes，那么此 Repository 的地址就可以用 germey/quotes 来表示。 为新建的镜像打一个标签，命令如下所示：</p>
                  <figure class="highlight crmsh">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker <span class="keyword">tag</span> <span class="title">quotes</span>:latest germey/quotes:latest</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>推送镜像到 Docker Hub 即可，命令如下所示：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">docker</span> <span class="keyword">push </span>germey/quotes</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>Docker Hub 便会出现新推送的 Docker 镜像了，如图 13-30 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034823.png" alt=""> 图 13-30 推送结果 如果我们想在其他的主机上运行这个镜像，主机上装好 Docker 后，可以直接执行如下命令：</p>
                  <figure class="highlight dockerfile">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">docker <span class="keyword">run</span><span class="bash"> germey/quotes</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样就会自动下载镜像，然后启动容器运行，不需要配置 Python 环境，不需要关心版本冲突问题。 运行效果如图 13-31 所示： <img src="https://qiniu.cuiqingcai.com/2019-11-27-034829.jpg" alt=""> 图 13-31 运行效果 整个项目爬取完成后，数据就可以存储到指定的数据库中。</p>
                  <h3 id="8-结语"><a href="#8-结语" class="headerlink" title="8. 结语"></a>8. 结语</h3>
                  <p>我们讲解了将 Scrapy 项目制作成 Docker 镜像并部署到远程服务器运行的过程。使用此种方式，我们在本节开头所列出的问题都迎刃而解。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-07 10:23:45" itemprop="dateCreated datePublished" datetime="2019-12-07T10:23:45+08:00">2019-12-07</time>
                </span>
                <span id="/8448.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.12–Scrapy 对接 Docker" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3.9k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>4 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8445.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8445.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.11–Scrapyrt 的使用</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-11-Scrapyrt-的使用"><a href="#13-11-Scrapyrt-的使用" class="headerlink" title="13.11 Scrapyrt 的使用"></a>13.11 Scrapyrt 的使用</h1>
                  <p>Scrapyrt 为 Scrapy 提供了一个调度的 HTTP 接口。有了它我们不需要再执行 Scrapy 命令，而是通过请求一个 HTTP 接口即可调度 Scrapy 任务，我们就不需要借助于命令行来启动项目了。如果项目是在远程服务器运行，利用它来启动项目是个不错的选择。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>我们以本章 Scrapy 入门项目为例来说明 Scrapyrt 的使用方法，项目源代码地址为：<a href="https://github.com/Python3WebSpider/ScrapyTutorial" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyTutorial</a>。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保 Scrapyrt 已经正确安装并正常运行，具体安装可以参考第 1 章的说明。</p>
                  <h3 id="3-启动服务"><a href="#3-启动服务" class="headerlink" title="3. 启动服务"></a>3. 启动服务</h3>
                  <p>首先将项目下载下来，在项目目录下运行 Scrapyrt，假设当前服务运行在 9080 端口上。下面将简单介绍 Scrapyrt 的使用方法。</p>
                  <h3 id="4-GET-请求"><a href="#4-GET-请求" class="headerlink" title="4. GET 请求"></a>4. GET 请求</h3>
                  <p>目前，GET 请求方式支持如下的参数。</p>
                  <ul>
                    <li>spider_name，Spider 名称，字符串类型，必传参数，如果传递的 Spider 名称不存在则会返回 404 错误。</li>
                    <li>url，爬取链接，字符串类型，如果起始链接没有定义的话就必须要传递，如果传递了该参数，Scrapy 会直接用该 URL 生成 Request，而直接忽略 start_requests() 方法和 start_urls 属性的定义。</li>
                    <li>callback，回调函数名称，字符串类型，可选参数，如果传递了就会使用此回调函数处理，否则会默认使用 Spider 内定义的回调函数。</li>
                    <li>max_requests，最大请求数量，数值类型，可选参数，它定义了 Scrapy 执行请求的 Request 的最大限制，如定义为 5，则最多只执行 5 次 Request 请求，其余的则会被忽略。</li>
                    <li>start_requests，是否要执行 start_request() 函数，布尔类型，可选参数，在 Scrapy 项目中如果定义了 start_requests() 方法，那么在项目启动时会默认调用该方法，但是在 Scrapyrt 就不一样了，它默认不执行 start_requests() 方法，如果要执行，需要将它设置为 true。</li>
                  </ul>
                  <p>例如我们执行如下命令：</p>
                  <figure class="highlight awk">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl http:<span class="regexp">//</span>localhost:<span class="number">9080</span><span class="regexp">/crawl.json?spider_name=quotes&amp;url=http:/</span><span class="regexp">/quotes.toscrape.com/</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>得到类似如下结果，如图 13-28 所示： <img src="https://qiniu.cuiqingcai.com/2019-11-27-034631.jpg" alt=""> 图 13-28 输出结果 返回的是一个 JSON 格式的字符串，我们解析它的结构，如下所示：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"status"</span>: <span class="string">"ok"</span>,</span><br><span class="line">  <span class="attr">"items"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"text"</span>: <span class="string">"“The world as we have created it is a process of o..."</span>,</span><br><span class="line">      <span class="attr">"author"</span>: <span class="string">"Albert Einstein"</span>,</span><br><span class="line">      <span class="attr">"tags"</span>: [</span><br><span class="line">        <span class="string">"change"</span>,</span><br><span class="line">        <span class="string">"deep-thoughts"</span>,</span><br><span class="line">        <span class="string">"thinking"</span>,</span><br><span class="line">        <span class="string">"world"</span></span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    ...</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"text"</span>: <span class="string">"“... a mind needs books as a sword needs a whetsto..."</span>,</span><br><span class="line">      <span class="attr">"author"</span>: <span class="string">"George R.R. Martin"</span>,</span><br><span class="line">      <span class="attr">"tags"</span>: [</span><br><span class="line">        <span class="string">"books"</span>,</span><br><span class="line">        <span class="string">"mind"</span></span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">"items_dropped"</span>: [],</span><br><span class="line">  <span class="attr">"stats"</span>: &#123;</span><br><span class="line">    <span class="attr">"downloader/request_bytes"</span>: <span class="number">2892</span>,</span><br><span class="line">    <span class="attr">"downloader/request_count"</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="attr">"downloader/request_method_count/GET"</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="attr">"downloader/response_bytes"</span>: <span class="number">24812</span>,</span><br><span class="line">    <span class="attr">"downloader/response_count"</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="attr">"downloader/response_status_count/200"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"downloader/response_status_count/404"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">"dupefilter/filtered"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">"finish_reason"</span>: <span class="string">"finished"</span>,</span><br><span class="line">    <span class="attr">"finish_time"</span>: <span class="string">"2017-07-12 15:09:02"</span>,</span><br><span class="line">    <span class="attr">"item_scraped_count"</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="attr">"log_count/DEBUG"</span>: <span class="number">112</span>,</span><br><span class="line">    <span class="attr">"log_count/INFO"</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="attr">"memusage/max"</span>: <span class="number">52510720</span>,</span><br><span class="line">    <span class="attr">"memusage/startup"</span>: <span class="number">52510720</span>,</span><br><span class="line">    <span class="attr">"request_depth_max"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"response_received_count"</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="attr">"scheduler/dequeued"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"scheduler/dequeued/memory"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"scheduler/enqueued"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"scheduler/enqueued/memory"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">"start_time"</span>: <span class="string">"2017-07-12 15:08:56"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"spider_name"</span>: <span class="string">"quotes"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里省略了 items 绝大部分。status 显示了爬取的状态，items 部分是 Scrapy 项目的爬取结果，items_dropped 是被忽略的 Item 列表，stats 是爬取结果的统计情况。此结果和直接运行 Scrapy 项目得到的统计是相同的。 这样一来，我们就通过 HTTP 接口调度 Scrapy 项目并获取爬取结果，如果 Scrapy 项目部署在服务器上，我们可以通过开启一个 Scrapyrt 服务实现任务的调度并直接取到爬取结果，这很方便。</p>
                  <h3 id="5-POST-请求"><a href="#5-POST-请求" class="headerlink" title="5. POST 请求"></a>5. POST 请求</h3>
                  <p>除了 GET 请求，我们还可以通过 POST 请求来请求 Scrapyrt。但是此处 Request Body 必须是一个合法的 JSON 配置，在 JSON 里面可以配置相应的参数，支持的配置参数更多。 目前，JSON 配置支持如下参数。</p>
                  <ul>
                    <li><strong>spider_name</strong>：Spider 名称，字符串类型，必传参数。如果传递的 Spider 名称不存在，则返回 404 错误。</li>
                    <li><strong>max_requests</strong>：最大请求数量，数值类型，可选参数。它定义了 Scrapy 执行请求的 Request 的最大限制，如定义为 5，则表示最多只执行 5 次 Request 请求，其余的则会被忽略。</li>
                    <li><strong>request</strong>：Request 配置，JSON 对象，必传参数。通过该参数可以定义 Request 的各个参数，必须指定 url 字段来指定爬取链接，其他字段可选。</li>
                  </ul>
                  <p>我们看一个 JSON 配置实例，如下所示：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"request"</span>: &#123;</span><br><span class="line">        <span class="attr">"url"</span>: <span class="string">"http://quotes.toscrape.com/"</span>,</span><br><span class="line">        <span class="attr">"callback"</span>: <span class="string">"parse"</span>,</span><br><span class="line">        <span class="attr">"dont_filter"</span>: <span class="string">"True"</span>,</span><br><span class="line">        <span class="attr">"cookies"</span>: &#123;<span class="attr">"foo"</span>: <span class="string">"bar"</span>&#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"max_requests"</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="attr">"spider_name"</span>: <span class="string">"quotes"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们执行如下命令传递该 Json 配置并发起 POST 请求：</p>
                  <figure class="highlight awk">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">curl http:<span class="regexp">//</span>localhost:<span class="number">9080</span><span class="regexp">/crawl.json -d '&#123;"request": &#123;"url": "http:/</span><span class="regexp">/quotes.toscrape.com/</span><span class="string">", "</span>dont_filte<span class="string">r": "</span>True<span class="string">", "</span>callback<span class="string">": "</span>parse<span class="string">", "</span>cookies<span class="string">": &#123;"</span>foo<span class="string">": "</span>ba<span class="string">r"&#125;&#125;, "</span>max_requests<span class="string">": 2, "</span>spider_name<span class="string">": "</span>quotes<span class="string">"&#125;'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果和上文类似，同样是输出了爬取状态、结果、统计信息等内容。</p>
                  <h3 id="6-结语"><a href="#6-结语" class="headerlink" title="6. 结语"></a>6. 结语</h3>
                  <p>以上内容便是 Scrapyrt 的相关用法介绍。通过它，我们方便地调度 Scrapy 项目的运行并获取爬取结果。更多的使用方法可以参考官方文档：<a href="http://scrapyrt.readthedocs.io" target="_blank" rel="noopener">http://scrapyrt.readthedocs.io</a>。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-07 10:20:01" itemprop="dateCreated datePublished" datetime="2019-12-07T10:20:01+08:00">2019-12-07</time>
                </span>
                <span id="/8445.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.11–Scrapyrt 的使用" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>3.3k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>3 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8443.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 技术杂谈 <i class="label-arrow"></i>
                  </a>
                  <a href="/8443.html" class="post-title-link" itemprop="url">Nginx 反向代理返回结果为空的问题</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>最近在开发过程中遇到了这么一个问题： 现在有一个 Web 项目，前端是使用 Vue.js 开发的，整个前端需要部署到 K8S 上，后端和前端分开，同样也需要部署到 K8S 上，因此二者需要打包为 Docker 镜像。 对前端来说，打包 Docker 就遇到了一个问题：跨域访问问题。 因此一个普遍的解决方案就是使用 Nginx 做反向代理。 一般来说，我们需要在打包时配置一下 nginx.conf 文件，然后在 Dockerfile 里面指定即可。</p>
                  <h2 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h2>
                  <p>首先看下 Dockerfile：</p>
                  <figure class="highlight dockerfile">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="comment"># build stage</span></span><br><span class="line"><span class="keyword">FROM</span> node:lts-alpine as build-stage</span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /app</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> package*.json ./</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> npm install</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . .</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> npm run build</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># production stage</span></span><br><span class="line"><span class="keyword">FROM</span> nginx:lts-alpine as production-stage</span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> --from=build-stage /app/dist /usr/share/nginx/html</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> nginx.conf /etc/nginx/conf.d/</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> rm /etc/nginx/conf.d/default.conf </span></span><br><span class="line">    &amp;&amp; mv /etc/nginx/conf.d/nginx.conf /etc/nginx/conf.d/default.conf</span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">80</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"nginx"</span>, <span class="string">"-g"</span>, <span class="string">"daemon off;"</span>]</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>一般来说，对于常规的 Vue.js 前端项目，Dockerfile 就这么写就行了。 简单介绍一下：</p>
                  <ul>
                    <li>第一步，使用 Node.js 镜像，在 Node.js 环境下对项目进行编译，默认会输出到 dist 文件夹下。</li>
                    <li>第二步，使用新的 Nginx 镜像，将编译得到的前端文件拷贝到 nginx 默认 serve 的目录，然后把自定义的 nginx.conf 文件替换为 Nginx 默认的 conf 文件，运行即可。</li>
                  </ul>
                  <h2 id="反向代理"><a href="#反向代理" class="headerlink" title="反向代理"></a>反向代理</h2>
                  <p>这里比较关键的就是 nginx.conf 文件了，为了解决跨域问题，我们一般会将后端的接口进行反向代理。 一般来说，后端的 API 接口都是以 api 为开头的，所以我们需要代理 api 开头的接口地址，nginx.conf 内容一般可以这么写：</p>
                  <figure class="highlight nginx">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span>       <span class="number">80</span>;</span><br><span class="line">    <span class="attribute">server_name</span>  localhost;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">location</span> /api/ &#123;</span><br><span class="line">                <span class="attribute">proxy_pass</span> http://domain.com/api/;</span><br><span class="line">                <span class="attribute">proxy_set_header</span> X-Forwarded-Proto <span class="variable">$scheme</span>;</span><br><span class="line">                <span class="attribute">proxy_set_header</span> Host <span class="variable">$http_host</span>;</span><br><span class="line">                <span class="attribute">proxy_set_header</span> X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">location</span> / &#123;</span><br><span class="line">        <span class="attribute">root</span>   /usr/share/nginx/html;</span><br><span class="line">        <span class="attribute">index</span>  index.html index.htm;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">location</span> = /50x.html &#123;</span><br><span class="line">        <span class="attribute">root</span>   /usr/share/nginx/html;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">error_page</span>  <span class="number">404</span>              /<span class="number">404</span>.html;</span><br><span class="line">    <span class="attribute">error_page</span>   <span class="number">500</span> <span class="number">502</span> <span class="number">503</span> <span class="number">504</span>  /50x.html;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>一般来说，以上的写法是没有问题的，proxy_set_header 也把一些 Header 进行设置，转发到后端服务器。 如果你这么写，打包 Docker 之后，测试没有遇到问题，那就完事了。</p>
                  <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2>
                  <p>但我遇到了一个奇怪的问题，某个接口在请求的时候，状态码还是 200，但其返回值总是为空，即 Response Data 的内容完全为空。 但是服务器端看 Log 确实有正常返回 Response，使用 Vue 的 devServer 也是正常的，使用 Postman 来请求也是正常的，但是经过 Nginx 这么一反向代理就不行了，什么 Response 都接收不到。 部署到 Prod 环境之后，浏览器上面可以得到这么个错误：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">ERR_INCOMPLETE_CHUNKED_ENCODING</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p><img src="https://qiniu.cuiqingcai.com/2019-12-06-202934.png" alt="image-20191207042932549"> 最后经排查，发现后端接口使用时设定了 <code>Transfer-Encoding: chunked</code> 响应头：</p>
                  <figure class="highlight fortran">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="built_in">Transfer</span>-Encoding: chunked</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这是啥？这时候就需要引出 Keep-Alive 的相关问题了。</p>
                  <h2 id="什么是-Keep-Alive？"><a href="#什么是-Keep-Alive？" class="headerlink" title="什么是 Keep-Alive？"></a>什么是 Keep-Alive？</h2>
                  <p>我们知道 HTTP 协议采用「请求-应答」模式，当使用普通模式，即非 Keep-Alive 模式时，每个请求/应答客户和服务器都要新建一个连接，完成之后立即断开连接（HTTP 协议为无连接的协议）。当使用 Keep-Alive 模式（又称持久连接、连接重用）时，Keep-Alive 功能使客户端到服务器端的连接持续有效，当出现对服务器的后继请求时，Keep-Alive 功能避免了建立或者重新建立连接。</p>
                  <ul>
                    <li>HTTP 1.0 中默认是关闭 Keep-Alive 的，需要在 HTTP 头加入<code>Connection: Keep-Alive</code>，才能启用 Keep-Alive</li>
                    <li>HTTP 1.1 中默认启用 Keep-Alive，如果请求头中加入 <code>Connection: close</code>，Keep-Alive 才关闭。</li>
                  </ul>
                  <p>目前大部分浏览器都是用 HTTP 1.1 协议，也就是说默认都会发起 Keep-Alive 的连接请求了，所以是否能完成一个完整的 Keep-Alive 连接就看服务器设置情况。 启用 Keep-Alive 模式肯定更高效，性能更高。因为避免了建立/释放连接的开销。</p>
                  <h2 id="Keep-Alive-模式下如何传输数据"><a href="#Keep-Alive-模式下如何传输数据" class="headerlink" title="Keep-Alive 模式下如何传输数据"></a>Keep-Alive 模式下如何传输数据</h2>
                  <p>Keep-Alive 模式，客户端如何判断请求所得到的响应数据已经接收完成呢？或者说如何知道服务器已经发生完了数据？ 我们已经知道了，Keep-Alive 模式发送完数据，HTTP 服务器不会自动断开连接，所有不能再使用返回 EOF（-1）来判断。 那么怎么判断呢？一个是使用 Content-Length ，一个是使用 Transfer-Encoding。</p>
                  <h3 id="Content-Length"><a href="#Content-Length" class="headerlink" title="Content-Length"></a>Content-Length</h3>
                  <p>顾名思义，Conent-Length 表示实体内容长度，客户端（服务器）可以根据这个值来判断数据是否接收完成。 由于 <code>Content-Length</code> 字段必须真实反映实体长度，但实际应用中，有些时候实体长度并没那么好获得，例如实体来自于网络文件，或者由动态语言生成。这时候要想准确获取长度，只能开一个足够大的 buffer，等内容全部生成好再计算。但这样做一方面需要更大的内存开销，另一方面也会让客户端等更久。 我们在做 WEB 性能优化时，有一个重要的指标叫 TTFB（Time To First Byte），它代表的是从客户端发出请求到收到响应的第一个字节所花费的时间。大部分浏览器自带的 Network 面板都可以看到这个指标，越短的 TTFB 意味着用户可以越早看到页面内容，体验越好。可想而知，服务端为了计算响应实体长度而缓存所有内容，跟更短的 TTFB 理念背道而驰。但在 HTTP 报文中，实体一定要在头部之后，顺序不能颠倒，为此我们需要一个新的机制：不依赖头部的长度信息，也能知道实体的边界。 但是如果消息中没有 Conent-Length，那该如何来判断呢？又在什么情况下会没有 Conent-Length 呢？</p>
                  <h3 id="Transfer-Encoding"><a href="#Transfer-Encoding" class="headerlink" title="Transfer-Encoding"></a>Transfer-Encoding</h3>
                  <p>当客户端向服务器请求一个静态页面或者一张图片时，服务器可以很清楚地知道内容大小，然后通过 Content-length 消息首部字段告诉客户端需要接收多少数据。但是如果是动态页面等时，服务器是不可能预先知道内容大小，这时就可以使用 分块编码模式来传输数据了。即如果要一边产生数据，一边发给客户端，服务器就需要在请求头中使用<code>Transfer-Encoding: chunked</code> 这样的方式来代替 Content-Length，这就是分块编码。 分块编码相当简单，在头部加入 <code>Transfer-Encoding: chunked</code> 之后，就代表这个报文采用了分块编码。这时，报文中的实体需要改为用一系列分块来传输。每个分块包含十六进制的长度值和数据，长度值独占一行，长度不包括它结尾的 CRLF（rn），也不包括分块数据结尾的 CRLF。最后一个分块长度值必须为 0，对应的分块数据没有内容，表示实体结束。</p>
                  <h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2>
                  <p>那么我说了这么一大通有什么用呢？ OK，在我遇到的业务场景中，我发现服务器的响应头中就包含了<code>Transfer-Encoding: chunked</code> 这个字段。 而这个字段，在 HTTP 1.0 是不被支持的。 而 Nginx 的反向代理，默认用的就是 HTTP 1.0，那就导致了数据无法获取的问题，可以参考 Nginx 的官方文档说明：<a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass" target="_blank" rel="noopener">http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass</a>。 原文中：</p>
                  <figure class="highlight pgsql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Syntax: proxy_http_version <span class="number">1.0</span> | <span class="number">1.1</span>;</span><br><span class="line"><span class="keyword">Default</span>: proxy_http_version <span class="number">1.0</span>;</span><br><span class="line"><span class="keyword">By</span> <span class="keyword">default</span>, <span class="keyword">version</span> <span class="number">1.0</span> <span class="keyword">is</span> used. <span class="keyword">Version</span> <span class="number">1.1</span> <span class="keyword">is</span> recommended <span class="keyword">for</span> use <span class="keyword">with</span> keepalive connections <span class="keyword">and</span> NTLM authentication.</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>所以，我们如果要解决这个问题，只需要设置一下 HTTP 版本为 1.1 就好了： 修改 nginx.conf 文件如下：</p>
                  <figure class="highlight nginx">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">location</span> /api/ &#123;</span><br><span class="line">    <span class="attribute">proxy_pass</span> http://domain.com/api/;</span><br><span class="line">    <span class="attribute">proxy_http_version</span> <span class="number">1</span>.<span class="number">1</span>;</span><br><span class="line">    <span class="attribute">proxy_set_header</span> X-Forwarded-Proto <span class="variable">$scheme</span>;</span><br><span class="line">    <span class="attribute">proxy_set_header</span> Host <span class="variable">$http_host</span>;</span><br><span class="line">    <span class="attribute">proxy_set_header</span> X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里就增加了一行：</p>
                  <figure class="highlight abnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">proxy_http_version <span class="number">1.1</span><span class="comment">;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样再测试，反向代理就会支持 <code>Transfer-Encoding: chunked</code> 模式了，这也就呼应了之前在浏览器中遇到的 ERR_INCOMPLETE_CHUNKED_ENCODING 错误。 自此，问题完美解决。</p>
                  <h2 id="复盘记录"><a href="#复盘记录" class="headerlink" title="复盘记录"></a>复盘记录</h2>
                  <p>一开始本来只想简单一记录就了事的，但一边写，发现某个地方还可以展开写得更详细。 所以干脆最后我对这个问题进行了详细的复盘和记录。在写本文之前，我其实只思考到了 Keep-Alive 和 HTTP 1.1 的问题，其实我对 Transfer-Encoding 这个并没有去深入思考。在边写边总结的过程中，为了把整个脉络讲明白，我又查询了一些 Transfer-Encoding 和 Nginx 的官方文档，对这块的了解变得更加深入，相当于我在整个记录的过程中，又对整个流程梳理了一遍，同时又有额外的收获。 所以，遇到问题，深入去思考、总结和复盘，是很有帮助的，这会让我们对问题的看法和理解更加透彻。 怎么说呢？在开发过程中，难免会遇到一些奇奇怪怪的 Bug，但这其实只是技术问题，总会解决的。 但怎样在开发过程中，不断提高自己的技术能力，我觉得需要从每一个细节出发，去思考一些事情的来龙去脉。思考得越多，我们对整个事件的把握也会越清晰，以后如果再遇到类似的或者关联的事情，就会迎刃而解了。 平时我们可能很多情况下都在写业务代码，可能比较枯燥，感觉对技术没有实质性的提升，但如果我们能从中提炼出一些核心的问题或解决方案，这才是能真正提高技术的时候，这才是最有价值的。</p>
                  <h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2>
                  <p>本文部分内容改写或摘自下列内容。</p>
                  <ul>
                    <li>HTTP Keep-Alive模式：<a href="https://www.cnblogs.com/skynet/archive/2010/12/11/1903347.html" target="_blank" rel="noopener">https://www.cnblogs.com/skynet/archive/2010/12/11/1903347.html</a></li>
                    <li>Nginx proxy_set_header 理解：<a href="https://www.jianshu.com/p/cc5167032525" target="_blank" rel="noopener">https://www.jianshu.com/p/cc5167032525</a></li>
                    <li>使用 Docker 打造超溜的前端环境：<a href="https://github.com/axetroy/blog/issues/178" target="_blank" rel="noopener">https://github.com/axetroy/blog/issues/178</a></li>
                    <li>HTTP 协议中的 Transfer-Encoding：<a href="https://imququ.com/post/transfer-encoding-header-in-http.html" target="_blank" rel="noopener">https://imququ.com/post/transfer-encoding-header-in-http.html</a></li>
                  </ul>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-07 06:26:39" itemprop="dateCreated datePublished" datetime="2019-12-07T06:26:39+08:00">2019-12-07</time>
                </span>
                <span id="/8443.html" class="post-meta-item leancloud_visitors" data-flag-title="Nginx 反向代理返回结果为空的问题" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>5k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>5 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8418.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> 技术杂谈 <i class="label-arrow"></i>
                  </a>
                  <a href="/8418.html" class="post-title-link" itemprop="url">阿里云服务器活动！阿里云代金券 + 1 折优惠码</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <p>阿里云作为国内最大的云服务商家，个人与企业上云都纷纷首选阿里云。但是在价格方面比整个市场有些许昂贵，让不少用户却而止步。因此<a href="https://www.xingsuyun58.com/" target="_blank" rel="noopener">星速云</a>小编呕心沥血整理阿里云最新优惠折扣【汇总篇】，让大家不用花时间到处寻找优惠信息，帮助站长、开发者和企业们上云购节省项目开支。</p>
                  <hr>
                  <h1 id="最全：阿里云最新优惠获取教程【长期有效】"><a href="#最全：阿里云最新优惠获取教程【长期有效】" class="headerlink" title="最全：阿里云最新优惠获取教程【长期有效】"></a>最全：阿里云最新优惠获取教程【长期有效】</h1>
                  <hr>
                  <h2 id="①：阿里云代金券2000元红包"><a href="#①：阿里云代金券2000元红包" class="headerlink" title="①：阿里云代金券2000元红包"></a>①：阿里云代金券2000元红包</h2>
                  <p>阿里云代金券领取很简单，点击下面链接进行领取。 <a href="https://www.xingsuyun58.com/wp-content/uploads/2019/11/1575037236.png" target="_blank" rel="noopener"><img src="https://www.xingsuyun58.com/wp-content/uploads/2019/11/1575037236.png" alt="阿里云代金券领取和使用步骤教程"></a> 阿里云代金券领取地址：<a href="https://promotion.aliyun.com/ntms/yunparter/invite.html?userCode=ahxhg8oc" target="_blank" rel="noopener">点击领取2000元代金券礼包</a> 点击“立即领取”按钮就可以一键领取到所有满减代金券，最高2000元。别忘记通过购物车一键批量购买哟！</p>
                  <h2 id="②：阿里云9折优惠码"><a href="#②：阿里云9折优惠码" class="headerlink" title="②：阿里云9折优惠码"></a>②：阿里云9折优惠码</h2>
                  <p>新用户还可以使用手机扫码领取一个阿里云9折折扣码叠加上述阿里云代金券使用。该9折码只能通过阿里云手机客户端扫描领取，PC端无法领取，（限ECS首购并且优惠高于7折才可以使用，比如优惠已经为5折，则该折扣码无效） <a href="https://www.xingsuyun58.com/wp-content/uploads/2019/11/15750316201.png" target="_blank" rel="noopener"><img src="https://www.xingsuyun58.com/wp-content/uploads/2019/11/15750316201.png" alt="阿里云代金券"></a> 注明：阿里云9折优惠码与阿里云2000元红包可叠加优惠折扣。</p>
                  <hr>
                  <h1 id="阿里云双12期间（2019-12-3-2019-12-31）最新优惠活动"><a href="#阿里云双12期间（2019-12-3-2019-12-31）最新优惠活动" class="headerlink" title="阿里云双12期间（2019.12.3-2019.12.31）最新优惠活动"></a>阿里云双12期间（2019.12.3-2019.12.31）最新优惠活动</h1>
                  <hr>
                  <p>阿里云双12优惠活动终于开启了，新用户1折甩卖，老用户五折，还可以领取2000元红包，优惠力度不亚于双11优惠活动哟！还不赶紧上云呢？错过双11优惠活动，那么双12不容错过了！ <a href="https://www.aliyun.com/minisite/goods?userCode=ahxhg8oc&amp;share_source=copy_link" target="_blank" rel="noopener"><img src="https://www.xingsuyun58.com/wp-content/uploads/2019/12/aliyun12pintuan.jpg" alt="阿里云双12活动"></a></p>
                  <h2 id="什么？您还不知道云服务器用途"><a href="#什么？您还不知道云服务器用途" class="headerlink" title="什么？您还不知道云服务器用途"></a>什么？您还不知道云服务器用途</h2>
                  <p>不管是做web网站、APP程序后端部署、应用程序后端、小程序后端等，还是打算创业的小伙伴，或者传统IDC自建机房的企业，上云已成为趋势。云服务器更便捷省心、节约IT运维的成本。</p>
                  <h2 id="新用户1折优惠售卖："><a href="#新用户1折优惠售卖：" class="headerlink" title="新用户1折优惠售卖："></a>新用户1折优惠售卖：</h2>
                  <p>实例规格</p>
                  <p>配置</p>
                  <p>带宽</p>
                  <p>时长</p>
                  <p>价格</p>
                  <p>官网购买</p>
                  <p>ECS突发性能型t5</p>
                  <p>1核2G40G高效云盘</p>
                  <p>1M</p>
                  <p>1年</p>
                  <p>89.00元</p>
                  <p>【<a href="https://www.aliyun.com/minisite/goods?userCode=ahxhg8oc&amp;share_source=copy_link" target="_blank" rel="noopener">立即抢购</a>】</p>
                  <p>ECS突发性能型t5</p>
                  <p>1核2G40G高效云盘</p>
                  <p>1M</p>
                  <p>3年</p>
                  <p>229.00元</p>
                  <p>ECS共享型n4</p>
                  <p>2核4G40G高效云盘</p>
                  <p>3M</p>
                  <p>2年</p>
                  <p>469.00元</p>
                  <p>ECS突发性能t5</p>
                  <p>2核4G40G高效云盘</p>
                  <p>5M</p>
                  <p>3年</p>
                  <p>899.00元</p>
                  <p>ECS突发性能t5</p>
                  <p>2核4G40G高效云盘</p>
                  <p>3M</p>
                  <p>3年</p>
                  <p>639.00元</p>
                  <p>ECS共享型n4</p>
                  <p>2核4G40G高效云盘</p>
                  <p>3M</p>
                  <p>3年</p>
                  <p>799.00元</p>
                  <p>ECS共享通用型mn4</p>
                  <p>2核8G40G高效云盘</p>
                  <p>5M</p>
                  <p>3年</p>
                  <p>1399.00元</p>
                  <p>ECS突发性能t5（香港）</p>
                  <p>1核1G40G高效云盘</p>
                  <p>1M</p>
                  <p>1年</p>
                  <p>119.00元</p>
                  <p>ECS网络增强型sn1ne</p>
                  <p>4核8G40G高效云盘</p>
                  <p>5M</p>
                  <p>3年</p>
                  <p>5621.00元</p>
                  <p>8核16G40G高效云盘</p>
                  <p>8M</p>
                  <p>3年</p>
                  <p>12209.00元</p>
                  <hr>
                  <p>注明：突发性t5实例，别看到价格比较便宜就直接购买，里面很多套路，购买页面有提示：限制20%性能基线。释义：依靠CPU 积分来提升 CPU 性能，满足业务需求。当实例实际工作性能高于基准 CPU 计算性能时，会把服务器 CPU 的性能限制在 20%以下，如果这时20%CPU性能满足不了业务需求，云服务器CPU会跑满100%，到那时候你以为是被某大佬攻击了，很有可能是你突发性t5实例CPU 积分消耗完了。笔者建议：如果用户业务对 CPU 要求高的，可以直接略过，选择t5实例（无限制CPU性能）、n4共享型、通用型mn4。以下笔者建议爆款：</p>
                  <h2 id="个人博客与企业微服务首选"><a href="#个人博客与企业微服务首选" class="headerlink" title="个人博客与企业微服务首选"></a>个人博客与企业微服务首选</h2>
                  <p><a href="https://www.aliyun.com/minisite/goods?userCode=ahxhg8oc&amp;share_source=copy_link" target="_blank" rel="noopener"><img src="https://www.xingsuyun58.com/wp-content/uploads/2019/12/15754425651.png" alt="阿里云双12云服务器爆款"></a></p>
                  <h2 id="老用户五折优惠甩卖："><a href="#老用户五折优惠甩卖：" class="headerlink" title="老用户五折优惠甩卖："></a>老用户五折优惠甩卖：</h2>
                  <p>实例规格</p>
                  <p>CPU/内存/云盘</p>
                  <p>带宽</p>
                  <p>时长</p>
                  <p>价格</p>
                  <p>老用户优惠购买</p>
                  <p>云服务器计算型ic5</p>
                  <p>8核8G40G高效云盘</p>
                  <p>1M</p>
                  <p>1年</p>
                  <p>4433.94元</p>
                  <p>【<a href="https://www.aliyun.com/minisite/goods?userCode=ahxhg8oc&amp;share_source=copy_link" target="_blank" rel="noopener">立即抢购</a>】</p>
                  <p>计算网络增强型sn1ne</p>
                  <p>8核16G40G高效云盘</p>
                  <p>1M</p>
                  <p>1年</p>
                  <p>3751.20元</p>
                  <p>通用网络增强型sn2ne</p>
                  <p>8核32G40G高效云盘</p>
                  <p>1M</p>
                  <p>1年</p>
                  <p>5353.20元</p>
                  <p>内存网络增强型se1ne</p>
                  <p>8核64G40G高效云盘</p>
                  <p>1M</p>
                  <p>1年</p>
                  <p>6793.20元</p>
                  <blockquote>
                    <p>注明：本文为星速云原创版权所有，禁止转载，一经发现将追究版权责任！</p>
                  </blockquote>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-06 18:43:17" itemprop="dateCreated datePublished" datetime="2019-12-06T18:43:17+08:00">2019-12-06</time>
                </span>
                <span id="/8418.html" class="post-meta-item leancloud_visitors" data-flag-title="阿里云服务器活动！阿里云代金券 + 1 折优惠码" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>1.5k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>1 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8413.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8413.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.10–Scrapy 通用爬虫</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-10-Scrapy-通用爬虫"><a href="#13-10-Scrapy-通用爬虫" class="headerlink" title="13.10 Scrapy 通用爬虫"></a>13.10 Scrapy 通用爬虫</h1>
                  <p>通过 Scrapy，我们可以轻松地完成一个站点爬虫的编写。但如果抓取的站点量非常大，比如爬取各大媒体的新闻信息，多个 Spider 则可能包含很多重复代码。 如果我们将各个站点的 Spider 的公共部分保留下来，不同的部分提取出来作为单独的配置，如爬取规则、页面解析方式等抽离出来做成一个配置文件，那么我们在新增一个爬虫的时候，只需要实现这些网站的爬取规则和提取规则即可。 本节我们就来探究一下 Scrapy 通用爬虫的实现方法。</p>
                  <h3 id="1-CrawlSpider"><a href="#1-CrawlSpider" class="headerlink" title="1. CrawlSpider"></a>1. CrawlSpider</h3>
                  <p>在实现通用爬虫之前我们需要先了解一下 CrawlSpider，其官方文档链接为：<a href="http://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider" target="_blank" rel="noopener">http://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider</a>。 CrawlSpider 是 Scrapy 提供的一个通用 Spider。在 Spider 里，我们可以指定一些爬取规则来实现页面的提取，这些爬取规则由一个专门的数据结构 Rule 表示。Rule 里包含提取和跟进页面的配置，Spider 会根据 Rule 来确定当前页面中的哪些链接需要继续爬取、哪些页面的爬取结果需要用哪个方法解析等。 CrawlSpider 继承自 Spider 类。除了 Spider 类的所有方法和属性，它还提供了一个非常重要的属性和方法。</p>
                  <ul>
                    <li>rules，它是爬取规则属性，是包含一个或多个 Rule 对象的列表。每个 Rule 对爬取网站的动作都做了定义，CrawlSpider 会读取 rules 的每一个 Rule 并进行解析。</li>
                    <li>parse_start_url()，它是一个可重写的方法。当 start_urls 里对应的 Request 得到 Response 时，该方法被调用，它会分析 Response 并必须返回 Item 对象或者 Request 对象。</li>
                  </ul>
                  <p>这里最重要的内容莫过于 Rule 的定义了，它的定义和参数如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">class scrapy.contrib.spiders.Rule(link_extractor, <span class="attribute">callback</span>=None, <span class="attribute">cb_kwargs</span>=None, <span class="attribute">follow</span>=None, <span class="attribute">process_links</span>=None, <span class="attribute">process_request</span>=None)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>下面对其参数依次说明：</p>
                  <ul>
                    <li>link_extractor，是一个 Link Extractor 对象。通过它，Spider 可以知道从爬取的页面中提取哪些链接。提取出的链接会自动生成 Request。它又是一个数据结构，一般常用 LxmlLinkExtractor 对象作为参数，其定义和参数如下所示：</li>
                  </ul>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">class scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), <span class="attribute">deny_extensions</span>=None, restrict_xpaths=(), restrict_css=(), tags=(<span class="string">'a'</span>, <span class="string">'area'</span>), attrs=(<span class="string">'href'</span>,), <span class="attribute">canonicalize</span>=<span class="literal">False</span>, <span class="attribute">unique</span>=<span class="literal">True</span>, <span class="attribute">process_value</span>=None, <span class="attribute">strip</span>=<span class="literal">True</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>allow 是一个正则表达式或正则表达式列表，它定义了从当前页面提取出的链接哪些是符合要求的，只有符合要求的链接才会被跟进。deny 则相反。allow_domains 定义了符合要求的域名，只有此域名的链接才会被跟进生成新的 Request，它相当于域名白名单。deny_domains 则相反，相当于域名黑名单。restrict_xpaths 定义了从当前页面中 XPath 匹配的区域提取链接，其值是 XPath 表达式或 XPath 表达式列表。restrict_css 定义了从当前页面中 CSS 选择器匹配的区域提取链接，其值是 CSS 选择器或 CSS 选择器列表。还有一些其他参数代表了提取链接的标签、是否去重、链接的处理等内容，使用的频率不高。可以参考文档的参数说明：<a href="http://scrapy.readthedocs.io/en/latest/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml" target="_blank" rel="noopener">http://scrapy.readthedocs.io/en/latest/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml</a>。</p>
                  <ul>
                    <li>callback，即回调函数，和之前定义 Request 的 callback 有相同的意义。每次从 link_extractor 中获取到链接时，该函数将会调用。该回调函数接收一个 response 作为其第一个参数，并返回一个包含 Item 或 Request 对象的列表。注意，避免使用 parse() 作为回调函数。由于 CrawlSpider 使用 parse() 方法来实现其逻辑，如果 parse() 方法覆盖了，CrawlSpider 将会运行失败。</li>
                    <li>cb_kwargs，字典，它包含传递给回调函数的参数。</li>
                    <li>follow，布尔值，即 True 或 False，它指定根据该规则从 response 提取的链接是否需要跟进。如果 callback 参数为 None，follow 默认设置为 True，否则默认为 False。</li>
                    <li>process_links，指定处理函数，从 link_extractor 中获取到链接列表时，该函数将会调用，它主要用于过滤。</li>
                    <li>process_request，同样是指定处理函数，根据该 Rule 提取到每个 Request 时，该函数都会调用，对 Request 进行处理。该函数必须返回 Request 或者 None。</li>
                  </ul>
                  <p>以上内容便是 CrawlSpider 中的核心 Rule 的基本用法。但这些内容可能还不足以完成一个 CrawlSpider 爬虫。下面我们利用 CrawlSpider 实现新闻网站的爬取实例，来更好地理解 Rule 的用法。</p>
                  <h3 id="2-Item-Loader"><a href="#2-Item-Loader" class="headerlink" title="2. Item Loader"></a>2. Item Loader</h3>
                  <p>我们了解了利用 CrawlSpider 的 Rule 来定义页面的爬取逻辑，这是可配置化的一部分内容。但是，Rule 并没有对 Item 的提取方式做规则定义。对于 Item 的提取，我们需要借助另一个模块 Item Loader 来实现。 Item Loader 提供一种便捷的机制来帮助我们方便地提取 Item。它提供的一系列 API 可以分析原始数据对 Item 进行赋值。Item 提供的是保存抓取数据的容器，而 Item Loader 提供的是填充容器的机制。有了它，数据的提取会变得更加规则化。 Item Loader 的 API 如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">class</span> scrapy.loader.<span class="constructor">ItemLoader([<span class="params">item</span>, <span class="params">selector</span>, <span class="params">response</span>,] <span class="operator">**</span><span class="params">kwargs</span>)</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>Item Loader 的 API 返回一个新的 Item Loader 来填充给定的 Item。如果没有给出 Item，则使用 default_item_class 中的类自动实例化。另外，它传入 selector 和 response 参数来使用选择器或响应参数实例化。 下面将依次说明 Item Loader 的 API 参数。</p>
                  <ul>
                    <li>item，Item 对象，可以调用 add_xpath()、add_css() 或 add_value() 等方法来填充 Item 对象。</li>
                    <li>selector，Selector 对象，用来提取填充数据的选择器。</li>
                    <li>response，Response 对象，用于使用构造选择器的 Response。</li>
                  </ul>
                  <p>一个比较典型的 Item Loader 实例如下：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy.loader import ItemLoader</span><br><span class="line">from project.items import Product</span><br><span class="line"></span><br><span class="line">def parse(self, response):</span><br><span class="line">    loader = <span class="constructor">ItemLoader(<span class="params">item</span>=Product()</span>, response=response)</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">name</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">class</span>=<span class="string">"product_name"</span>]')</span></span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">name</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">class</span>=<span class="string">"product_title"</span>]')</span></span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">price</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">p</span>[@<span class="params">id</span>=<span class="string">"price"</span>]')</span></span><br><span class="line">    loader.add<span class="constructor">_css('<span class="params">stock</span>', '<span class="params">p</span>#<span class="params">stock</span>]')</span></span><br><span class="line">    loader.add<span class="constructor">_value('<span class="params">last_updated</span>', '<span class="params">today</span>')</span></span><br><span class="line">    return loader.load<span class="constructor">_item()</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先声明一个 Product Item，用该 Item 和 Response 对象实例化 ItemLoader，调用 add_xpath() 方法把来自两个不同位置的数据提取出来，分配给 name 属性，再用 add_xpath()、add_css()、add_value() 等方法对不同属性依次赋值，最后调用 load_item() 方法实现 Item 的解析。这种方式比较规则化，我们可以把一些参数和规则单独提取出来做成配置文件或存到数据库，即可实现可配置化。 另外，Item Loader 每个字段中都包含了一个 Input Processor（输入处理器）和一个 Output Processor（输出处理器）。Input Processor 收到数据时立刻提取数据，Input Processor 的结果被收集起来并且保存在 ItemLoader 内，但是不分配给 Item。收集到所有的数据后，load_item() 方法被调用来填充再生成 Item 对象。在调用时会先调用 Output Processor 来处理之前收集到的数据，然后再存入 Item 中，这样就生成了 Item。 下面将介绍一些内置的 Processor。</p>
                  <h4 id="Identity"><a href="#Identity" class="headerlink" title="Identity"></a>Identity</h4>
                  <p>Identity 是最简单的 Processor，不进行任何处理，直接返回原来的数据。</p>
                  <h4 id="TakeFirst"><a href="#TakeFirst" class="headerlink" title="TakeFirst"></a>TakeFirst</h4>
                  <p>TakeFirst 返回列表的第一个非空值，类似 extract_first() 的功能，常用作 Output Processor，如下所示：</p>
                  <figure class="highlight stylus">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy<span class="selector-class">.loader</span><span class="selector-class">.processors</span> import TakeFirst</span><br><span class="line">processor = TakeFirst()</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(processor([<span class="string">''</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span></span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>输出结果如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="number">1</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>经过此 Processor 处理后的结果返回了第一个不为空的值。</p>
                  <h4 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h4>
                  <p>Join 方法相当于字符串的 join() 方法，可以把列表拼合成字符串，字符串默认使用空格分隔，如下所示：</p>
                  <figure class="highlight gradle">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> <span class="keyword">Join</span></span><br><span class="line">processor = <span class="keyword">Join</span>()</span><br><span class="line"><span class="keyword">print</span>(processor([<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>输出结果如下所示：</p>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="literal">one</span> <span class="literal">two</span> <span class="literal">three</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>它也可以通过参数更改默认的分隔符，例如改成逗号：</p>
                  <figure class="highlight gradle">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> <span class="keyword">Join</span></span><br><span class="line">processor = <span class="keyword">Join</span>(<span class="string">','</span>)</span><br><span class="line"><span class="keyword">print</span>(processor([<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下所示：</p>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="literal">one</span>,<span class="literal">two</span>,<span class="literal">three</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="Compose"><a href="#Compose" class="headerlink" title="Compose"></a>Compose</h4>
                  <p>Compose 是用给定的多个函数的组合而构造的 Processor，每个输入值被传递到第一个函数，其输出再传递到第二个函数，依次类推，直到最后一个函数返回整个处理器的输出，如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy.loader.processors import Compose</span><br><span class="line">processor = <span class="constructor">Compose(<span class="params">str</span>.<span class="params">upper</span>, <span class="params">lambda</span> <span class="params">s</span>: <span class="params">s</span>.<span class="params">strip</span>()</span>)</span><br><span class="line">print(processor(' hello world'))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">HELLO WORLD</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们构造了一个 Compose Processor，传入一个开头带有空格的字符串。Compose Processor 的参数有两个：第一个是 str.upper，它可以将字母全部转为大写；第二个是一个匿名函数，它调用 strip() 方法去除头尾空白字符。Compose 会顺次调用两个参数，最后返回结果的字符串全部转化为大写并且去除了开头的空格。</p>
                  <h4 id="MapCompose"><a href="#MapCompose" class="headerlink" title="MapCompose"></a>MapCompose</h4>
                  <p>与 Compose 类似，MapCompose 可以迭代处理一个列表输入值，如下所示：</p>
                  <figure class="highlight stylus">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy<span class="selector-class">.loader</span><span class="selector-class">.processors</span> import MapCompose</span><br><span class="line">processor = MapCompose(str<span class="selector-class">.upper</span>, lambda s: s.strip())</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(processor([<span class="string">'Hello'</span>, <span class="string">'World'</span>, <span class="string">'Python'</span>])</span></span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下所示：</p>
                  <figure class="highlight scheme">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">[<span class="symbol">'HELLO</span>', <span class="symbol">'WORLD</span>', <span class="symbol">'PYTHON</span>']</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>被处理的内容是一个可迭代对象，MapCompose 会将该对象遍历然后依次处理。</p>
                  <h4 id="SelectJmes"><a href="#SelectJmes" class="headerlink" title="SelectJmes"></a>SelectJmes</h4>
                  <p>SelectJmes 可以查询 JSON，传入 Key，返回查询所得的 Value。不过需要先安装 jmespath 库才可以使用它，命令如下所示：</p>
                  <figure class="highlight cmake">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pip3 <span class="keyword">install</span> jmespath</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>安装好 jmespath 之后，便可以使用这个 Processor 了，如下所示：</p>
                  <figure class="highlight isbl">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="variable">from</span> <span class="variable">scrapy.loader.processors</span> <span class="variable">import</span> <span class="variable">SelectJmes</span></span><br><span class="line"><span class="variable">proc</span> = <span class="function"><span class="title">SelectJmes</span>(<span class="string">'foo'</span>)</span></span><br><span class="line"><span class="variable">processor</span> = <span class="function"><span class="title">SelectJmes</span>(<span class="string">'foo'</span>)</span></span><br><span class="line"><span class="function"><span class="title">print</span>(<span class="title">processor</span>(&#123;<span class="string">'foo'</span>: <span class="string">'bar'</span>&#125;))</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">bar</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>以上内容便是一些常用的 Processor，在本节的实例中我们会使用 Processor 来进行数据的处理。 接下来，我们用一个实例来了解 Item Loader 的用法。</p>
                  <h3 id="3-本节目标"><a href="#3-本节目标" class="headerlink" title="3. 本节目标"></a>3. 本节目标</h3>
                  <p>我们以中华网科技类新闻为例，来了解 CrawlSpider 和 Item Loader 的用法，再提取其可配置信息实现可配置化。官网链接为：<a href="http://tech.china.com/。我们需要爬取它的科技类新闻内容，链接为：http://tech.china.com/articles/，页面如图" target="_blank" rel="noopener">http://tech.china.com/。我们需要爬取它的科技类新闻内容，链接为：http://tech.china.com/articles/，页面如图</a> 13-19 所示。 我们要抓取新闻列表中的所有分页的新闻详情，包括标题、正文、时间、来源等信息。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034144.png" alt=""> 图 13-19 爬取站点</p>
                  <h3 id="4-新建项目"><a href="#4-新建项目" class="headerlink" title="4. 新建项目"></a>4. 新建项目</h3>
                  <p>首先新建一个 Scrapy 项目，名为 scrapyuniversal，如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy startproject scrapyuniversal</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>创建一个 CrawlSpider，需要先制定一个模板。我们可以先看看有哪些可用模板，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy genspider -l</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如下所示：</p>
                  <figure class="highlight armasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="symbol">Available</span> templates:</span><br><span class="line">  <span class="keyword">basic</span></span><br><span class="line"><span class="keyword"> </span> crawl</span><br><span class="line">  csvfeed</span><br><span class="line">  xmlfeed</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>之前创建 Spider 的时候，我们默认使用了第一个模板 basic。这次要创建 CrawlSpider，就需要使用第二个模板 crawl，创建命令如下所示：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">genspider</span> <span class="selector-tag">-t</span> <span class="selector-tag">crawl</span> <span class="selector-tag">china</span> <span class="selector-tag">tech</span><span class="selector-class">.china</span><span class="selector-class">.com</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行之后便会生成一个 CrawlSpider，其内容如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChinaSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'china'</span></span><br><span class="line">    allowed_domains = [<span class="string">'tech.china.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://tech.china.com/'</span>]</span><br><span class="line"></span><br><span class="line">    rules = (Rule(LinkExtractor(allow=<span class="string">r'Items/'</span>), callback=<span class="string">'parse_item'</span>, follow=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        i = &#123;&#125;</span><br><span class="line">        <span class="comment">#i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()</span></span><br><span class="line">        <span class="comment">#i['name'] = response.xpath('//div[@id="name"]').extract()</span></span><br><span class="line">        <span class="comment">#i['description'] = response.xpath('//div[@id="description"]').extract()</span></span><br><span class="line">        <span class="keyword">return</span> i</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这次生成的 Spider 内容多了一个 rules 属性的定义。Rule 的第一个参数是 LinkExtractor，就是上文所说的 LxmlLinkExtractor，只是名称不同。同时，默认的回调函数也不再是 parse，而是 parse_item。</p>
                  <h3 id="5-定义-Rule"><a href="#5-定义-Rule" class="headerlink" title="5. 定义 Rule"></a>5. 定义 Rule</h3>
                  <p>要实现新闻的爬取，我们需要做的就是定义好 Rule，然后实现解析函数。下面我们就来一步步实现这个过程。 首先将 start_urls 修改为起始链接，代码如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">start_urls</span> = [<span class="string">'http://tech.china.com/articles/'</span>]</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>之后，Spider 爬取 start_urls 里面的每一个链接。所以这里第一个爬取的页面就是我们刚才所定义的链接。得到 Response 之后，Spider 就会根据每一个 Rule 来提取这个页面内的超链接，去生成进一步的 Request。接下来，我们就需要定义 Rule 来指定提取哪些链接。 当前页面如图 13-20 所示： <img src="https://qiniu.cuiqingcai.com/2019-11-27-034532.png" alt=""> 图 13-20 页面内容 这是新闻的列表页，下一步自然就是将列表中的每条新闻详情的链接提取出来。这里直接指定这些链接所在区域即可。查看源代码，所有链接都在 ID 为 left_side 的节点内，具体来说是它内部的 class 为 con_item 的节点，如图 13-21 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034527.jpg" alt=""> 图 13-21 列表源码 此处我们可以用 LinkExtractor 的 restrict_xpaths 属性来指定，之后 Spider 就会从这个区域提取所有的超链接并生成 Request。但是，每篇文章的导航中可能还有一些其他的超链接标签，我们只想把需要的新闻链接提取出来。真正的新闻链接路径都是以 article 开头的，我们用一个正则表达式将其匹配出来再赋值给 allow 参数即可。另外，这些链接对应的页面其实就是对应的新闻详情页，而我们需要解析的就是新闻的详情信息，所以此处还需要指定一个回调函数 callback。 到现在我们就可以构造出一个 Rule 了，代码如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="constructor">Rule(LinkExtractor(<span class="params">allow</span>='<span class="params">article</span><span class="operator">/</span>.<span class="operator">*</span>.<span class="params">html</span>', <span class="params">restrict_xpaths</span>='<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"left_side"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">class</span>=<span class="string">"con_item"</span>]')</span>, callback='parse_item')</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>接下来，我们还要让当前页面实现分页功能，所以还需要提取下一页的链接。分析网页源码之后可以发现下一页链接是在 ID 为 pageStyle 的节点内，如图 13-22 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034519.jpg" alt=""> 图 13-22 分页源码 但是，下一页节点和其他分页链接区分度不高，要取出此链接我们可以直接用 XPath 的文本匹配方式，所以这里我们直接用 LinkExtractor 的 restrict_xpaths 属性来指定提取的链接即可。另外，我们不需要像新闻详情页一样去提取此分页链接对应的页面详情信息，也就是不需要生成 Item，所以不需要加 callback 参数。另外这下一页的页面如果请求成功了就需要继续像上述情况一样分析，所以它还需要加一个 follow 参数为 True，代表继续跟进匹配分析。其实，follow 参数也可以不加，因为当 callback 为空的时候，follow 默认为 True。此处 Rule 定义为如下所示：</p>
                  <figure class="highlight lisp">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">Rule(<span class="name">LinkExtractor</span>(<span class="name">restrict_xpaths=</span>'//div[@id=<span class="string">"pageStyle"</span>]//a[contains(., <span class="string">"下一页"</span>)]'))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>所以现在 rules 就变成了：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">rules = (<span class="constructor">Rule(LinkExtractor(<span class="params">allow</span>='<span class="params">article</span><span class="operator">/</span>.<span class="operator">*</span>.<span class="params">html</span>', <span class="params">restrict_xpaths</span>='<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"left_side"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">class</span>=<span class="string">"con_item"</span>]')</span>, callback='parse_item'),</span><br><span class="line">    <span class="constructor">Rule(LinkExtractor(<span class="params">restrict_xpaths</span>='<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"pageStyle"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">a</span>[<span class="params">contains</span>(., <span class="string">"下一页"</span>)</span>]'))</span><br><span class="line">)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>接着我们运行一下代码，命令如下：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl china</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>现在已经实现页面的翻页和详情页的抓取了，我们仅仅通过定义了两个 Rule 即实现了这样的功能，运行效果如图 13-23 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034515.jpg" alt=""> 图 13-23 运行效果</p>
                  <h3 id="6-解析页面"><a href="#6-解析页面" class="headerlink" title="6. 解析页面"></a>6. 解析页面</h3>
                  <p>接下来我们需要做的就是解析页面内容了，将标题、发布时间、正文、来源提取出来即可。首先定义一个 Item，如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Field, Item</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="symbol">NewsItem</span>(<span class="symbol">Item</span>):</span><br><span class="line">    <span class="symbol">title</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">url</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">text</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">datetime</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">source</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">website</span> = <span class="symbol">Field</span>()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里的字段分别指新闻标题、链接、正文、发布时间、来源、站点名称，其中站点名称直接赋值为中华网。因为既然是通用爬虫，肯定还有很多爬虫也来爬取同样结构的其他站点的新闻内容，所以需要一个字段来区分一下站点名称。 详情页的预览图如图 13-24 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034510.png" alt=""> 图 13-24 详情页面 如果像之前一样提取内容，就直接调用 response 变量的 xpath()、css() 等方法即可。这里 parse_item() 方法的实现如下所示：</p>
                  <figure class="highlight xquery">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse_item(self, response):</span><br><span class="line">    <span class="type">item</span> = NewsItem()</span><br><span class="line">    <span class="type">item</span>[<span class="string">'title'</span>] = response.xpath(<span class="string">'//h1[@id="chan_newsTitle"]/text()'</span>).extract_first()</span><br><span class="line">    <span class="type">item</span>[<span class="string">'url'</span>] = response.url</span><br><span class="line">    <span class="type">item</span>[<span class="string">'text'</span>] = <span class="string">''</span>.join(response.xpath(<span class="string">'//div[@id="chan_newsDetail"]//text()'</span>).extract()).<span class="keyword">strip</span>()</span><br><span class="line">    <span class="type">item</span>[<span class="string">'datetime'</span>] = response.xpath(<span class="string">'//div[@id="chan_newsInfo"]/text()'</span>).re_first(<span class="string">'(d+-d+-d+sd+:d+:d+)'</span>)</span><br><span class="line">    <span class="type">item</span>[<span class="string">'source'</span>] = response.xpath(<span class="string">'//div[@id="chan_newsInfo"]/text()'</span>).re_first(<span class="string">' 来源：(.*)'</span>).<span class="keyword">strip</span>()</span><br><span class="line">    <span class="type">item</span>[<span class="string">'website'</span>] = <span class="string">' 中华网 '</span></span><br><span class="line">    yield <span class="type">item</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就把每条新闻的信息提取形成了一个 NewsItem 对象。 这时实际上我们就已经完成了 Item 的提取。再运行一下 Spider，如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl china</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>输出内容如图 13-25 所示： <img src="https://qiniu.cuiqingcai.com/2019-11-27-034503.jpg" alt=""> 图 13-25 输出内容 现在我们就可以成功将每条新闻的信息提取出来。 不过我们发现这种提取方式非常不规整。下面我们再用 Item Loader，通过 add_xpath()、add_css()、add_value() 等方式实现配置化提取。我们可以改写 parse_item()，如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse<span class="constructor">_item(<span class="params">self</span>, <span class="params">response</span>)</span>:</span><br><span class="line">    loader = <span class="constructor">ChinaLoader(<span class="params">item</span>=NewsItem()</span>, response=response)</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">title</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">h1</span>[@<span class="params">id</span>=<span class="string">"chan_newsTitle"</span>]<span class="operator">/</span><span class="params">text</span>()</span>')</span><br><span class="line">    loader.add<span class="constructor">_value('<span class="params">url</span>', <span class="params">response</span>.<span class="params">url</span>)</span></span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">text</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"chan_newsDetail"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">text</span>()</span>')</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">datetime</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"chan_newsInfo"</span>]<span class="operator">/</span><span class="params">text</span>()</span>', re='(d+-d+-d+sd+:d+:d+)')</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">source</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"chan_newsInfo"</span>]<span class="operator">/</span><span class="params">text</span>()</span>', re=' 来源：(.*)')</span><br><span class="line">    loader.add<span class="constructor">_value('<span class="params">website</span>', ' 中华网 ')</span></span><br><span class="line">    yield loader.load<span class="constructor">_item()</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了一个 ItemLoader 的子类，名为 ChinaLoader，其实现如下所示：</p>
                  <figure class="highlight haskell">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="title">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="title">from</span> scrapy.loader.processors <span class="keyword">import</span> TakeFirst, Join, Compose</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">NewsLoader</span>(<span class="type">ItemLoader</span>):</span></span><br><span class="line"><span class="class">    default_output_processor = <span class="type">TakeFirst</span>()</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">ChinaLoader</span>(<span class="type">NewsLoader</span>):</span></span><br><span class="line"><span class="class">    text_out = <span class="type">Compose</span>(<span class="type">Join</span>(), lambda s: s.strip())</span></span><br><span class="line"><span class="class">    source_out = <span class="type">Compose</span>(<span class="type">Join</span>(), lambda s: s.strip())</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>ChinaLoader 继承了 NewsLoader 类，其内定义了一个通用的 Out Processor 为 TakeFirst，这相当于之前所定义的 extract_first() 方法的功能。我们在 ChinaLoader 中定义了 text_out 和 source_out 字段。这里使用了一个 Compose Processor，它有两个参数：第一个参数 Join 也是一个 Processor，它可以把列表拼合成一个字符串；第二个参数是一个匿名函数，可以将字符串的头尾空白字符去掉。经过这一系列处理之后，我们就将列表形式的提取结果转化为去除头尾空白字符的字符串。 代码重新运行，提取效果是完全一样的。 至此，我们已经实现了爬虫的半通用化配置。</p>
                  <h3 id="7-通用配置抽取"><a href="#7-通用配置抽取" class="headerlink" title="7. 通用配置抽取"></a>7. 通用配置抽取</h3>
                  <p>为什么现在只做到了半通用化？如果我们需要扩展其他站点，仍然需要创建一个新的 CrawlSpider，定义这个站点的 Rule，单独实现 parse_item() 方法。还有很多代码是重复的，如 CrawlSpider 的变量、方法名几乎都是一样的。那么我们可不可以把多个类似的几个爬虫的代码共用，把完全不相同的地方抽离出来，做成可配置文件呢？ 当然可以。那我们可以抽离出哪些部分？所有的变量都可以抽取，如 name、allowed_domains、start_urls、rules 等。这些变量在 CrawlSpider 初始化的时候赋值即可。我们就可以新建一个通用的 Spider 来实现这个功能，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy genspider -t crawl universal universal</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这个全新的 Spider 名为 universal。接下来，我们将刚才所写的 Spider 内的属性抽离出来配置成一个 JSON，命名为 china.json，放到 configs 文件夹内，和 spiders 文件夹并列，代码如下所示：</p>
                  <figure class="highlight json">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"spider"</span>: <span class="string">"universal"</span>,</span><br><span class="line">  <span class="attr">"website"</span>: <span class="string">"中华网科技"</span>,</span><br><span class="line">  <span class="attr">"type"</span>: <span class="string">"新闻"</span>,</span><br><span class="line">  <span class="attr">"index"</span>: <span class="string">"http://tech.china.com/"</span>,</span><br><span class="line">  <span class="attr">"settings"</span>: &#123;<span class="attr">"USER_AGENT"</span>: <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"start_urls"</span>: [<span class="string">"http://tech.china.com/articles/"</span>],</span><br><span class="line">  <span class="attr">"allowed_domains"</span>: [<span class="string">"tech.china.com"</span>],</span><br><span class="line">  <span class="attr">"rules"</span>: <span class="string">"china"</span></span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>第一个字段 spider 即 Spider 的名称，在这里是 universal。后面是站点的描述，比如站点名称、类型、首页等。随后的 settings 是该 Spider 特有的 settings 配置，如果要覆盖全局项目，settings.py 内的配置可以单独为其配置。随后是 Spider 的一些属性，如 start_urls、allowed_domains、rules 等。rules 也可以单独定义成一个 rules.py 文件，做成配置文件，实现 Rule 的分离，如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import Rule</span><br><span class="line"></span><br><span class="line">rules = &#123;</span><br><span class="line">    'china': (<span class="constructor">Rule(LinkExtractor(<span class="params">allow</span>='<span class="params">article</span><span class="operator">/</span>.<span class="operator">*</span>.<span class="params">html</span>', <span class="params">restrict_xpaths</span>='<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"left_side"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">class</span>=<span class="string">"con_item"</span>]')</span>,</span><br><span class="line">             callback='parse_item'),</span><br><span class="line">        <span class="constructor">Rule(LinkExtractor(<span class="params">restrict_xpaths</span>='<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"pageStyle"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">a</span>[<span class="params">contains</span>(., <span class="string">"下一页"</span>)</span>]'))</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们将基本的配置抽取出来。如果要启动爬虫，只需要从该配置文件中读取然后动态加载到 Spider 中即可。所以我们需要定义一个读取该 JSON 文件的方法，如下所示：</p>
                  <figure class="highlight xl">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">from os.<span class="built_in">path</span> <span class="keyword">import</span> realpath, dirname</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">def get_config(<span class="keyword">name</span>):</span><br><span class="line">    <span class="built_in">path</span> = dirname(realpath(__file__)) + <span class="string">'/configs/'</span> + <span class="keyword">name</span> + <span class="string">'.json'</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="built_in">path</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        return json.loads(f.read())</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>定义了 get_config() 方法之后，我们只需要向其传入 JSON 配置文件的名称即可获取此 JSON 配置信息。随后我们定义入口文件 run.py，把它放在项目根目录下，它的作用是启动 Spider，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import sys</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.project import get_project_settings</span><br><span class="line"><span class="keyword">from</span> scrapyuniversal.spiders.universal import UniversalSpider</span><br><span class="line"><span class="keyword">from</span> scrapyuniversal.utils import get_config</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler import CrawlerProcess</span><br><span class="line"></span><br><span class="line">def <span class="builtin-name">run</span>():</span><br><span class="line">    name = sys.argv[1]</span><br><span class="line">    custom_settings = get_config(name)</span><br><span class="line">    # 爬取使用的 Spider 名称</span><br><span class="line">    spider = custom_settings.<span class="builtin-name">get</span>(<span class="string">'spider'</span>, <span class="string">'universal'</span>)</span><br><span class="line">    project_settings = get_project_settings()</span><br><span class="line">   <span class="built_in"> settings </span>= dict(project_settings.copy())</span><br><span class="line">    # 合并配置</span><br><span class="line">    settings.update(custom_settings.<span class="builtin-name">get</span>(<span class="string">'settings'</span>))</span><br><span class="line">    process = CrawlerProcess(settings)</span><br><span class="line">    # 启动爬虫</span><br><span class="line">    process.crawl(spider, **&#123;<span class="string">'name'</span>: name&#125;)</span><br><span class="line">    process.start()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="builtin-name">run</span>()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行入口为 run()。首先获取命令行的参数并赋值为 name，name 就是 JSON 文件的名称，其实就是要爬取的目标网站的名称。我们首先利用 get_config() 方法，传入该名称读取刚才定义的配置文件。获取爬取使用的 spider 的名称、配置文件中的 settings 配置，然后将获取到的 settings 配置和项目全局的 settings 配置做了合并。新建一个 CrawlerProcess，传入爬取使用的配置。调用 crawl() 和 start() 方法即可启动爬取。 在 universal 中，我们新建一个<strong>init</strong>() 方法，进行初始化配置，实现如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy.linkextractors import LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders import CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapyuniversal.utils import get_config</span><br><span class="line"><span class="keyword">from</span> scrapyuniversal.rules import rules</span><br><span class="line"></span><br><span class="line">class UniversalSpider(CrawlSpider):</span><br><span class="line">    name = <span class="string">'universal'</span></span><br><span class="line">    def __init__(self, name, <span class="number">*a</span>rgs, **kwargs):</span><br><span class="line">       <span class="built_in"> config </span>= get_config(name)</span><br><span class="line">        self.config = config</span><br><span class="line">        self.rules = rules.<span class="builtin-name">get</span>(config.<span class="builtin-name">get</span>(<span class="string">'rules'</span>))</span><br><span class="line">        self.start_urls = config.<span class="builtin-name">get</span>(<span class="string">'start_urls'</span>)</span><br><span class="line">        self.allowed_domains = config.<span class="builtin-name">get</span>(<span class="string">'allowed_domains'</span>)</span><br><span class="line">        super(UniversalSpider, self).__init__(<span class="number">*a</span>rgs, **kwargs)</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        i = &#123;&#125;</span><br><span class="line">        return i</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在 <strong>init</strong>() 方法中，start_urls、allowed_domains、rules 等属性被赋值。其中，rules 属性另外读取了 rules.py 的配置，这样就成功实现爬虫的基础配置。 接下来，执行如下命令运行爬虫：</p>
                  <figure class="highlight dockerfile">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">python3 <span class="keyword">run</span>.<span class="bash">py china</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>程序会首先读取 JSON 配置文件，将配置中的一些属性赋值给 Spider，然后启动爬取。运行效果完全相同，运行结果如图 13-26 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034600.jpg" alt=""> 图 13-26 运行结果 现在我们已经对 Spider 的基础属性实现了可配置化。剩下的解析部分同样需要实现可配置化，原来的解析函数如下所示：</p>
                  <figure class="highlight reasonml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse<span class="constructor">_item(<span class="params">self</span>, <span class="params">response</span>)</span>:</span><br><span class="line">    loader = <span class="constructor">ChinaLoader(<span class="params">item</span>=NewsItem()</span>, response=response)</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">title</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">h1</span>[@<span class="params">id</span>=<span class="string">"chan_newsTitle"</span>]<span class="operator">/</span><span class="params">text</span>()</span>')</span><br><span class="line">    loader.add<span class="constructor">_value('<span class="params">url</span>', <span class="params">response</span>.<span class="params">url</span>)</span></span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">text</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"chan_newsDetail"</span>]<span class="operator">/</span><span class="operator">/</span><span class="params">text</span>()</span>')</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">datetime</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"chan_newsInfo"</span>]<span class="operator">/</span><span class="params">text</span>()</span>', re='(d+-d+-d+sd+:d+:d+)')</span><br><span class="line">    loader.add<span class="constructor">_xpath('<span class="params">source</span>', '<span class="operator">/</span><span class="operator">/</span><span class="params">div</span>[@<span class="params">id</span>=<span class="string">"chan_newsInfo"</span>]<span class="operator">/</span><span class="params">text</span>()</span>', re=' 来源：(.*)')</span><br><span class="line">    loader.add<span class="constructor">_value('<span class="params">website</span>', ' 中华网 ')</span></span><br><span class="line">    yield loader.load<span class="constructor">_item()</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们需要将这些配置也抽离出来。这里的变量主要有 Item Loader 类的选用、Item 类的选用、Item Loader 方法参数的定义，我们可以在 JSON 文件中添加如下 item 的配置：</p>
                  <figure class="highlight prolog">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">"item"</span>: &#123;</span><br><span class="line">  <span class="string">"class"</span>: <span class="string">"NewsItem"</span>,</span><br><span class="line">  <span class="string">"loader"</span>: <span class="string">"ChinaLoader"</span>,</span><br><span class="line">  <span class="string">"attrs"</span>: &#123;</span><br><span class="line">    <span class="string">"title"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"xpath"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"//h1[@id='chan_newsTitle']/text()"</span>]</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"url"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"attr"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"url"</span>]</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"text"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"xpath"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"//div[@id='chan_newsDetail']//text()"</span>]</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"datetime"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"xpath"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"//div[@id='chan_newsInfo']/text()"</span>],</span><br><span class="line">        <span class="string">"re"</span>: <span class="string">"(\\d+-\\d+-\\d+\\s\\d+:\\d+:\\d+)"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"source"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"xpath"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"//div[@id='chan_newsInfo']/text()"</span>],</span><br><span class="line">        <span class="string">"re"</span>: <span class="string">"来源：(.*)"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"website"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"method"</span>: <span class="string">"value"</span>,</span><br><span class="line">        <span class="string">"args"</span>: [<span class="string">"中华网"</span>]</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里定义了 class 和 loader 属性，它们分别代表 Item 和 Item Loader 所使用的类。定义了 attrs 属性来定义每个字段的提取规则，例如，title 定义的每一项都包含一个 method 属性，它代表使用的提取方法，如 xpath 即代表调用 Item Loader 的 add_xpath() 方法。args 即参数，就是 add_xpath() 的第二个参数，即 XPath 表达式。针对 datetime 字段，我们还用了一次正则提取，所以这里还可以定义一个 re 参数来传递提取时所使用的正则表达式。 我们还要将这些配置之后动态加载到 parse_item() 方法里。最后，最重要的就是实现 parse_item() 方法，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse_item(self, response):</span><br><span class="line">   item = self.config.<span class="builtin-name">get</span>(<span class="string">'item'</span>)</span><br><span class="line">   <span class="keyword">if</span> item:</span><br><span class="line">       cls = eval(item.<span class="builtin-name">get</span>(<span class="string">'class'</span>))()</span><br><span class="line">       loader = eval(item.<span class="builtin-name">get</span>(<span class="string">'loader'</span>))(cls, <span class="attribute">response</span>=response)</span><br><span class="line">       # 动态获取属性配置</span><br><span class="line">       <span class="keyword">for</span> key, value <span class="keyword">in</span> item.<span class="builtin-name">get</span>(<span class="string">'attrs'</span>).items():</span><br><span class="line">           <span class="keyword">for</span> extractor <span class="keyword">in</span> value:</span><br><span class="line">               <span class="keyword">if</span> extractor.<span class="builtin-name">get</span>(<span class="string">'method'</span>) == <span class="string">'xpath'</span>:</span><br><span class="line">                   loader.add_xpath(key, <span class="number">*e</span>xtractor.<span class="builtin-name">get</span>(<span class="string">'args'</span>), **&#123;<span class="string">'re'</span>: extractor.<span class="builtin-name">get</span>(<span class="string">'re'</span>)&#125;)</span><br><span class="line">               <span class="keyword">if</span> extractor.<span class="builtin-name">get</span>(<span class="string">'method'</span>) == <span class="string">'css'</span>:</span><br><span class="line">                   loader.add_css(key, <span class="number">*e</span>xtractor.<span class="builtin-name">get</span>(<span class="string">'args'</span>), **&#123;<span class="string">'re'</span>: extractor.<span class="builtin-name">get</span>(<span class="string">'re'</span>)&#125;)</span><br><span class="line">               <span class="keyword">if</span> extractor.<span class="builtin-name">get</span>(<span class="string">'method'</span>) == <span class="string">'value'</span>:</span><br><span class="line">                   loader.add_value(key, <span class="number">*e</span>xtractor.<span class="builtin-name">get</span>(<span class="string">'args'</span>), **&#123;<span class="string">'re'</span>: extractor.<span class="builtin-name">get</span>(<span class="string">'re'</span>)&#125;)</span><br><span class="line">               <span class="keyword">if</span> extractor.<span class="builtin-name">get</span>(<span class="string">'method'</span>) == <span class="string">'attr'</span>:</span><br><span class="line">                   loader.add_value(key, getattr(response, <span class="number">*e</span>xtractor.<span class="builtin-name">get</span>(<span class="string">'args'</span>)))</span><br><span class="line">       yield loader.load_item()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里首先获取 Item 的配置信息，然后获取 class 的配置，将其初始化，初始化 Item Loader，遍历 Item 的各个属性依次进行提取。判断 method 字段，调用对应的处理方法进行处理。如 method 为 css，就调用 Item Loader 的 add_css() 方法进行提取。所有配置动态加载完毕之后，调用 load_item() 方法将 Item 提取出来。 重新运行程序，结果如图 13-27 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034606.jpg" alt=""> 图 13-27 运行结果 运行结果是完全相同的。 我们再回过头看一下 start_urls 的配置。这里 start_urls 只可以配置具体的链接。如果这些链接有 100 个、1000 个，我们总不能将所有的链接全部列出来吧？在某些情况下，start_urls 也需要动态配置。我们将 start_urls 分成两种，一种是直接配置 URL 列表，一种是调用方法生成，它们分别定义为 static 和 dynamic 类型。 本例中的 start_urls 很明显是 static 类型的，所以 start_urls 配置改写如下所示： ```json”start_urls”: {“type”:”static”,”value”: [“<a href="http://tech.china.com/articles/" target="_blank" rel="noopener">http://tech.china.com/articles/</a>“] }</p>
                  <figure class="highlight autohotkey">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">如果 start_urls 是动态生成的，我们可以调用方法传参数，如下所示：</span><br><span class="line">```json</span><br><span class="line"><span class="string">"start_urls"</span>: &#123;</span><br><span class="line">  <span class="string">"type"</span>: <span class="string">"dynamic"</span>,</span><br><span class="line">  <span class="string">"method"</span>: <span class="string">"china"</span>,</span><br><span class="line">  <span class="string">"args"</span>: [<span class="number">5</span>, <span class="number">10</span>]</span><br><span class="line">&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里 start_urls 定义为 dynamic 类型，指定方法为 urls_china()，然后传入参数 5 和 10，来生成第 5 到 10 页的链接。这样我们只需要实现该方法即可，统一新建一个 urls.py 文件，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def china(start, end):</span><br><span class="line">    <span class="keyword">for</span><span class="built_in"> page </span><span class="keyword">in</span> range(start, end + 1):</span><br><span class="line">        yield <span class="string">'http://tech.china.com/articles/index_'</span> + str(page) + <span class="string">'.html'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其他站点可以自行配置。如某些链接需要用到时间戳，加密参数等，均可通过自定义方法实现。 接下来在 Spider 的 <strong>init</strong>() 方法中，start_urls 的配置改写如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapyuniversal import urls</span><br><span class="line"></span><br><span class="line">start_urls = config.<span class="builtin-name">get</span>(<span class="string">'start_urls'</span>)</span><br><span class="line"><span class="keyword">if</span> start_urls:</span><br><span class="line">    <span class="keyword">if</span> start_urls.<span class="builtin-name">get</span>(<span class="string">'type'</span>) == <span class="string">'static'</span>:</span><br><span class="line">        self.start_urls = start_urls.<span class="builtin-name">get</span>(<span class="string">'value'</span>)</span><br><span class="line">    elif start_urls.<span class="builtin-name">get</span>(<span class="string">'type'</span>) == <span class="string">'dynamic'</span>:</span><br><span class="line">        self.start_urls = list(eval(<span class="string">'urls.'</span> + start_urls.<span class="builtin-name">get</span>(<span class="string">'method'</span>))(*start_urls.<span class="builtin-name">get</span>(<span class="string">'args'</span>, [])))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里通过判定 start_urls 的类型分别进行不同的处理，这样我们就可以实现 start_urls 的配置了。 至此，Spider 的设置、起始链接、属性、提取方法都已经实现了全部的可配置化。 综上所述，整个项目的配置包括如下内容。</p>
                  <ul>
                    <li>spider，指定所使用的 Spider 的名称。</li>
                    <li>settings，可以专门为 Spider 定制配置信息，会覆盖项目级别的配置。</li>
                    <li>start_urls，指定爬虫爬取的起始链接。</li>
                    <li>allowed_domains，允许爬取的站点。</li>
                    <li>rules，站点的爬取规则。</li>
                    <li>item，数据的提取规则。</li>
                  </ul>
                  <p>我们实现了 Scrapy 的通用爬虫，每个站点只需要修改 JSON 文件即可实现自由配置。</p>
                  <h3 id="7-本节代码"><a href="#7-本节代码" class="headerlink" title="7. 本节代码"></a>7. 本节代码</h3>
                  <p>本节代码地址为：<a href="https://github.com/Python3WebSpider/ScrapyUniversal" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapyUniversal</a>。</p>
                  <h3 id="8-结语"><a href="#8-结语" class="headerlink" title="8. 结语"></a>8. 结语</h3>
                  <p>本节介绍了 Scrapy 通用爬虫的实现。我们将所有配置抽离出来，每增加一个爬虫，就只需要增加一个 JSON 文件配置。之后我们只需要维护这些配置文件即可。如果要更加方便的管理，可以将规则存入数据库，再对接可视化管理页面即可。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-06 09:30:05" itemprop="dateCreated datePublished" datetime="2019-12-06T09:30:05+08:00">2019-12-06</time>
                </span>
                <span id="/8413.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.10–Scrapy 通用爬虫" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>18k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>16 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8410.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8410.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.9–Scrapy 对接 Splash</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-9-Scrapy-对接-Splash"><a href="#13-9-Scrapy-对接-Splash" class="headerlink" title="13.9 Scrapy 对接 Splash"></a>13.9 Scrapy 对接 Splash</h1>
                  <p>在上一节我们实现了 Scrapy 对接 Selenium 抓取淘宝商品的过程，这是一种抓取 JavaScript 动态渲染页面的方式。除了 Selenium，Splash 也可以实现同样的功能。本节我们来了解 Scrapy 对接 Splash 来进行页面抓取的方式。</p>
                  <h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h3>
                  <p>请确保 Splash 已经正确安装并正常运行，同时安装好 Scrapy-Splash 库，如果没有安装可以参考第 1 章的安装说明。</p>
                  <h3 id="2-新建项目"><a href="#2-新建项目" class="headerlink" title="2. 新建项目"></a>2. 新建项目</h3>
                  <p>首先新建一个项目，名为 scrapysplashtest，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy startproject scrapysplashtest</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>新建一个 Spider，命令如下所示：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">genspider</span> <span class="selector-tag">taobao</span> <span class="selector-tag">www</span><span class="selector-class">.taobao</span><span class="selector-class">.com</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="3-添加配置"><a href="#3-添加配置" class="headerlink" title="3. 添加配置"></a>3. 添加配置</h3>
                  <p>可以参考 Scrapy-Splash 的配置说明进行一步步的配置，链接如下：<a href="https://github.com/scrapy-plugins/scrapy-splash#configuration" target="_blank" rel="noopener">https://github.com/scrapy-plugins/scrapy-splash#configuration</a>。 修改 settings.py，配置 SPLASH_URL。在这里我们的 Splash 是在本地运行的，所以可以直接配置本地的地址：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">SPLASH_URL</span> = <span class="string">'http://localhost:8050'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如果 Splash 是在远程服务器运行的，那此处就应该配置为远程的地址。例如运行在 IP 为 120.27.34.25 的服务器上，则此处应该配置为：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">SPLASH_URL</span> = <span class="string">'http://120.27.34.25:8050'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>还需要配置几个 Middleware，代码如下所示：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">DOWNLOADER_MIDDLEWARES</span> <span class="string">=</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'scrapy_splash.SplashCookiesMiddleware':</span> <span class="number">723</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy_splash.SplashMiddleware':</span> <span class="number">725</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware':</span> <span class="number">810</span><span class="string">,</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">SPIDER_MIDDLEWARES</span> <span class="string">=</span> <span class="string">&#123;'scrapy_splash.SplashDeduplicateArgsMiddleware':</span> <span class="number">100</span><span class="string">,&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里配置了三个 Downloader Middleware 和一个 Spider Middleware，这是 Scrapy-Splash 的核心部分。我们不再需要像对接 Selenium 那样实现一个 Downloader Middleware，Scrapy-Splash 库都为我们准备好了，直接配置即可。 还需要配置一个去重的类 DUPEFILTER_CLASS，代码如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">DUPEFILTER_CLASS</span> = <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>最后配置一个 Cache 存储 HTTPCACHE_STORAGE，代码如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">HTTPCACHE_STORAGE</span> = <span class="string">'scrapy_splash.SplashAwareFSCacheStorage'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="4-新建请求"><a href="#4-新建请求" class="headerlink" title="4. 新建请求"></a>4. 新建请求</h3>
                  <p>配置完成之后，我们就可以利用 Splash 来抓取页面了。我们可以直接生成一个 SplashRequest 对象并传递相应的参数，Scrapy 会将此请求转发给 Splash，Splash 对页面进行渲染加载，然后再将渲染结果传递回来。此时 Response 的内容就是渲染完成的页面结果了，最后交给 Spider 解析即可。 我们来看一个示例，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">yield SplashRequest(url, self.parse_result,</span><br><span class="line">    args=&#123;</span><br><span class="line">        # optional; parameters passed <span class="keyword">to</span> Splash HTTP API</span><br><span class="line">        <span class="string">'wait'</span>: 0.5,</span><br><span class="line">        # <span class="string">'url'</span> is prefilled <span class="keyword">from</span> request url</span><br><span class="line">        # <span class="string">'http_method'</span> is <span class="builtin-name">set</span> <span class="keyword">to</span> <span class="string">'POST'</span> <span class="keyword">for</span> POST requests</span><br><span class="line">        # <span class="string">'body'</span> is <span class="builtin-name">set</span> <span class="keyword">to</span> request body <span class="keyword">for</span> POST requests</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attribute">endpoint</span>=<span class="string">'render.json'</span>, # optional;<span class="built_in"> default </span>is render.html</span><br><span class="line">    <span class="attribute">splash_url</span>=<span class="string">'&lt;url&gt;'</span>,     # optional; overrides SPLASH_URL</span><br><span class="line">)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里构造了一个 SplashRequest 对象，前两个参数依然是请求的 URL 和回调函数，另外还可以通过 args 传递一些渲染参数，例如等待时间 wait 等，还可以根据 endpoint 参数指定渲染接口，另外还有更多的参数可以参考文档的说明：<a href="https://github.com/scrapy-plugins/scrapy-splash#requests" target="_blank" rel="noopener">https://github.com/scrapy-plugins/scrapy-splash#requests</a>。 另外我们也可以生成 Request 对象，关于 Splash 的配置通过 meta 属性配置即可，代码如下：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">yield scrapy.Request(url, self.parse_result, meta=&#123;</span><br><span class="line">    <span class="string">'splash'</span>: &#123;</span><br><span class="line">        <span class="string">'args'</span>: &#123;</span><br><span class="line">            # <span class="builtin-name">set</span> rendering arguments here</span><br><span class="line">            <span class="string">'html'</span>: 1,</span><br><span class="line">            <span class="string">'png'</span>: 1,</span><br><span class="line">            # <span class="string">'url'</span> is prefilled <span class="keyword">from</span> request url</span><br><span class="line">            # <span class="string">'http_method'</span> is <span class="builtin-name">set</span> <span class="keyword">to</span> <span class="string">'POST'</span> <span class="keyword">for</span> POST requests</span><br><span class="line">            # <span class="string">'body'</span> is <span class="builtin-name">set</span> <span class="keyword">to</span> request body <span class="keyword">for</span> POST requests</span><br><span class="line">        &#125;,</span><br><span class="line">        # optional parameters</span><br><span class="line">        <span class="string">'endpoint'</span>: <span class="string">'render.json'</span>,  # optional;<span class="built_in"> default </span>is render.json</span><br><span class="line">        <span class="string">'splash_url'</span>: <span class="string">'&lt;url&gt;'</span>,      # optional; overrides SPLASH_URL</span><br><span class="line">        <span class="string">'slot_policy'</span>: scrapy_splash.SlotPolicy.PER_DOMAIN,</span><br><span class="line">        <span class="string">'splash_headers'</span>: &#123;&#125;,       # optional; a dict with headers sent <span class="keyword">to</span> Splash</span><br><span class="line">        <span class="string">'dont_process_response'</span>: <span class="literal">True</span>, # optional,<span class="built_in"> default </span>is <span class="literal">False</span></span><br><span class="line">        <span class="string">'dont_send_headers'</span>: <span class="literal">True</span>,  # optional,<span class="built_in"> default </span>is <span class="literal">False</span></span><br><span class="line">        <span class="string">'magic_response'</span>: <span class="literal">False</span>,    # optional,<span class="built_in"> default </span>is <span class="literal">True</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>SplashRequest 对象通过 args 来配置和 Request 对象通过 meta 来配置，两种方式达到的效果是相同的。 本节我们要做的抓取是淘宝商品信息，涉及页面加载等待、模拟点击翻页等操作。我们可以首先定义一个 Lua 脚本，来实现页面加载、模拟点击翻页的功能，代码如下所示：</p>
                  <figure class="highlight irpf90">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="function"><span class="keyword">function</span></span> main(splash, args)</span><br><span class="line">  args = &#123;</span><br><span class="line">    url=<span class="string">"https://s.taobao.com/search?q=iPad"</span>,</span><br><span class="line">    <span class="keyword">wait</span>=<span class="number">5</span>,</span><br><span class="line">    page=<span class="number">5</span></span><br><span class="line">  &#125;</span><br><span class="line">  splash.images_enabled = false</span><br><span class="line">  <span class="keyword">assert</span>(splash:go(args.url))</span><br><span class="line">  <span class="keyword">assert</span>(splash:<span class="keyword">wait</span>(args.<span class="keyword">wait</span>))</span><br><span class="line">  js = string.<span class="keyword">format</span>(<span class="string">"document.querySelector('#mainsrp-pager div.form&gt; input').value=% d;document.querySelector('#mainsrp-pager div.form&gt; span.btn.J_Submit').click()"</span>, args.page)</span><br><span class="line">  splash:evaljs(js)</span><br><span class="line">  <span class="keyword">assert</span>(splash:<span class="keyword">wait</span>(args.<span class="keyword">wait</span>))</span><br><span class="line">  <span class="keyword">return</span> splash:png()</span><br><span class="line"><span class="keyword">end</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们定义了三个参数：请求的链接 url、等待时间 wait、分页页码 page。然后禁用图片加载，请求淘宝的商品列表页面，通过 evaljs() 方法调用 JavaScript 代码，实现页码填充和翻页点击，最后返回页面截图。我们将脚本放到 Splash 中运行，正常获取到页面截图，如图 13-15 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034117.jpg" alt=""> 图 13-15 页面截图 翻页操作也成功实现，如图 13-16 所示即为当前页码，和我们传入的页码 page 参数是相同的。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034121.jpg" alt=""> 图 13-16 翻页结果 我们只需要在 Spider 里用 SplashRequest 对接 Lua 脚本就好了，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy import Spider</span><br><span class="line"><span class="keyword">from</span> urllib.parse import quote</span><br><span class="line"><span class="keyword">from</span> scrapysplashtest.items import ProductItem</span><br><span class="line"><span class="keyword">from</span> scrapy_splash import SplashRequest</span><br><span class="line"></span><br><span class="line">script = <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">function main(splash, args)</span></span><br><span class="line"><span class="string">  splash.images_enabled = false</span></span><br><span class="line"><span class="string">  assert(splash:go(args.url))</span></span><br><span class="line"><span class="string">  assert(splash:wait(args.wait))</span></span><br><span class="line"><span class="string">  js = string.format("</span>document.querySelector(<span class="string">'#mainsrp-pager div.form&gt; input'</span>).<span class="attribute">value</span>=% d;document.querySelector(<span class="string">'#mainsrp-pager div.form&gt; span.btn.J_Submit'</span>).click()<span class="string">", args.page)</span></span><br><span class="line"><span class="string">  splash:evaljs(js)</span></span><br><span class="line"><span class="string">  assert(splash:wait(args.wait))</span></span><br><span class="line"><span class="string">  return splash:html()</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">class TaobaoSpider(Spider):</span><br><span class="line">    name = <span class="string">'taobao'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.taobao.com'</span>]</span><br><span class="line">    base_url = <span class="string">'https://s.taobao.com/search?q='</span></span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        <span class="keyword">for</span> keyword <span class="keyword">in</span> self.settings.<span class="builtin-name">get</span>(<span class="string">'KEYWORDS'</span>):</span><br><span class="line">            <span class="keyword">for</span><span class="built_in"> page </span><span class="keyword">in</span> range(1, self.settings.<span class="builtin-name">get</span>(<span class="string">'MAX_PAGE'</span>) + 1):</span><br><span class="line">                url = self.base_url + quote(keyword)</span><br><span class="line">                yield SplashRequest(url, <span class="attribute">callback</span>=self.parse, <span class="attribute">endpoint</span>=<span class="string">'execute'</span>, args=&#123;<span class="string">'lua_source'</span>: script, <span class="string">'page'</span>: page, <span class="string">'wait'</span>: 7&#125;)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>我们把 Lua 脚本定义成长字符串，通过 SplashRequest 的 args 来传递参数，接口修改为 execute。另外，args 参数里还有一个 lua_source 字段用于指定 Lua 脚本内容。这样我们就成功构造了一个 SplashRequest，对接 Splash 的工作就完成了。 其他的配置不需要更改，Item、Item Pipeline 等设置与上节对接 Selenium 的方式相同，parse() 回调函数也是完全一致的。</p>
                  <h3 id="5-运行"><a href="#5-运行" class="headerlink" title="5. 运行"></a>5. 运行</h3>
                  <p>接下来，我们通过如下命令运行爬虫：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl taobao</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如图 13-17 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034129.jpg" alt=""> 图 13-17 运行结果 由于 Splash 和 Scrapy 都支持异步处理，我们可以看到同时会有多个抓取成功的结果。在 Selenium 的对接过程中，每个页面渲染下载是在 Downloader Middleware 里完成的，所以整个过程是阻塞式的。Scrapy 会等待这个过程完成后再继续处理和调度其他请求，这影响了爬取效率。因此使用 Splash 的爬取效率比 Selenium 高很多。 最后我们再看看 MongoDB 的结果，如图 13-18 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034134.jpg" alt=""> 图 13-18 存储结果 结果同样正常保存到了 MongoDB 中。</p>
                  <h3 id="6-本节代码"><a href="#6-本节代码" class="headerlink" title="6. 本节代码"></a>6. 本节代码</h3>
                  <p>本节代码地址：<a href="https://github.com/Python3WebSpider/ScrapySplashTest" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapySplashTest</a>。</p>
                  <h3 id="7-结语"><a href="#7-结语" class="headerlink" title="7. 结语"></a>7. 结语</h3>
                  <p>在 Scrapy 中，建议使用 Splash 处理 JavaScript 动态渲染的页面。这样不会破坏 Scrapy 中的异步处理过程，会大大提高爬取效率。而且 Splash 的安装和配置比较简单，通过 API 调用的方式实现了模块分离，大规模爬取的部署也更加方便。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-06 09:27:03" itemprop="dateCreated datePublished" datetime="2019-12-06T09:27:03+08:00">2019-12-06</time>
                </span>
                <span id="/8410.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.9–Scrapy 对接 Splash" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>5.1k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>5 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8397.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8397.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.8–Scrapy 对接 Selenium</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-8-Scrapy-对接-Selenium"><a href="#13-8-Scrapy-对接-Selenium" class="headerlink" title="13.8 Scrapy 对接 Selenium"></a>13.8 Scrapy 对接 Selenium</h1>
                  <p>Scrapy 抓取页面的方式和 requests 库类似，都是直接模拟 HTTP 请求，而 Scrapy 也不能抓取 JavaScript 动态渲染的页面。在前文中抓取 JavaScript 渲染的页面有两种方式。一种是分析 Ajax 请求，找到其对应的接口抓取，Scrapy 同样可以用此种方式抓取。另一种是直接用 Selenium 或 Splash 模拟浏览器进行抓取，我们不需要关心页面后台发生的请求，也不需要分析渲染过程，只需要关心页面最终结果即可，可见即可爬。那么，如果 Scrapy 可以对接 Selenium，那 Scrapy 就可以处理任何网站的抓取了。</p>
                  <h3 id="1-本节目标"><a href="#1-本节目标" class="headerlink" title="1. 本节目标"></a>1. 本节目标</h3>
                  <p>本节我们来看看 Scrapy 框架如何对接 Selenium，以 PhantomJS 进行演示。我们依然抓取淘宝商品信息，抓取逻辑和前文中用 Selenium 抓取淘宝商品完全相同。</p>
                  <h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2. 准备工作"></a>2. 准备工作</h3>
                  <p>请确保 PhantomJS 和 MongoDB 已经安装好并可以正常运行，安装好 Scrapy、Selenium、PyMongo 库，安装方式可以参考第 1 章的安装说明。</p>
                  <h3 id="3-新建项目"><a href="#3-新建项目" class="headerlink" title="3. 新建项目"></a>3. 新建项目</h3>
                  <p>首先新建项目，名为 scrapyseleniumtest，命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy startproject scrapyseleniumtest</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>新建一个 Spider，命令如下所示：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">genspider</span> <span class="selector-tag">taobao</span> <span class="selector-tag">www</span><span class="selector-class">.taobao</span><span class="selector-class">.com</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>修改 ROBOTSTXT_OBEY 为 False，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">ROBOTSTXT_OBEY</span> = <span class="literal">False</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="4-定义-Item"><a href="#4-定义-Item" class="headerlink" title="4. 定义 Item"></a>4. 定义 Item</h3>
                  <p>首先定义 Item 对象，名为 ProductItem，代码如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item, Field</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="symbol">ProductItem</span>(<span class="symbol">Item</span>):</span><br><span class="line"></span><br><span class="line">    <span class="symbol">collection</span> = '<span class="symbol">products</span>'</span><br><span class="line">    <span class="symbol">image</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">price</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">deal</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">title</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">shop</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">location</span> = <span class="symbol">Field</span>()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里我们定义了 6 个 Field，也就是 6 个字段，跟之前的案例完全相同。然后定义了一个 collection 属性，即此 Item 保存到 MongoDB 的 Collection 名称。 初步实现 Spider 的 start_requests() 方法，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy import Request, Spider</span><br><span class="line"><span class="keyword">from</span> urllib.parse import quote</span><br><span class="line"><span class="keyword">from</span> scrapyseleniumtest.items import ProductItem</span><br><span class="line"></span><br><span class="line">class TaobaoSpider(Spider):</span><br><span class="line">    name = <span class="string">'taobao'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.taobao.com'</span>]</span><br><span class="line">    base_url = <span class="string">'https://s.taobao.com/search?q='</span></span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        <span class="keyword">for</span> keyword <span class="keyword">in</span> self.settings.<span class="builtin-name">get</span>(<span class="string">'KEYWORDS'</span>):</span><br><span class="line">            <span class="keyword">for</span><span class="built_in"> page </span><span class="keyword">in</span> range(1, self.settings.<span class="builtin-name">get</span>(<span class="string">'MAX_PAGE'</span>) + 1):</span><br><span class="line">                url = self.base_url + quote(keyword)</span><br><span class="line">                yield Request(<span class="attribute">url</span>=url, <span class="attribute">callback</span>=self.parse, meta=&#123;<span class="string">'page'</span>: page&#125;, <span class="attribute">dont_filter</span>=<span class="literal">True</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先定义了一个 base_url，即商品列表的 URL，其后拼接一个搜索关键字就是该关键字在淘宝的搜索结果商品列表页面。 关键字用 KEYWORDS 标识，定义为一个列表。最大翻页页码用 MAX_PAGE 表示。它们统一定义在 setttings.py 里面，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">KEYWORDS</span> = [<span class="string">'iPad'</span>]</span><br><span class="line"><span class="attr">MAX_PAGE</span> = <span class="number">100</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在 start_requests() 方法里，我们首先遍历了关键字，遍历了分页页码，构造并生成 Request。由于每次搜索的 URL 是相同的，所以分页页码用 meta 参数来传递，同时设置 dont_filter 不去重。这样爬虫启动的时候，就会生成每个关键字对应的商品列表的每一页的请求了。</p>
                  <h3 id="5-对接-Selenium"><a href="#5-对接-Selenium" class="headerlink" title="5. 对接 Selenium"></a>5. 对接 Selenium</h3>
                  <p>接下来我们需要处理这些请求的抓取。这次我们对接 Selenium 进行抓取，采用 Downloader Middleware 来实现。在 Middleware 里面的 process_request() 方法里对每个抓取请求进行处理，启动浏览器并进行页面渲染，再将渲染后的结果构造一个 HtmlResponse 对象返回。代码实现如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> selenium import webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions import TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by import By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui import WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support import expected_conditions as EC</span><br><span class="line"><span class="keyword">from</span> scrapy.http import HtmlResponse</span><br><span class="line"><span class="keyword">from</span><span class="built_in"> logging </span>import getLogger</span><br><span class="line"></span><br><span class="line">class SeleniumMiddleware():</span><br><span class="line">    def __init__(self, <span class="attribute">timeout</span>=None, service_args=[]):</span><br><span class="line">        self.logger = getLogger(__name__)</span><br><span class="line">        self.timeout = timeout</span><br><span class="line">        self.browser = webdriver.PhantomJS(<span class="attribute">service_args</span>=service_args)</span><br><span class="line">        self.browser.set_window_size(1400, 700)</span><br><span class="line">        self.browser.set_page_load_timeout(self.timeout)</span><br><span class="line">        self.wait = WebDriverWait(self.browser, self.timeout)</span><br><span class="line"></span><br><span class="line">    def __del__(self):</span><br><span class="line">        self.browser.close()</span><br><span class="line"></span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        用 PhantomJS 抓取页面</span></span><br><span class="line"><span class="string">        :param request: Request 对象</span></span><br><span class="line"><span class="string">        :param spider: Spider 对象</span></span><br><span class="line"><span class="string">        :return: HtmlResponse</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        self.logger.<span class="builtin-name">debug</span>(<span class="string">'PhantomJS is Starting'</span>)</span><br><span class="line">       <span class="built_in"> page </span>= request.meta.<span class="builtin-name">get</span>(<span class="string">'page'</span>, 1)</span><br><span class="line">        try:</span><br><span class="line">            self.browser.<span class="builtin-name">get</span>(request.url)</span><br><span class="line">            <span class="keyword">if</span><span class="built_in"> page </span>&gt; 1:</span><br><span class="line">                input = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, <span class="string">'#mainsrp-pager div.form&gt; input'</span>)))</span><br><span class="line">                submit = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, <span class="string">'#mainsrp-pager div.form&gt; span.btn.J_Submit'</span>)))</span><br><span class="line">                input.clear()</span><br><span class="line">                input.send_keys(page)</span><br><span class="line">                submit.click()</span><br><span class="line">            self.wait.until(EC.text_to_be_present_in_element((By.CSS_SELECTOR, <span class="string">'#mainsrp-pager li.item.active&gt; span'</span>), str(page)))</span><br><span class="line">            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, <span class="string">'.m-itemlist .items .item'</span>)))</span><br><span class="line">            return HtmlResponse(<span class="attribute">url</span>=request.url, <span class="attribute">body</span>=self.browser.page_source, <span class="attribute">request</span>=request, <span class="attribute">encoding</span>=<span class="string">'utf-8'</span>, <span class="attribute">status</span>=200)</span><br><span class="line">        except TimeoutException:</span><br><span class="line">            return HtmlResponse(<span class="attribute">url</span>=request.url, <span class="attribute">status</span>=500, <span class="attribute">request</span>=request)</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(<span class="attribute">timeout</span>=crawler.settings.get('SELENIUM_TIMEOUT'),</span><br><span class="line">                   <span class="attribute">service_args</span>=crawler.settings.get('PHANTOMJS_SERVICE_ARGS'))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先我们在 <strong>init</strong>() 里对一些对象进行初始化，包括 PhantomJS、WebDriverWait 等对象，同时设置页面大小和页面加载超时时间。在 process_request() 方法中，我们通过 Request 的 meta 属性获取当前需要爬取的页码，调用 PhantomJS 对象的 get() 方法访问 Request 的对应的 URL。这就相当于从 Request 对象里获取请求链接，然后再用 PhantomJS 加载，而不再使用 Scrapy 里的 Downloader。 随后的处理等待和翻页的方法在此不再赘述，和前文的原理完全相同。最后，页面加载完成之后，我们调用 PhantomJS 的 page_source 属性即可获取当前页面的源代码，然后用它来直接构造并返回一个 HtmlResponse 对象。构造这个对象的时候需要传入多个参数，如 url、body 等，这些参数实际上就是它的基础属性。可以在官方文档查看 HtmlResponse 对象的结构：<a href="https://doc.scrapy.org/en/latest/topics/request-response.html" target="_blank" rel="noopener">https://doc.scrapy.org/en/latest/topics/request-response.html</a>，这样我们就成功利用 PhantomJS 来代替 Scrapy 完成了页面的加载，最后将 Response 返回即可。 有人可能会纳闷：为什么实现这么一个 Downloader Middleware 就可以了？之前的 Request 对象怎么办？Scrapy 不再处理了吗？Response 返回后又传递给了谁？ 是的，Request 对象到这里就不会再处理了，也不会再像以前一样交给 Downloader 下载。Response 会直接传给 Spider 进行解析。 我们需要回顾一下 Downloader Middleware 的 process_request() 方法的处理逻辑，内容如下所示： 当 process_request() 方法返回 Response 对象的时候，更低优先级的 Downloader Middleware 的 process_request() 和 process_exception() 方法就不会被继续调用了，转而开始执行每个 Downloader Middleware 的 process_response() 方法，调用完毕之后直接将 Response 对象发送给 Spider 来处理。 这里直接返回了一个 HtmlResponse 对象，它是 Response 的子类，返回之后便顺次调用每个 Downloader Middleware 的 process_response() 方法。而在 process_response() 中我们没有对其做特殊处理，它会被发送给 Spider，传给 Request 的回调函数进行解析。 到现在，我们应该能了解 Downloader Middleware 实现 Selenium 对接的原理了。 在 settings.py 里，我们设置调用刚才定义的 SeleniumMiddleware、设置等待超时变量 SELENIUM_TIMEOUT、设置 PhantomJS 配置参数 PHANTOMJS_SERVICE_ARGS，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">DOWNLOADER_MIDDLEWARES</span> = &#123;<span class="string">'scrapyseleniumtest.middlewares.SeleniumMiddleware'</span>: <span class="number">543</span>,&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="6-解析页面"><a href="#6-解析页面" class="headerlink" title="6. 解析页面"></a>6. 解析页面</h3>
                  <p>Response 对象就会回传给 Spider 内的回调函数进行解析。所以下一步我们就实现其回调函数，对网页来进行解析，代码如下所示：</p>
                  <figure class="highlight nginx">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">def</span> parse(self, response):</span><br><span class="line">    products = response.xpath(<span class="string">'//div[<span class="variable">@id</span>="mainsrp-itemlist"]//div[<span class="variable">@class</span>="items"][1]//div[contains(<span class="variable">@class</span>, "item")]'</span>)</span><br><span class="line">    for product in products:</span><br><span class="line">        item = ProductItem()</span><br><span class="line">        item[<span class="string">'price'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[contains(<span class="variable">@class</span>, "price")]//text()'</span>).extract()).strip()</span><br><span class="line">        item[<span class="string">'title'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[contains(<span class="variable">@class</span>, "title")]//text()'</span>).extract()).strip()</span><br><span class="line">        item[<span class="string">'shop'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[contains(<span class="variable">@class</span>, "shop")]//text()'</span>).extract()).strip()</span><br><span class="line">        item[<span class="string">'image'</span>] = <span class="string">''</span>.join(product.xpath(<span class="string">'.//div[<span class="variable">@class</span>="pic"]//img[contains(<span class="variable">@class</span>, "img")]/<span class="variable">@data</span>-src'</span>).extract()).strip()</span><br><span class="line">        item[<span class="string">'deal'</span>] = product.xpath(<span class="string">'.//div[contains(<span class="variable">@class</span>, "deal-cnt")]//text()'</span>).extract_first()</span><br><span class="line">        item[<span class="string">'location'</span>] = product.xpath(<span class="string">'.//div[contains(<span class="variable">@class</span>, "location")]//text()'</span>).extract_first()</span><br><span class="line">        yield item</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们使用 XPath 进行解析，调用 response 变量的 xpath() 方法即可。首先我们传递选取所有商品对应的 XPath，可以匹配所有商品，随后对结果进行遍历，依次选取每个商品的名称、价格、图片等内容，构造并返回一个 ProductItem 对象。</p>
                  <h3 id="7-存储结果"><a href="#7-存储结果" class="headerlink" title="7. 存储结果"></a>7. 存储结果</h3>
                  <p>最后我们实现一个 Item Pipeline，将结果保存到 MongoDB，如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, mongo_uri, mongo_db)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.mongo_uri = mongo_uri</span><br><span class="line">        <span class="keyword">self</span>.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> cls(mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>), mongo_db=crawler.settings.get(<span class="string">'MONGO_DB'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client = pymongo.MongoClient(<span class="keyword">self</span>.mongo_uri)</span><br><span class="line">        <span class="keyword">self</span>.db = <span class="keyword">self</span>.client[<span class="keyword">self</span>.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db[item.collection].insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>此实现和前文中存储到 MongoDB 的方法完全一致，原理不再赘述。记得在 settings.py 中开启它的调用，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">ITEM_PIPELINES</span> = &#123;<span class="string">'scrapyseleniumtest.pipelines.MongoPipeline'</span>: <span class="number">300</span>,&#125;</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>其中，MONGO_URI 和 MONGO_DB 的定义如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">MONGO_URI</span> = <span class="string">'localhost'</span></span><br><span class="line"><span class="attr">MONGO_DB</span> = <span class="string">'taobao'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="8-运行"><a href="#8-运行" class="headerlink" title="8. 运行"></a>8. 运行</h3>
                  <p>整个项目就完成了，执行如下命令启动抓取即可：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl taobao</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行结果如图 13-13 所示： <img src="https://qiniu.cuiqingcai.com/2019-11-27-034100.jpg" alt=""> 图 13-13 运行结果 再查看一下 MongoDB，结果如图 13-14 所示： <img src="https://qiniu.cuiqingcai.com/2019-11-27-034105.jpg" alt=""> 图 13-14 MongoDB 结果 这样我们便成功在 Scrapy 中对接 Selenium 并实现了淘宝商品的抓取。</p>
                  <h3 id="9-本节代码"><a href="#9-本节代码" class="headerlink" title="9. 本节代码"></a>9. 本节代码</h3>
                  <p>本节代码地址为：<a href="https://github.com/Python3WebSpider/ScrapySeleniumTest" target="_blank" rel="noopener">https://github.com/Python3WebSpider/ScrapySeleniumTest</a>。</p>
                  <h3 id="10-结语"><a href="#10-结语" class="headerlink" title="10. 结语"></a>10. 结语</h3>
                  <p>我们通过改写 Downloader Middleware 的方式实现了 Selenium 的对接。但这种方法其实是阻塞式的，也就是说这样就破坏了 Scrapy 异步处理的逻辑，速度会受到影响。为了不破坏其异步加载逻辑，我们可以使用 Splash 实现。下一节我们再来看看 Scrapy 对接 Splash 的方式。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-05 09:30:23" itemprop="dateCreated datePublished" datetime="2019-12-05T09:30:23+08:00">2019-12-05</time>
                </span>
                <span id="/8397.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.8–Scrapy 对接 Selenium" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>7.2k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>7 分钟</span>
                </span>
              </div>
            </article>
            <article itemscope itemtype="http://schema.org/Article" class="post-block index" lang="zh-CN">
              <link itemprop="mainEntityOfPage" href="https://cuiqingcai.com/8394.html">
              <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
                <meta itemprop="image" content="/images/avatar.png">
                <meta itemprop="name" content="崔庆才">
                <meta itemprop="description" content="崔庆才的个人站点，记录生活的瞬间，分享学习的心得。">
              </span>
              <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
                <meta itemprop="name" content="静觅">
              </span>
              <header class="post-header">
                <h2 class="post-title" itemprop="name headline">
                  <a class="label"> Python <i class="label-arrow"></i>
                  </a>
                  <a href="/8394.html" class="post-title-link" itemprop="url">[Python3网络爬虫开发实战] 13.7–Item Pipeline 的用法</a>
                </h2>
              </header>
              <div class="post-body" itemprop="articleBody">
                <div class="thumb">
                  <img itemprop="contentUrl" class="random">
                </div>
                <div class="excerpt">
                  <p>
                  <h1 id="13-7-Item-Pipeline-的用法"><a href="#13-7-Item-Pipeline-的用法" class="headerlink" title="13.7 Item Pipeline 的用法"></a>13.7 Item Pipeline 的用法</h1>
                  <p>Item Pipeline 是项目管道。在前面我们已经了解了 Item Pipeline 的基本用法，本节我们再作详细了解它的用法。 首先我们看看 Item Pipeline 在 Scrapy 中的架构，如图 13-1 所示。 图中的最左侧即为 Item Pipeline，它的调用发生在 Spider 产生 Item 之后。当 Spider 解析完 Response 之后，Item 就会传递到 Item Pipeline，被定义的 Item Pipeline 组件会顺次调用，完成一连串的处理过程，比如数据清洗、存储等。 它的主要功能有：</p>
                  <ul>
                    <li>清洗 HTML 数据</li>
                    <li>验证爬取数据，检查爬取字段</li>
                    <li>查重并丢弃重复内容</li>
                    <li>将爬取结果储存到数据库</li>
                  </ul>
                  <h3 id="1-核心方法"><a href="#1-核心方法" class="headerlink" title="1. 核心方法"></a>1. 核心方法</h3>
                  <p>我们可以自定义 Item Pipeline，只需要实现指定的方法就好，其中必须要实现的一个方法是：</p>
                  <ul>
                    <li>process_item(item, spider)</li>
                  </ul>
                  <p>另外还有几个比较实用的方法，它们分别是：</p>
                  <ul>
                    <li>open_spider(spider)</li>
                    <li>close_spider(spider)</li>
                    <li>from_crawler(cls, crawler)</li>
                  </ul>
                  <p>下面我们对这几个方法的用法作下详细的介绍：</p>
                  <h4 id="process-item-item-spider"><a href="#process-item-item-spider" class="headerlink" title="process_item(item, spider)"></a>process_item(item, spider)</h4>
                  <p>process_item() 是必须要实现的方法，被定义的 Item Pipeline 会默认调用这个方法对 Item 进行处理。比如，我们可以进行数据处理或者将数据写入到数据库等操作。它必须返回 Item 类型的值或者抛出一个 DropItem 异常。 process_item() 方法的参数有如下两个。</p>
                  <ul>
                    <li>item，是 Item 对象，即被处理的 Item</li>
                    <li>spider，是 Spider 对象，即生成该 Item 的 Spider</li>
                  </ul>
                  <p>下面对该方法的返回类型归纳如下：</p>
                  <ul>
                    <li>如果返回的是 Item 对象，那么此 Item 会接着被低优先级的 Item Pipeline 的 process_item() 方法进行处理，直到所有的方法被调用完毕。</li>
                    <li>如果抛出的是 DropItem 异常，那么此 Item 就会被丢弃，不再进行处理。</li>
                  </ul>
                  <h4 id="open-spider-self-spider"><a href="#open-spider-self-spider" class="headerlink" title="open_spider(self, spider)"></a>open_spider(self, spider)</h4>
                  <p>open_spider() 方法是在 Spider 开启的时候被自动调用的，在这里我们可以做一些初始化操作，如开启数据库连接等。其中参数 spider 就是被开启的 Spider 对象。</p>
                  <h4 id="close-spider-spider"><a href="#close-spider-spider" class="headerlink" title="close_spider(spider)"></a>close_spider(spider)</h4>
                  <p>close_spider() 方法是在 Spider 关闭的时候自动调用的，在这里我们可以做一些收尾工作，如关闭数据库连接等，其中参数 spider 就是被关闭的 Spider 对象。</p>
                  <h4 id="from-crawler-cls-crawler"><a href="#from-crawler-cls-crawler" class="headerlink" title="from_crawler(cls, crawler)"></a>from_crawler(cls, crawler)</h4>
                  <p>from_crawler() 方法是一个类方法，用 @classmethod 标识，是一种依赖注入的方式。它的参数是 crawler，通过 crawler 对象，我们可以拿到 Scrapy 的所有核心组件，如全局配置的每个信息，然后创建一个 Pipeline 实例。参数 cls 就是 Class，最后返回一个 Class 实例。 下面我们用一个实例来加深对 Item Pipeline 用法的理解。</p>
                  <h3 id="2-本节目标"><a href="#2-本节目标" class="headerlink" title="2. 本节目标"></a>2. 本节目标</h3>
                  <p>我们以爬取 360 摄影美图为例，来分别实现 MongoDB 存储、MySQL 存储、Image 图片存储的三个 Pipeline。</p>
                  <h3 id="3-准备工作"><a href="#3-准备工作" class="headerlink" title="3. 准备工作"></a>3. 准备工作</h3>
                  <p>请确保已经安装好 MongoDB 和 MySQL 数据库，安装好 Python 的 PyMongo、PyMySQL、Scrapy 框架，另外需要安装 pillow 图像处理库，如没有安装可以参考第 1 章的安装说明。</p>
                  <h3 id="4-抓取分析"><a href="#4-抓取分析" class="headerlink" title="4. 抓取分析"></a>4. 抓取分析</h3>
                  <p>我们这次爬取的目标网站为：<a href="https://image.so.com。打开此页面，切换到摄影页面，网页中呈现了许许多多的摄影美图。我们打开浏览器开发者工具，过滤器切换到" target="_blank" rel="noopener">https://image.so.com。打开此页面，切换到摄影页面，网页中呈现了许许多多的摄影美图。我们打开浏览器开发者工具，过滤器切换到</a> XHR 选项，然后下拉页面，可以看到下面就会呈现许多 Ajax 请求，如图 13-6 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-033958.png" alt=""> 图 13-6 请求列表 我们查看一个请求的详情，观察返回的数据结构，如图 13-7 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034003.jpg" alt=""> 图 13-7 返回结果 返回格式是 JSON。其中 list 字段就是一张张图片的详情信息，包含了 30 张图片的 ID、名称、链接、缩略图等信息。另外观察 Ajax 请求的参数信息，有一个参数 sn 一直在变化，这个参数很明显就是偏移量。当 sn 为 30 时，返回的是前 30 张图片，sn 为 60 时，返回的就是第 31~60 张图片。另外，ch 参数是摄影类别，listtype 是排序方式，temp 参数可以忽略。 所以我们抓取时只需要改变 sn 的数值就好了。 下面我们用 Scrapy 来实现图片的抓取，将图片的信息保存到 MongoDB、MySQL，同时将图片存储到本地。</p>
                  <h3 id="5-新建项目"><a href="#5-新建项目" class="headerlink" title="5. 新建项目"></a>5. 新建项目</h3>
                  <p>首先新建一个项目，命令如下：</p>
                  <figure class="highlight mipsasm">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">scrapy </span>startproject images360</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>接下来新建一个 Spider，命令如下：</p>
                  <figure class="highlight css">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">genspider</span> <span class="selector-tag">images</span> <span class="selector-tag">images</span><span class="selector-class">.so</span><span class="selector-class">.com</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样我们就成功创建了一个 Spider。</p>
                  <h3 id="6-构造请求"><a href="#6-构造请求" class="headerlink" title="6. 构造请求"></a>6. 构造请求</h3>
                  <p>接下来定义爬取的页数。比如爬取 50 页、每页 30 张，也就是 1500 张图片，我们可以先在 settings.py 里面定义一个变量 MAX_PAGE，添加如下定义：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">MAX_PAGE</span> = <span class="number">50</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>定义 start_requests() 方法，用来生成 50 次请求，如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def start_requests(self):</span><br><span class="line">    data = &#123;<span class="string">'ch'</span>: <span class="string">'photography'</span>, <span class="string">'listtype'</span>: <span class="string">'new'</span>&#125;</span><br><span class="line">    base_url = <span class="string">'https://image.so.com/zj?'</span></span><br><span class="line">    <span class="keyword">for</span><span class="built_in"> page </span><span class="keyword">in</span> range(1, self.settings.<span class="builtin-name">get</span>(<span class="string">'MAX_PAGE'</span>) + 1):</span><br><span class="line">        data[<span class="string">'sn'</span>] =<span class="built_in"> page </span>* 30</span><br><span class="line">        params = urlencode(data)</span><br><span class="line">        url = base_url + params</span><br><span class="line">        yield Request(url, self.parse)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们首先定义了初始的两个参数，sn 参数是遍历循环生成的。然后利用 urlencode() 方法将字典转化为 URL 的 GET 参数，构造出完整的 URL，构造并生成 Request。 还需要引入 scrapy.Request 和 urllib.parse 模块，如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Spider, Request</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>再修改 settings.py 中的 ROBOTSTXT_OBEY 变量，将其设置为 False，否则无法抓取，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">ROBOTSTXT_OBEY</span> = <span class="literal">False</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行爬虫，即可以看到链接都请求成功，执行命令如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl images</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>运行示例结果如图 13-8 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034009.jpg" alt=""> 图 13-8 运行结果 所有请求的状态码都是 200，这就证明图片信息爬取成功了。</p>
                  <h3 id="7-提取信息"><a href="#7-提取信息" class="headerlink" title="7. 提取信息"></a>7. 提取信息</h3>
                  <p>首先定义一个 Item，叫作 ImageItem，如下所示：</p>
                  <figure class="highlight angelscript">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item, Field</span><br><span class="line"><span class="keyword">class</span> <span class="symbol">ImageItem</span>(<span class="symbol">Item</span>):</span><br><span class="line">    <span class="symbol">collection</span> = <span class="symbol">table</span> = '<span class="symbol">images</span>'</span><br><span class="line">    <span class="symbol">id</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">url</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">title</span> = <span class="symbol">Field</span>()</span><br><span class="line">    <span class="symbol">thumb</span> = <span class="symbol">Field</span>()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们定义了 4 个字段，包括图片的 ID、链接、标题、缩略图。另外还有两个属性 collection 和 table，都定义为 images 字符串，分别代表 MongoDB 存储的 Collection 名称和 MySQL 存储的表名称。 接下来我们提取 Spider 里有关信息，将 parse() 方法改写为如下所示：</p>
                  <figure class="highlight livecodeserver">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">def parse(self, response):</span><br><span class="line">    <span class="built_in">result</span> = json.loads(response.<span class="keyword">text</span>)</span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> <span class="built_in">result</span>.<span class="built_in">get</span>(<span class="string">'list'</span>):</span><br><span class="line">        <span class="keyword">item</span> = ImageItem()</span><br><span class="line">        <span class="keyword">item</span>[<span class="string">'id'</span>] = image.<span class="built_in">get</span>(<span class="string">'imageid'</span>)</span><br><span class="line">        <span class="keyword">item</span>[<span class="string">'url'</span>] = image.<span class="built_in">get</span>(<span class="string">'qhimg_url'</span>)</span><br><span class="line">        <span class="keyword">item</span>[<span class="string">'title'</span>] = image.<span class="built_in">get</span>(<span class="string">'group_title'</span>)</span><br><span class="line">        <span class="keyword">item</span>[<span class="string">'thumb'</span>] = image.<span class="built_in">get</span>(<span class="string">'qhimg_thumb_url'</span>)</span><br><span class="line">        yield <span class="keyword">item</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>首先解析 JSON，遍历其 list 字段，取出一个个图片信息，然后再对 ImageItem 赋值，生成 Item 对象。 这样我们就完成了信息的提取。</p>
                  <h3 id="8-存储信息"><a href="#8-存储信息" class="headerlink" title="8. 存储信息"></a>8. 存储信息</h3>
                  <p>接下来我们需要将图片的信息保存到 MongoDB、MySQL，同时将图片保存到本地。</p>
                  <h4 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h4>
                  <p>首先确保 MongoDB 已经正常安装并且正常运行。 我们用一个 MongoPipeline 将信息保存到 MongoDB，在 pipelines.py 里添加如下类的实现：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, mongo_uri, mongo_db)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.mongo_uri = mongo_uri</span><br><span class="line">        <span class="keyword">self</span>.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> cls(mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DB'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client = pymongo.MongoClient(<span class="keyword">self</span>.mongo_uri)</span><br><span class="line">        <span class="keyword">self</span>.db = <span class="keyword">self</span>.client[<span class="keyword">self</span>.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db[item.collection].insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.client.close()</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里需要用到两个变量，MONGO_URI 和 MONGO_DB，即存储到 MongoDB 的链接地址和数据库名称。我们在 settings.py 里添加这两个变量，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">MONGO_URI</span> = <span class="string">'localhost'</span></span><br><span class="line"><span class="attr">MONGO_DB</span> = <span class="string">'images360'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这样一个保存到 MongoDB 的 Pipeline 的就创建好了。这里最主要的方法是 process_item() 方法，直接调用 Collection 对象的 insert() 方法即可完成数据的插入，最后返回 Item 对象。</p>
                  <h4 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h4>
                  <p>首先确保 MySQL 已经正确安装并且正常运行。 新建一个数据库，名字还是 images360，SQL 语句如下所示：</p>
                  <figure class="highlight routeros">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">CREATE DATABASE images360<span class="built_in"> DEFAULT </span>CHARACTER <span class="builtin-name">SET</span> utf8 COLLATE utf8_general_ci</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>新建一个数据表，包含 id、url、title、thumb 四个字段，SQL 语句如下所示：</p>
                  <figure class="highlight sql">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> images (<span class="keyword">id</span> <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>, <span class="keyword">url</span> <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="literal">NULL</span> , title <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="literal">NULL</span> , thumb <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="literal">NULL</span>)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>执行完 SQL 语句之后，我们就成功创建好了数据表。接下来就可以往表里存储数据了。 接下来我们实现一个 MySQLPipeline，代码如下所示：</p>
                  <figure class="highlight ruby">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, host, database, user, password, port)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.host = host</span><br><span class="line">        <span class="keyword">self</span>.database = database</span><br><span class="line">        <span class="keyword">self</span>.user = user</span><br><span class="line">        <span class="keyword">self</span>.password = password</span><br><span class="line">        <span class="keyword">self</span>.port = port</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> cls(host=crawler.settings.get(<span class="string">'MYSQL_HOST'</span>),</span><br><span class="line">            database=crawler.settings.get(<span class="string">'MYSQL_DATABASE'</span>),</span><br><span class="line">            user=crawler.settings.get(<span class="string">'MYSQL_USER'</span>),</span><br><span class="line">            password=crawler.settings.get(<span class="string">'MYSQL_PASSWORD'</span>),</span><br><span class="line">            port=crawler.settings.get(<span class="string">'MYSQL_PORT'</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db = pymysql.connect(<span class="keyword">self</span>.host, <span class="keyword">self</span>.user, <span class="keyword">self</span>.password, <span class="keyword">self</span>.database, charset=<span class="string">'utf8'</span>, port=<span class="keyword">self</span>.port)</span><br><span class="line">        <span class="keyword">self</span>.cursor = <span class="keyword">self</span>.db.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        data = dict(item)</span><br><span class="line">        keys = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">        values = <span class="string">', '</span>.join([<span class="string">'% s'</span>] * len(data))</span><br><span class="line">        sql = <span class="string">'insert into % s (% s) values (% s)'</span> % (item.table, keys, values)</span><br><span class="line">        <span class="keyword">self</span>.cursor.execute(sql, tuple(data.values()))</span><br><span class="line">        <span class="keyword">self</span>.db.commit()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>如前所述，这里用到的数据插入方法是一个动态构造 SQL 语句的方法。 这里又需要几个 MySQL 的配置，我们在 settings.py 里添加几个变量，如下所示：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">MYSQL_HOST</span> = <span class="string">'localhost'</span></span><br><span class="line"><span class="attr">MYSQL_DATABASE</span> = <span class="string">'images360'</span></span><br><span class="line"><span class="attr">MYSQL_PORT</span> = <span class="number">3306</span></span><br><span class="line"><span class="attr">MYSQL_USER</span> = <span class="string">'root'</span></span><br><span class="line"><span class="attr">MYSQL_PASSWORD</span> = <span class="string">'123456'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里分别定义了 MySQL 的地址、数据库名称、端口、用户名、密码。 这样，MySQL Pipeline 就完成了。</p>
                  <h4 id="Image-Pipeline"><a href="#Image-Pipeline" class="headerlink" title="Image Pipeline"></a>Image Pipeline</h4>
                  <p>Scrapy 提供了专门处理下载的 Pipeline，包括文件下载和图片下载。下载文件和图片的原理与抓取页面的原理一样，因此下载过程支持异步和多线程，下载十分高效。下面我们来看看具体的实现过程。 官方文档地址为：<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html" target="_blank" rel="noopener">https://doc.scrapy.org/en/latest/topics/media-pipeline.html</a>。 首先定义存储文件的路径，需要定义一个 IMAGES_STORE 变量，在 settings.py 中添加如下代码：</p>
                  <figure class="highlight ini">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attr">IMAGES_STORE</span> = <span class="string">'./images'</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们将路径定义为当前路径下的 images 子文件夹，即下载的图片都会保存到本项目的 images 文件夹中。 内置的 ImagesPipeline 会默认读取 Item 的 image_urls 字段，并认为该字段是一个列表形式，它会遍历 Item 的 image_urls 字段，然后取出每个 URL 进行图片下载。 但是现在生成的 Item 的图片链接字段并不是 image_urls 字段表示的，也不是列表形式，而是单个的 URL。所以为了实现下载，我们需要重新定义下载的部分逻辑，即要自定义 ImagePipeline，继承内置的 ImagesPipeline，重写几个方法。 我们定义 ImagePipeline，如下所示：</p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_path</span><span class="params">(self, request, response=None, info=None)</span>:</span></span><br><span class="line">        url = request.url</span><br><span class="line">        file_name = url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">return</span> file_name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        image_paths = [x[<span class="string">'path'</span>] <span class="keyword">for</span> ok, x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_paths:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">'Image Downloaded Failed'</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(item[<span class="string">'url'</span>])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>在这里我们实现了 ImagePipeline，继承 Scrapy 内置的 ImagesPipeline，重写下面几个方法。</p>
                  <ul>
                    <li>get_media_requests()。它的第一个参数 item 是爬取生成的 Item 对象。我们将它的 url 字段取出来，然后直接生成 Request 对象。此 Request 加入到调度队列，等待被调度，执行下载。</li>
                    <li>file_path()。它的第一个参数 request 就是当前下载对应的 Request 对象。这个方法用来返回保存的文件名，直接将图片链接的最后一部分当作文件名即可。它利用 split() 函数分割链接并提取最后一部分，返回结果。这样此图片下载之后保存的名称就是该函数返回的文件名。</li>
                    <li>item_completed()，它是当单个 Item 完成下载时的处理方法。因为并不是每张图片都会下载成功，所以我们需要分析下载结果并剔除下载失败的图片。如果某张图片下载失败，那么我们就不需保存此 Item 到数据库。该方法的第一个参数 results 就是该 Item 对应的下载结果，它是一个列表形式，列表每一个元素是一个元组，其中包含了下载成功或失败的信息。这里我们遍历下载结果找出所有成功的下载列表。如果列表为空，那么该 Item 对应的图片下载失败，随即抛出异常 DropItem，该 Item 忽略。否则返回该 Item，说明此 Item 有效。</li>
                  </ul>
                  <p>现在为止，三个 Item Pipeline 的定义就完成了。最后只需要启用就可以了，修改 settings.py，设置 ITEM_PIPELINES，如下所示：</p>
                  <figure class="highlight yaml">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="string">ITEM_PIPELINES</span> <span class="string">=</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'images360.pipelines.ImagePipeline':</span> <span class="number">300</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'images360.pipelines.MongoPipeline':</span> <span class="number">301</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'images360.pipelines.MysqlPipeline':</span> <span class="number">302</span><span class="string">,</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>这里注意调用的顺序。我们需要优先调用 ImagePipeline 对 Item 做下载后的筛选，下载失败的 Item 就直接忽略，它们就不会保存到 MongoDB 和 MySQL 里。随后再调用其他两个存储的 Pipeline，这样就能确保存入数据库的图片都是下载成功的。 接下来运行程序，执行爬取，如下所示：</p>
                  <figure class="highlight ebnf">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="attribute">scrapy crawl images</span></span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>爬虫一边爬取一边下载，下载速度非常快，对应的输出日志如图 13-9 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034019.jpg" alt=""> 图 13-9 输出日志 查看本地 images 文件夹，发现图片都已经成功下载，如图 13-10 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034024.jpg" alt=""> 图 13-10 下载结果 查看 MySQL，下载成功的图片信息也已成功保存，如图 13-11 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034029.jpg" alt=""> 图 13-11 MySQL 结果 查看 MongoDB，下载成功的图片信息同样已成功保存，如图 13-12 所示。 <img src="https://qiniu.cuiqingcai.com/2019-11-27-034034.jpg" alt=""> 图 13-12 MongoDB 结果 这样我们就可以成功实现图片的下载并把图片的信息存入数据库了。</p>
                  <h3 id="9-本节代码"><a href="#9-本节代码" class="headerlink" title="9. 本节代码"></a>9. 本节代码</h3>
                  <p>本节代码地址为：<a href="https://github.com/Python3WebSpider/Images360" target="_blank" rel="noopener">https://github.com/Python3WebSpider/Images360</a>。</p>
                  <h3 id="10-结语"><a href="#10-结语" class="headerlink" title="10. 结语"></a>10. 结语</h3>
                  <p>Item Pipeline 是 Scrapy 非常重要的组件，数据存储几乎都是通过此组件实现的。请读者认真掌握此内容。</p>
                  </p>
                </div>
              </div>
              <div class="post-meta">
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-user"></i>
                  </span>
                  <span class="post-meta-item-text">作者</span>
                  <span><a href="/authors/崔庆才" class="author" itemprop="url" rel="index">崔庆才</a></span>
                </span>
                <span class="post-meta-item">
                  <span class="post-meta-item-icon">
                    <i class="far fa-calendar"></i>
                  </span>
                  <span class="post-meta-item-text">发表于</span>
                  <time title="创建时间：2019-12-05 09:26:32" itemprop="dateCreated datePublished" datetime="2019-12-05T09:26:32+08:00">2019-12-05</time>
                </span>
                <span id="/8394.html" class="post-meta-item leancloud_visitors" data-flag-title="[Python3网络爬虫开发实战] 13.7–Item Pipeline 的用法" title="阅读次数">
                  <span class="post-meta-item-icon">
                    <i class="fa fa-eye"></i>
                  </span>
                  <span class="post-meta-item-text">阅读次数：</span>
                  <span class="leancloud-visitors-count"></span>
                </span>
                <span class="post-meta-item" title="本文字数">
                  <span class="post-meta-item-icon">
                    <i class="far fa-file-word"></i>
                  </span>
                  <span class="post-meta-item-text">本文字数：</span>
                  <span>8.1k</span>
                </span>
                <span class="post-meta-item" title="阅读时长">
                  <span class="post-meta-item-icon">
                    <i class="far fa-clock"></i>
                  </span>
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                  <span>7 分钟</span>
                </span>
              </div>
            </article>
            <script>
              document.querySelectorAll('.random').forEach(item => item.src = "https://picsum.photos/id/" + Math.floor(Math.random() * Math.floor(300)) + "/200/133")

            </script>
            <nav class="pagination">
              <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
            </nav>
          </div>
          <script>
            window.addEventListener('tabs:register', () =>
            {
              let
              {
                activeClass
              } = CONFIG.comments;
              if (CONFIG.comments.storage)
              {
                activeClass = localStorage.getItem('comments_active') || activeClass;
              }
              if (activeClass)
              {
                let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
                if (activeTab)
                {
                  activeTab.click();
                }
              }
            });
            if (CONFIG.comments.storage)
            {
              window.addEventListener('tabs:click', event =>
              {
                if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
                let commentClass = event.target.classList[1];
                localStorage.setItem('comments_active', commentClass);
              });
            }

          </script>
        </div>
        <div class="toggle sidebar-toggle">
          <span class="toggle-line toggle-line-first"></span>
          <span class="toggle-line toggle-line-middle"></span>
          <span class="toggle-line toggle-line-last"></span>
        </div>
        <aside class="sidebar">
          <div class="sidebar-inner">
            <ul class="sidebar-nav motion-element">
              <li class="sidebar-nav-toc"> 文章目录 </li>
              <li class="sidebar-nav-overview"> 站点概览 </li>
            </ul>
            <!--noindex-->
            <div class="post-toc-wrap sidebar-panel">
            </div>
            <!--/noindex-->
            <div class="site-overview-wrap sidebar-panel">
              <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
                <img class="site-author-image" itemprop="image" alt="崔庆才" src="/images/avatar.png">
                <p class="site-author-name" itemprop="name">崔庆才</p>
                <div class="site-description" itemprop="description">崔庆才的个人站点，记录生活的瞬间，分享学习的心得。</div>
              </div>
              <div class="site-state-wrap motion-element">
                <nav class="site-state">
                  <div class="site-state-item site-state-posts">
                    <a href="/archives/">
                      <span class="site-state-item-count">518</span>
                      <span class="site-state-item-name">日志</span>
                    </a>
                  </div>
                  <div class="site-state-item site-state-categories">
                    <a href="/categories/">
                      <span class="site-state-item-count">21</span>
                      <span class="site-state-item-name">分类</span></a>
                  </div>
                  <div class="site-state-item site-state-tags">
                    <a href="/tags/">
                      <span class="site-state-item-count">121</span>
                      <span class="site-state-item-name">标签</span></a>
                  </div>
                </nav>
              </div>
              <div class="links-of-author motion-element">
                <span class="links-of-author-item">
                  <a href="https://github.com/Germey" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Germey" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
                </span>
                <span class="links-of-author-item">
                  <a href="mailto:cqc@cuiqingcai.com.com" title="邮件 → mailto:cqc@cuiqingcai.com.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮件</a>
                </span>
                <span class="links-of-author-item">
                  <a href="https://weibo.com/cuiqingcai" title="微博 → https:&#x2F;&#x2F;weibo.com&#x2F;cuiqingcai" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>微博</a>
                </span>
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/Germey" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;Germey" rel="noopener" target="_blank"><i class="fa fa-magic fa-fw"></i>知乎</a>
                </span>
              </div>
            </div>
            <div style=" width: 100%;" class="sidebar-panel sidebar-panel-image sidebar-panel-active">
              <a href="https://www.abuyun.com/http-proxy/introduce.html" target="_blank" rel="noopener">
                <img src="https://qiniu.cuiqingcai.com/88au8.jpg" style=" width: 100%;">
              </a>
            </div>
            <div style=" width: 100%;" class="sidebar-panel sidebar-panel-image sidebar-panel-active">
              <a href="https://tutorial.lengyue.video/?coupon=12ef4b1a-a3db-11ea-bb37-0242ac130002_cqx_850" target="_blank" rel="noopener">
                <img src="https://qiniu.cuiqingcai.com/bco2a.png" style=" width: 100%;">
              </a>
            </div>
            <div style=" width: 100%;" class="sidebar-panel sidebar-panel-image sidebar-panel-active">
              <a href="https://luminati-china.io/?affiliate=ref_5fbbaaa9647883f5c6f77095" target="_blank" rel="noopener">
                <img src="https://qiniu.cuiqingcai.com/ikkq9.jpg" style=" width: 100%;">
              </a>
            </div>
            <div class="sidebar-panel sidebar-panel-tags sidebar-panel-active">
              <h4 class="name"> 标签云 </h4>
              <div class="content">
                <a href="/tags/2048/" style="font-size: 10px;">2048</a> <a href="/tags/599/" style="font-size: 10px;">599</a> <a href="/tags/Bootstrap/" style="font-size: 11.25px;">Bootstrap</a> <a href="/tags/CDN/" style="font-size: 10px;">CDN</a> <a href="/tags/CQC/" style="font-size: 10px;">CQC</a> <a href="/tags/CSS/" style="font-size: 10px;">CSS</a> <a href="/tags/CSS-%E5%8F%8D%E7%88%AC%E8%99%AB/" style="font-size: 10px;">CSS 反爬虫</a> <a href="/tags/CV/" style="font-size: 10px;">CV</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Eclipse/" style="font-size: 11.25px;">Eclipse</a> <a href="/tags/FTP/" style="font-size: 10px;">FTP</a> <a href="/tags/GitHub/" style="font-size: 11.25px;">GitHub</a> <a href="/tags/HTML5/" style="font-size: 10px;">HTML5</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/IT/" style="font-size: 10px;">IT</a> <a href="/tags/JSP/" style="font-size: 10px;">JSP</a> <a href="/tags/JavaScript/" style="font-size: 10px;">JavaScript</a> <a href="/tags/K8s/" style="font-size: 10px;">K8s</a> <a href="/tags/LOGO/" style="font-size: 10px;">LOGO</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/MIUI/" style="font-size: 10px;">MIUI</a> <a href="/tags/MongoDB/" style="font-size: 10px;">MongoDB</a> <a href="/tags/Mysql/" style="font-size: 10px;">Mysql</a> <a href="/tags/PHP/" style="font-size: 11.25px;">PHP</a> <a href="/tags/PS/" style="font-size: 10px;">PS</a> <a href="/tags/Pathlib/" style="font-size: 10px;">Pathlib</a> <a href="/tags/PhantomJS/" style="font-size: 10px;">PhantomJS</a> <a href="/tags/PySpider/" style="font-size: 10px;">PySpider</a> <a href="/tags/Python/" style="font-size: 16.25px;">Python</a> <a href="/tags/Python3/" style="font-size: 12.5px;">Python3</a> <a href="/tags/Pythonic/" style="font-size: 10px;">Pythonic</a> <a href="/tags/QQ/" style="font-size: 10px;">QQ</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/SAE/" style="font-size: 10px;">SAE</a> <a href="/tags/SSH/" style="font-size: 10px;">SSH</a> <a href="/tags/SVG/" style="font-size: 10px;">SVG</a> <a href="/tags/Scrapy/" style="font-size: 10px;">Scrapy</a> <a href="/tags/Scrapy-redis/" style="font-size: 10px;">Scrapy-redis</a> <a href="/tags/Scrapy%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 10px;">Scrapy分布式</a> <a href="/tags/Selenium/" style="font-size: 10px;">Selenium</a> <a href="/tags/TKE/" style="font-size: 10px;">TKE</a> <a href="/tags/Ubuntu/" style="font-size: 11.25px;">Ubuntu</a> <a href="/tags/Vue/" style="font-size: 11.25px;">Vue</a> <a href="/tags/Webpack/" style="font-size: 10px;">Webpack</a> <a href="/tags/Windows/" style="font-size: 10px;">Windows</a> <a href="/tags/Winpcap/" style="font-size: 10px;">Winpcap</a> <a href="/tags/WordPress/" style="font-size: 13.75px;">WordPress</a> <a href="/tags/object-Object/" style="font-size: 10px;">[object Object]</a> <a href="/tags/android/" style="font-size: 10px;">android</a> <a href="/tags/ansible/" style="font-size: 10px;">ansible</a> <a href="/tags/cocos2d-x/" style="font-size: 10px;">cocos2d-x</a> <a href="/tags/e6/" style="font-size: 10px;">e6</a> <a href="/tags/fitvids/" style="font-size: 10px;">fitvids</a> <a href="/tags/git/" style="font-size: 11.25px;">git</a> <a href="/tags/json/" style="font-size: 10px;">json</a> <a href="/tags/js%E9%80%86%E5%90%91/" style="font-size: 10px;">js逆向</a> <a href="/tags/kubernetes/" style="font-size: 10px;">kubernetes</a> <a href="/tags/log/" style="font-size: 10px;">log</a> <a href="/tags/logging/" style="font-size: 10px;">logging</a> <a href="/tags/matlab/" style="font-size: 11.25px;">matlab</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/pywin32/" style="font-size: 10px;">pywin32</a> <a href="/tags/style/" style="font-size: 10px;">style</a> <a href="/tags/tomcat/" style="font-size: 10px;">tomcat</a> <a href="/tags/ubuntu/" style="font-size: 10px;">ubuntu</a> <a href="/tags/uwsgi/" style="font-size: 10px;">uwsgi</a> <a href="/tags/validate-cert/" style="font-size: 10px;">validate_cert</a> <a href="/tags/vsftpd/" style="font-size: 10px;">vsftpd</a> <a href="/tags/wamp/" style="font-size: 10px;">wamp</a> <a href="/tags/wineQQ/" style="font-size: 10px;">wineQQ</a> <a href="/tags/%E4%B8%83%E7%89%9B/" style="font-size: 11.25px;">七牛</a> <a href="/tags/%E4%B8%8A%E6%B5%B7/" style="font-size: 10px;">上海</a> <a href="/tags/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/" style="font-size: 10px;">个人网站</a> <a href="/tags/%E4%B8%BB%E9%A2%98/" style="font-size: 10px;">主题</a> <a href="/tags/%E4%BA%91%E5%AD%98%E5%82%A8/" style="font-size: 10px;">云存储</a> <a href="/tags/%E4%BA%AC%E4%B8%9C%E4%BA%91/" style="font-size: 10px;">京东云</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 10px;">人工智能</a> <a href="/tags/%E4%BB%A3%E7%90%86/" style="font-size: 10px;">代理</a> <a href="/tags/%E4%BB%A3%E7%A0%81/" style="font-size: 10px;">代码</a> <a href="/tags/%E4%BC%98%E5%8C%96/" style="font-size: 10px;">优化</a> <a href="/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/" style="font-size: 10px;">位运算</a> <a href="/tags/%E5%88%86%E4%BA%AB/" style="font-size: 10px;">分享</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 10px;">分布式</a> <a href="/tags/%E5%88%9B%E4%B8%9A/" style="font-size: 10px;">创业</a> <a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 11.25px;">前端</a> <a href="/tags/%E5%8E%9F%E7%94%9FAPP/" style="font-size: 10px;">原生APP</a> <a href="/tags/%E5%8F%8D%E7%88%AC%E8%99%AB/" style="font-size: 12.5px;">反爬虫</a> <a href="/tags/%E5%91%BD%E4%BB%A4/" style="font-size: 10px;">命令</a> <a href="/tags/%E5%93%8D%E5%BA%94%E5%BC%8F%E5%B8%83%E5%B1%80/" style="font-size: 10px;">响应式布局</a> <a href="/tags/%E5%9F%9F%E5%90%8D%E7%BB%91%E5%AE%9A/" style="font-size: 10px;">域名绑定</a> <a href="/tags/%E5%A4%A7%E4%BC%97%E7%82%B9%E8%AF%84/" style="font-size: 10px;">大众点评</a> <a href="/tags/%E5%AD%97%E4%BD%93%E5%8F%8D%E7%88%AC%E8%99%AB/" style="font-size: 10px;">字体反爬虫</a> <a href="/tags/%E5%AE%9E%E7%94%A8/" style="font-size: 10px;">实用</a> <a href="/tags/%E5%B4%94%E5%BA%86%E6%89%8D/" style="font-size: 18.75px;">崔庆才</a> <a href="/tags/%E5%B7%A5%E5%85%B7/" style="font-size: 10px;">工具</a> <a href="/tags/%E6%89%8B%E6%9C%BA%E8%AE%BF%E9%97%AE/" style="font-size: 10px;">手机访问</a> <a href="/tags/%E6%95%99%E7%A8%8B/" style="font-size: 10px;">教程</a> <a href="/tags/%E6%97%85%E6%B8%B8/" style="font-size: 10px;">旅游</a> <a href="/tags/%E6%97%A5%E5%BF%97/" style="font-size: 10px;">日志</a> <a href="/tags/%E6%9D%9C%E5%85%B0%E7%89%B9/" style="font-size: 10px;">杜兰特</a> <a href="/tags/%E6%A1%8C%E9%9D%A2/" style="font-size: 10px;">桌面</a> <a href="/tags/%E6%B1%9F%E5%8D%97/" style="font-size: 10px;">江南</a> <a href="/tags/%E6%B8%B8%E6%88%8F/" style="font-size: 10px;">游戏</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 15px;">爬虫</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/" style="font-size: 10px;">环境变量</a> <a href="/tags/%E7%94%9F%E6%B4%BB%E7%AC%94%E8%AE%B0/" style="font-size: 10px;">生活笔记</a> <a href="/tags/%E7%99%BB%E5%BD%95/" style="font-size: 10px;">登录</a> <a href="/tags/%E7%9F%A5%E4%B9%8E/" style="font-size: 10px;">知乎</a> <a href="/tags/%E7%9F%AD%E4%BF%A1/" style="font-size: 10px;">短信</a> <a href="/tags/%E7%BA%B8%E5%BC%A0/" style="font-size: 10px;">纸张</a> <a href="/tags/%E7%BD%91%E7%AB%99/" style="font-size: 10px;">网站</a> <a href="/tags/%E8%82%89%E5%A4%B9%E9%A6%8D/" style="font-size: 10px;">肉夹馍</a> <a href="/tags/%E8%A5%BF%E5%B0%91%E7%88%B7/" style="font-size: 10px;">西少爷</a> <a href="/tags/%E8%A7%86%E9%A2%91/" style="font-size: 10px;">视频</a> <a href="/tags/%E8%BF%9C%E7%A8%8B/" style="font-size: 10px;">远程</a> <a href="/tags/%E9%80%86%E5%90%91/" style="font-size: 10px;">逆向</a> <a href="/tags/%E9%85%8D%E7%BD%AE/" style="font-size: 10px;">配置</a> <a href="/tags/%E9%87%8D%E8%A3%85/" style="font-size: 10px;">重装</a> <a href="/tags/%E9%9D%99%E8%A7%85/" style="font-size: 17.5px;">静觅</a> <a href="/tags/%E9%A2%A0%E8%A6%86/" style="font-size: 10px;">颠覆</a> <a href="/tags/%E9%A3%9E%E4%BF%A1/" style="font-size: 10px;">飞信</a>
              </div>
              <script>
                const tagsColors = ['#00a67c', '#5cb85c', '#d9534f', '#567e95', '#b37333', '#f4843d', '#15a287']
                const tagsElements = document.querySelectorAll('.sidebar-panel-tags .content a')
                tagsElements.forEach((item) =>
                {
                  item.style.backgroundColor = tagsColors[Math.floor(Math.random() * tagsColors.length)]
                })

              </script>
            </div>
            <div class="sidebar-panel sidebar-panel-categories sidebar-panel-active">
              <h4 class="name"> 分类 </h4>
              <div class="content">
                <ul class="category-list">
                  <li class="category-list-item"><a class="category-list-link" href="/categories/C-C/">C/C++</a><span class="category-list-count">23</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/HTML/">HTML</a><span class="category-list-count">13</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">5</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/JavaScript/">JavaScript</a><span class="category-list-count">26</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">15</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Markdown/">Markdown</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Net/">Net</a><span class="category-list-count">4</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Other/">Other</a><span class="category-list-count">38</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/PHP/">PHP</a><span class="category-list-count">27</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">256</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/TypeScript/">TypeScript</a><span class="category-list-count">2</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E5%B1%95%E7%A4%BA/">个人展示</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E6%97%A5%E8%AE%B0/">个人日记</a><span class="category-list-count">6</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/">个人随笔</a><span class="category-list-count">10</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/">技术杂谈</a><span class="category-list-count">74</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/">未分类</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB%E7%AC%94%E8%AE%B0/">生活笔记</a><span class="category-list-count">1</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A6%8F%E5%88%A9%E4%B8%93%E5%8C%BA/">福利专区</a><span class="category-list-count">6</span></li>
                  <li class="category-list-item"><a class="category-list-link" href="/categories/%E8%81%8C%E4%BD%8D%E6%8E%A8%E8%8D%90/">职位推荐</a><span class="category-list-count">2</span></li>
                </ul>
              </div>
            </div>
            <div class="sidebar-panel sidebar-panel-friends sidebar-panel-active">
              <h4 class="name"> 友情链接 </h4>
              <ul class="friends">
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/j2dub.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.findhao.net/" target="_blank" rel="noopener">FindHao</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/ou6mm.jpg">
                  </span>
                  <span class="link">
                    <a href="https://diygod.me/" target="_blank" rel="noopener">DIYgod</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/6apxu.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.51dev.com/" target="_blank" rel="noopener">IT技术社区</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://www.jankl.com/img/titleshu.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.jankl.com/" target="_blank" rel="noopener">liberalist</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/bqlbs.png">
                  </span>
                  <span class="link">
                    <a href="http://www.urselect.com/" target="_blank" rel="noopener">优社电商</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/8s88c.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.yuanrenxue.com/" target="_blank" rel="noopener">猿人学</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/4i7yf.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.lizenghai.com/" target="_blank" rel="noopener">Python量化投资</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/2wgg5.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.yunlifang.cn/" target="_blank" rel="noopener">云立方</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/shwr6.png">
                  </span>
                  <span class="link">
                    <a href="http://lanbing510.info/" target="_blank" rel="noopener">冰蓝</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/blvoh.jpg">
                  </span>
                  <span class="link">
                    <a href="https://lengyue.me/" target="_blank" rel="noopener">冷月</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="http://qianxunclub.com/favicon.png">
                  </span>
                  <span class="link">
                    <a href="http://qianxunclub.com/" target="_blank" rel="noopener">千寻啊千寻</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/0044u.jpg">
                  </span>
                  <span class="link">
                    <a href="http://kodcloud.com/" target="_blank" rel="noopener">可道云</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/ygnpn.jpg">
                  </span>
                  <span class="link">
                    <a href="http://www.kunkundashen.cn/" target="_blank" rel="noopener">坤坤大神</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/22uv1.png">
                  </span>
                  <span class="link">
                    <a href="http://www.cenchong.com/" target="_blank" rel="noopener">岑冲博客</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/ev9kl.png">
                  </span>
                  <span class="link">
                    <a href="http://www.zxiaoji.com/" target="_blank" rel="noopener">张小鸡</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://www.chrafz.com/favicon.ico">
                  </span>
                  <span class="link">
                    <a href="https://www.chrafz.com/" target="_blank" rel="noopener">张弦先生</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://www.503error.com/favicon.ico">
                  </span>
                  <span class="link">
                    <a href="https://www.503error.com/" target="_blank" rel="noopener">张志明个人博客</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://seofangfa.com/favicon.ico">
                  </span>
                  <span class="link">
                    <a href="https://seofangfa.com/" target="_blank" rel="noopener">方法SEO</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/x714o.jpg">
                  </span>
                  <span class="link">
                    <a href="http://www.hubwiz.com/" target="_blank" rel="noopener">汇智网</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/lfmj3.png">
                  </span>
                  <span class="link">
                    <a href="http://frankchen.xyz/" target="_blank" rel="noopener">不正经数据科学家</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/129d8.png">
                  </span>
                  <span class="link">
                    <a href="https://www.bysocket.com/" target="_blank" rel="noopener">泥瓦匠BYSocket</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://www.xiongge.club/favicon.ico">
                  </span>
                  <span class="link">
                    <a href="https://www.xiongge.club/" target="_blank" rel="noopener">熊哥club</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/3w4fe.png">
                  </span>
                  <span class="link">
                    <a href="https://zerlong.com/" target="_blank" rel="noopener">知语</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/262r1.jpg">
                  </span>
                  <span class="link">
                    <a href="http://www.ysir308.com/" target="_blank" rel="noopener">程序员虾说</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/44hxf.png">
                  </span>
                  <span class="link">
                    <a href="http://redstonewill.com/" target="_blank" rel="noopener">红色石头</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/8g1fk.jpg">
                  </span>
                  <span class="link">
                    <a href="http://www.laodong.me/" target="_blank" rel="noopener">老董博客</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/wkaus.jpg">
                  </span>
                  <span class="link">
                    <a href="https://zhaoshuai.me/" target="_blank" rel="noopener">碎念</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/pgo0r.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.chenwenguan.com/" target="_blank" rel="noopener">陈文管的博客</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/kk82a.jpg">
                  </span>
                  <span class="link">
                    <a href="https://www.lxlinux.net/" target="_blank" rel="noopener">良许Linux教程网</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/lj0t2.jpg">
                  </span>
                  <span class="link">
                    <a href="https://tanqingbo.cn/" target="_blank" rel="noopener">IT码农</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/i8cdr.png">
                  </span>
                  <span class="link">
                    <a href="https://junyiseo.com/" target="_blank" rel="noopener">均益个人博客</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/chwv2.png">
                  </span>
                  <span class="link">
                    <a href="https://brucedone.com/" target="_blank" rel="noopener">大鱼的鱼塘</a>
                  </span>
                </li>
                <li class="friend">
                  <span class="logo">
                    <img src="https://qiniu.cuiqingcai.com/2y43o.png">
                  </span>
                  <span class="link">
                    <a href="http://bbs.nightteam.cn/" target="_blank" rel="noopener">夜幕爬虫安全论坛</a>
                  </span>
                </li>
              </ul>
            </div>
          </div>
        </aside>
        <div id="sidebar-dimmer"></div>
      </div>
    </main>
    <footer class="footer">
      <div class="footer-inner">
        <div class="copyright"> &copy; <span itemprop="copyrightYear">2021</span>
          <span class="with-love">
            <i class="fa fa-heart"></i>
          </span>
          <span class="author" itemprop="copyrightHolder">崔庆才丨静觅</span>
          <span class="post-meta-divider">|</span>
          <span class="post-meta-item-icon">
            <i class="fa fa-chart-area"></i>
          </span>
          <span title="站点总字数">2.5m</span>
          <span class="post-meta-divider">|</span>
          <span class="post-meta-item-icon">
            <i class="fa fa-coffee"></i>
          </span>
          <span title="站点阅读时长">37:31</span>
        </div>
        <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动 </div>
        <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">京ICP备18015597号-1 </a>
        </div>
        <script>
          (function ()
          {
            function leancloudSelector(url)
            {
              url = encodeURI(url);
              return document.getElementById(url).querySelector('.leancloud-visitors-count');
            }

            function addCount(Counter)
            {
              var visitors = document.querySelector('.leancloud_visitors');
              var url = decodeURI(visitors.id);
              var title = visitors.dataset.flagTitle;
              Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify(
              {
                url
              }))).then(response => response.json()).then((
              {
                results
              }) =>
              {
                if (results.length > 0)
                {
                  var counter = results[0];
                  leancloudSelector(url).innerText = counter.time + 1;
                  Counter('put', '/classes/Counter/' + counter.objectId,
                  {
                    time:
                    {
                      '__op': 'Increment',
                      'amount': 1
                    }
                  }).catch(error =>
                  {
                    console.error('Failed to save visitor count', error);
                  });
                }
                else
                {
                  Counter('post', '/classes/Counter',
                  {
                    title,
                    url,
                    time: 1
                  }).then(response => response.json()).then(() =>
                  {
                    leancloudSelector(url).innerText = 1;
                  }).catch(error =>
                  {
                    console.error('Failed to create', error);
                  });
                }
              }).catch(error =>
              {
                console.error('LeanCloud Counter Error', error);
              });
            }

            function showTime(Counter)
            {
              var visitors = document.querySelectorAll('.leancloud_visitors');
              var entries = [...visitors].map(element =>
              {
                return decodeURI(element.id);
              });
              Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify(
              {
                url:
                {
                  '$in': entries
                }
              }))).then(response => response.json()).then((
              {
                results
              }) =>
              {
                for (let url of entries)
                {
                  let target = results.find(item => item.url === url);
                  leancloudSelector(url).innerText = target ? target.time : 0;
                }
              }).catch(error =>
              {
                console.error('LeanCloud Counter Error', error);
              });
            }
            let
            {
              app_id,
              app_key,
              server_url
            } = {
              "enable": true,
              "app_id": "6X5dRQ0pnPWJgYy8SXOg0uID-gzGzoHsz",
              "app_key": "ziLDVEy73ne5HtFTiGstzHMS",
              "server_url": null,
              "security": false
            };

            function fetchData(api_server)
            {
              var Counter = (method, url, data) =>
              {
                return fetch(`${api_server}/1.1${url}`,
                {
                  method,
                  headers:
                  {
                    'X-LC-Id': app_id,
                    'X-LC-Key': app_key,
                    'Content-Type': 'application/json',
                  },
                  body: JSON.stringify(data)
                });
              };
              if (CONFIG.page.isPost)
              {
                if (CONFIG.hostname !== location.hostname) return;
                addCount(Counter);
              }
              else if (document.querySelectorAll('.post-title-link').length >= 1)
              {
                showTime(Counter);
              }
            }
            let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;
            if (api_server)
            {
              fetchData(api_server);
            }
            else
            {
              fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id).then(response => response.json()).then((
              {
                api_server
              }) =>
              {
                fetchData('https://' + api_server);
              });
            }
          })();

        </script>
      </div>
      <div class="footer-stat">
        <span id="cnzz_stat_icon_1279355174"></span>
        <script type="text/javascript">
          document.write(unescape("%3Cspan id='cnzz_stat_icon_1279355174'%3E%3C/span%3E%3Cscript src='https://v1.cnzz.com/z_stat.php%3Fid%3D1279355174%26online%3D1%26show%3Dline' type='text/javascript'%3E%3C/script%3E"));

        </script>
      </div>
    </footer>
  </div>
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/js/utils.js"></script>
  <script src="/.js"></script>
  <script src="/js/schemes/pisces.js"></script>
  <script src="/.js"></script>
  <script src="/js/next-boot.js"></script>
  <script src="/.js"></script>
  <script>
    (function ()
    {
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x = document.getElementsByTagName("link");
      //Find the last canonical URL
      if (x.length > 0)
      {
        for (i = 0; i < x.length; i++)
        {
          if (x[i].rel.toLowerCase() == 'canonical' && x[i].href)
          {
            canonicalURL = x[i].href;
          }
        }
      }
      //Get protocol
      if (!canonicalURL)
      {
        curProtocol = window.location.protocol.split(':')[0];
      }
      else
      {
        curProtocol = canonicalURL.split(':')[0];
      }
      //Get current URL if the canonical URL does not exist
      if (!canonicalURL) canonicalURL = window.location.href;
      //Assign script content. Replace current URL with the canonical URL
      ! function ()
      {
        var e = /([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,
          r = canonicalURL,
          t = document.referrer;
        if (!e.test(r))
        {
          var n = (String(curProtocol).toLowerCase() === 'https') ? "https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif" : "//api.share.baidu.com/s.gif";
          t ? (n += "?r=" + encodeURIComponent(document.referrer), r && (n += "&l=" + r)) : r && (n += "?l=" + r);
          var i = new Image;
          i.src = n
        }
      }(window);
    })();

  </script>
  <script src="/js/local-search.js"></script>
  <script src="/.js"></script>
</body>

</html>
